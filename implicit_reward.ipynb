{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rcParams['text.latex.preamble']=r\"\\usepackage{bm} \\usepackage{amsmath}\"\n",
    "\n",
    "params = {'text.usetex' : True,\n",
    "          'font.size' : 12,\n",
    "          'font.family' : 'lmodern',\n",
    "          }\n",
    "plt.rcParams.update(params)\n",
    "colors = ['#d3494e', '#1f77b4', '#ff7f0e', '#2ca02c','#7D3C98', '#873600', '#d3494e', '#9467bd', '#e377c2', '#7f7f7f'] \n",
    "fmts={6:'-o',1:'-s',2:'-d', 3:'-s',4:'-d', 0:'-^',5:'--P'}\n",
    "marksz={6:4,1:4,2:5,3:4,4:5,0:5,5:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "reference_policy_1_list: list = [0.01, 0.1, 0.3, 0.6, 0.9, 0.99]\n",
    "preference_coef: torch.Tensor = torch.linspace(.01, .99, 99)\n",
    "kl_coef_list: list = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "def RLHF_reward(\n",
    "    kl_coef: float,\n",
    "    preference_coef: torch.Tensor\n",
    "):\n",
    "    reward_1: torch.Tensor = (preference_coef / (1. - preference_coef) ).log()\n",
    "    reward_1 = reward_1 / (2 * kl_coef)\n",
    "    return reward_1, -1. * reward_1\n",
    "\n",
    "def Nash_RS_implicit_reward_sumexp(\n",
    "    kl_coef: float,\n",
    "    preference_coef: torch.Tensor,\n",
    "    reference_policy_1: float = .5\n",
    "):\n",
    "    reference_policy_2: float = 1. - reference_policy_1\n",
    "\n",
    "    sequence_1: torch.Tensor = np.log(reference_policy_2) - preference_coef / kl_coef\n",
    "    sequence_2: torch.Tensor = (np.log(reference_policy_1) - 1. / (2 * kl_coef)) * torch.ones_like(sequence_1)\n",
    "    sequence: torch.Tensor = torch.cat((sequence_1.reshape(sequence_1.shape[0],1), sequence_2.reshape(sequence_2.shape[0], 1)), dim=1)\n",
    "    reward_1: torch.Tensor = -1. * sequence.logsumexp(dim=1)\n",
    "\n",
    "    sequence_1: torch.Tensor = np.log(reference_policy_1) - (1. - preference_coef) / kl_coef\n",
    "    sequence_2: torch.Tensor = (np.log(reference_policy_2) - 1. / (2 * kl_coef)) * torch.ones_like(sequence_1)\n",
    "    sequence: torch.Tensor = torch.cat((sequence_1.reshape(sequence_1.shape[0],1), sequence_2.reshape(sequence_2.shape[0], 1)), dim=1)\n",
    "    reward_2: torch.Tensor = -1. * sequence.logsumexp(dim=1)    \n",
    "    \n",
    "    return reward_1, reward_2\n",
    "\n",
    "plt.figure(figsize=(21, 30), dpi=1200)\n",
    "gs = gridspec.GridSpec(len(reference_policy_1_list), len(kl_coef_list))\n",
    "gs.update(left=0.05, right=0.95, wspace=0.35, hspace=0.35)\n",
    "for index_1, reference_policy_1 in enumerate(reference_policy_1_list):\n",
    "    for index_2, kl_coef in enumerate(kl_coef_list):\n",
    "\n",
    "        ax = plt.subplot(gs[index_1, index_2])\n",
    "        ax.grid(alpha=0.5, linestyle='--')\n",
    "\n",
    "        reward_1, reward_2 = RLHF_reward(kl_coef, preference_coef)\n",
    "        ax.plot(preference_coef.cpu().detach().numpy(), reward_1.cpu().detach().numpy(), label=f\"RLHF $r_1$\")\n",
    "        ax.plot(preference_coef.cpu().detach().numpy(), reward_2.cpu().detach().numpy(), label=f\"RLHF $r_2$\")\n",
    "\n",
    "        reward_1, reward_2 = Nash_RS_implicit_reward_sumexp(kl_coef, preference_coef, reference_policy_1=reference_policy_1)\n",
    "        ax.plot(preference_coef.cpu().detach().numpy(), reward_1.cpu().detach().numpy(), label=f\"NashRS $r_1$\")\n",
    "        ax.plot(preference_coef.cpu().detach().numpy(), reward_2.cpu().detach().numpy(), label=f\"NashRS $r_2$\")\n",
    "        ax.set_yscale(\"symlog\")\n",
    "        ax.legend(framealpha=0.1, fontsize=16, loc=(0.5, 0.2))\n",
    "        ax.set_xlabel('preference ratio $a$', fontsize=25)\n",
    "        ax.set_title(f\"$\\pi_1$={reference_policy_1}, $\\\\tau$={kl_coef}\", fontsize=32)\n",
    "\n",
    "plt.savefig('implicit_reward_visualization.pdf', bbox_inches='tight', dpi=1200, format='pdf')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
