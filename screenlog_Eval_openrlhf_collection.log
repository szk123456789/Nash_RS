+--------------------------------------------------AutoDL--------------------------------------------------------+
[32m目录说明:[0m
╔═════════════════╦════════╦════╦═════════════════════════════════════════════════════════════════════════╗
║目录             ║名称    ║速度║说明                                                                     ║
╠═════════════════╬════════╬════╬═════════════════════════════════════════════════════════════════════════╣
║/                ║系 统 盘║一般║实例关机数据不会丢失，可存放代码等。会随保存镜像一起保存。               ║
║/root/autodl-tmp ║数 据 盘║ 快 ║实例关机数据不会丢失，可存放读写IO要求高的数据。但不会随保存镜像一起保存 ║
╚═════════════════╩════════╩════╩═════════════════════════════════════════════════════════════════════════╝
[32mCPU[0m ：56 核心
[32m内存[0m：400 GB
[32mGPU [0m：NVIDIA A800 80GB PCIe, 4
[32m存储[0m：
[32m  系 统 盘/               [0m：64% 20G/30G
[32m  数 据 盘/root/autodl-tmp[0m：16% 46G/300G
+----------------------------------------------------------------------------------------------------------------+
[31m*注意: [0m
[31m1.系统盘较小请将大的数据存放于数据盘或网盘中，重置系统时数据盘和网盘中的数据不受影响[0m
[31m2.清理系统盘请参考：https://www.autodl.com/docs/qa1/[0m
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# nvidia-smi
[?2004lMon Oct 21 21:14:48 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A800 80GB PCIe          On  |   00000000:4F:00.0 Off |                  Off |
| N/A   31C    P0             42W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A800 80GB PCIe          On  |   00000000:52:00.0 Off |                  Off |
| N/A   32C    P0             46W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A800 80GB PCIe          On  |   00000000:57:00.0 Off |                  Off |
| N/A   32C    P0             45W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A800 80GB PCIe          On  |   00000000:D5:00.0 Off |                  Off |
| N/A   33C    P0             46W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# nvidia-smibash run_eval_reward_openrlhf.shnvidia-smi[Kln -s /root/autodl-tmp/ckpt/models_NashRS_512prompt_trivial-5th /root/.cache/huggingface/hub4[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C3[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[60Pbash run_eval_reward_openrlhf.sh/root/eval_reward_openrlhf.sh /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_Original 512 2 2[A[48P/root/miniconda3/bin/python /root/eval_reward_openrlhf.py
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cbash /root/eval_reward_openrlhf.sh /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_Original 512 2 2[Anvidia-smi[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C/root/miniconda3/bin/python /root/eval_win_rate.pypython /root/.vscode-server/extensions/ms-python.python-2024.16.1-linux-x64/python_files/printEnvVariablesToFile.py /root/.vscode-server/extensions/ms-python.python-2024.16.1-linux-x64/python_files/deactivate/bash/envVars.txt[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cnvidia-smi[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpip install ml_collectionspython-box[all]~=7.0 --upgrade[7Python -m pip install --upgrade pip/root/.vscode-server/extensions/ms-python.python-2024.16.1-linux-x64/python_files/printEnvVariablesToFile.py /root/.vscode-server/extensions/ms-python.python-2024.16.1-linux-x64/python_files/deactivate/bash/envVars.txt[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cnvidia-smi -l 5[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cscreen -L -t NashRS_512prompt_trivial-5thls -lh[Kcd models_NashRS_512prompt_trivial-4thls -lh[Kcd models_NashRS_512prompt_trivial-3thls[Kcd /root/autodl-tmp/ckptls -lh[Kcd models_NashRS_512prompt_trivial-5thls[Kcd ..ls -lhcd models_NashRS_512prompt_trivial-4thls[Kcd /root/autodl-tmp/ckptpython /root/.vscode-server/extensions/ms-python.python-2024.16.1-linux-x64/python_files/printEnvVariablesToFile.py /root/.vscode-server/extensions/ms-python.python-2024.16.1-linux-x64/python_files/deactivate/bash/envVars.txt[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cnvidia-smi[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cscreen -X -S 563797 quitls[Knvidia-smi -l 5[9Pls -lhcd models_NashRS_512prompt_trivial-4thls[Kcd /root/autodl-tmp/ckpt[7Pbash nash_main.shscreen -L -t NashRS_512prompt_trivial-5thnvidia-smi[K[4Ptmux anvidia-smi[6PhtopCUDA_VISIBLE_DEVICES=2 python eval_decoder.py [16Pls Meta-Llama-3.1-8B-Instruct/[KCUDA_VISIBLE_DEVICES=2 python eval_decoder.py ls[KCUDA_VISIBLE_DEVICES=2 python eval_decoder.py ls[KCUDA_VISIBLE_DEVICES=2 python eval_decoder.py [22Pmkdir -p /dev/shm/models[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kbash run_eval_reward_openrlhf.sh
[?2004l+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_Original --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-21 21:15:26,521] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 21:15:28,302] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-21 21:15:28,302] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_Original --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-21 21:15:31,459] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 21:15:33,243] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-21 21:15:33,243] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-21 21:15:33,243] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-21 21:15:33,243] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-21 21:15:33,243] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-21 21:15:33,243] [INFO] [launch.py:256:main] process 602389 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_Original', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 21:15:33,244] [INFO] [launch.py:256:main] process 602390 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_Original', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 21:15:33,244] [INFO] [launch.py:256:main] process 602391 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_Original', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 21:15:33,244] [INFO] [launch.py:256:main] process 602392 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_Original', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 21:15:35,265] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 21:15:35,315] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 21:15:35,333] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 21:15:35,335] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-21 21:15:38,254] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-21 21:15:38,254] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-21 21:15:40,292] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-21 21:15:40,295] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-21 21:15:40,295] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.50it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  6.50it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.55it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.24it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.59it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  6.59it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.59it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.54it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.25it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  6.64it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.83it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.73it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.55it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.28it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.81it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.73it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.69it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.64it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.41it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.35it/s]
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-21 21:15:42,418] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-21 21:15:42,418] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-21 21:15:42,418] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 21:15:42,522] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 21:15:42,698] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 21:15:42,710] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 21:15:44,180] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-21 21:15:44,181] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-21 21:15:44,182] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 21:15:44,182] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 21:15:44,182] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 21:15:44,309] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-21 21:15:44,310] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-21 21:15:44,310] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.0 GB, percent = 3.1%
[2024-10-21 21:15:44,437] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-21 21:15:44,438] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-21 21:15:44,438] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.01 GB, percent = 3.1%
[2024-10-21 21:15:44,439] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-21 21:15:44,439] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-21 21:15:44,439] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-21 21:15:44,439] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-21 21:15:44,439] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-21 21:15:44,439] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8b04149150>
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-21 21:15:44,440] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-21 21:15:44,441] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-21 21:15:44,442] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-21 21:15:44,442] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-21 21:15:44,442] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-21 21:15:44,442] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-21 21:15:44,442] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
[2024-10-21 21:15:50,439] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-21 21:15:50,441] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-21 21:15:50,584] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-21 21:15:50,585] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-21 21:15:50,585] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.04 GB, percent = 3.1%
[2024-10-21 21:15:50,712] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-21 21:15:50,713] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-21 21:15:50,713] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.04 GB, percent = 3.1%
[2024-10-21 21:15:50,715] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8af5dae830>
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-21 21:15:50,715] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-21 21:15:50,716] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-21 21:15:50,717] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-21 21:15:50,717] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-21 21:15:50,717] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-21 21:15:50,717] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-21 21:15:50,717] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-21 21:15:50,717] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-21 21:15:50,717] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-21 21:15:50,717] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-21 21:15:50,717] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 646/100000 [00:00<00:15, 6455.27it/s]Preprocessing data:   2%|█▌                                                                                          | 1671/100000 [00:00<00:11, 8682.87it/s]Preprocessing data:   3%|██▍                                                                                         | 2717/100000 [00:00<00:10, 9492.77it/s]Preprocessing data:   4%|███▍                                                                                        | 3750/100000 [00:00<00:09, 9820.91it/s]Preprocessing data:   5%|████▍                                                                                       | 4769/100000 [00:00<00:09, 9950.59it/s]Preprocessing data:   6%|█████▎                                                                                     | 5798/100000 [00:00<00:09, 10065.60it/s]Preprocessing data:   7%|██████▏                                                                                    | 6828/100000 [00:00<00:09, 10139.68it/s]Preprocessing data:   8%|███████▏                                                                                   | 7852/100000 [00:00<00:09, 10168.68it/s]Preprocessing data:   9%|████████                                                                                   | 8869/100000 [00:00<00:08, 10155.16it/s]Preprocessing data:  10%|█████████                                                                                  | 9896/100000 [00:01<00:08, 10188.33it/s]Preprocessing data:  11%|█████████▊                                                                                | 10971/100000 [00:01<00:08, 10358.20it/s]Preprocessing data:  12%|██████████▊                                                                               | 12048/100000 [00:01<00:08, 10481.52it/s]Preprocessing data:  13%|███████████▊                                                                              | 13126/100000 [00:01<00:08, 10569.85it/s]Preprocessing data:  14%|████████████▊                                                                             | 14204/100000 [00:01<00:08, 10631.72it/s]Preprocessing data:  15%|█████████████▋                                                                            | 15275/100000 [00:01<00:07, 10653.29it/s]Preprocessing data:  16%|██████████████▋                                                                           | 16353/100000 [00:01<00:07, 10690.36it/s]Preprocessing data:  17%|███████████████▋                                                                          | 17423/100000 [00:01<00:07, 10692.48it/s]Preprocessing data:  18%|████████████████▋                                                                         | 18500/100000 [00:01<00:07, 10714.59it/s]Preprocessing data:  20%|█████████████████▌                                                                        | 19577/100000 [00:01<00:07, 10729.46it/s]Preprocessing data:  21%|██████████████████▌                                                                       | 20672/100000 [00:02<00:07, 10794.58it/s]Preprocessing data:  22%|███████████████████▌                                                                      | 21767/100000 [00:02<00:07, 10838.45it/s]Preprocessing data:  23%|████████████████████▌                                                                     | 22851/100000 [00:02<00:07, 10798.60it/s]Preprocessing data:  24%|█████████████████████▌                                                                    | 23931/100000 [00:02<00:07, 10748.31it/s]Preprocessing data:  25%|██████████████████████▌                                                                   | 25006/100000 [00:02<00:06, 10720.35it/s]Preprocessing data:  26%|███████████████████████▍                                                                  | 26079/100000 [00:02<00:06, 10574.57it/s]Preprocessing data:  27%|████████████████████████▍                                                                 | 27137/100000 [00:02<00:06, 10573.56it/s]Preprocessing data:  28%|█████████████████████████▍                                                                | 28195/100000 [00:02<00:06, 10570.81it/s]Preprocessing data:  29%|██████████████████████████▎                                                               | 29253/100000 [00:02<00:06, 10552.86it/s]Preprocessing data:  30%|███████████████████████████▎                                                              | 30309/100000 [00:02<00:06, 10542.88it/s]Preprocessing data:  31%|████████████████████████████▏                                                             | 31366/100000 [00:03<00:06, 10548.43it/s]Preprocessing data:  32%|█████████████████████████████▏                                                            | 32421/100000 [00:03<00:06, 10548.02it/s]Preprocessing data:  33%|██████████████████████████████▏                                                           | 33476/100000 [00:03<00:06, 10545.94it/s]Preprocessing data:  35%|███████████████████████████████                                                           | 34531/100000 [00:03<00:06, 10503.44it/s]Preprocessing data:  36%|████████████████████████████████                                                          | 35582/100000 [00:03<00:06, 10498.41it/s]Preprocessing data:  37%|████████████████████████████████▉                                                         | 36632/100000 [00:03<00:06, 10495.97it/s]Preprocessing data:  38%|█████████████████████████████████▉                                                        | 37682/100000 [00:03<00:05, 10495.56it/s]Preprocessing data:  39%|██████████████████████████████████▊                                                       | 38732/100000 [00:03<00:05, 10416.05it/s]Preprocessing data:  40%|███████████████████████████████████▊                                                      | 39785/100000 [00:03<00:05, 10449.14it/s]Preprocessing data:  41%|████████████████████████████████████▊                                                     | 40841/100000 [00:03<00:05, 10481.36it/s]Preprocessing data:  42%|█████████████████████████████████████▋                                                    | 41890/100000 [00:04<00:05, 10480.43it/s]Preprocessing data:  43%|██████████████████████████████████████▋                                                   | 42939/100000 [00:04<00:05, 10473.09it/s]Preprocessing data:  44%|███████████████████████████████████████▌                                                  | 44001/100000 [00:04<00:05, 10516.25it/s]Preprocessing data:  45%|████████████████████████████████████████▌                                                 | 45077/100000 [00:04<00:05, 10588.76it/s]Preprocessing data:  46%|█████████████████████████████████████████▌                                                | 46154/100000 [00:04<00:05, 10640.51it/s]Preprocessing data:  47%|██████████████████████████████████████████▌                                               | 47228/100000 [00:04<00:04, 10669.87it/s]Preprocessing data:  48%|███████████████████████████████████████████▍                                              | 48296/100000 [00:04<00:04, 10531.96it/s]Preprocessing data:  49%|████████████████████████████████████████████▍                                             | 49358/100000 [00:04<00:04, 10557.88it/s]Preprocessing data:  50%|█████████████████████████████████████████████▍                                            | 50419/100000 [00:04<00:04, 10572.69it/s]Preprocessing data:  51%|██████████████████████████████████████████████▎                                           | 51481/100000 [00:04<00:04, 10585.04it/s]Preprocessing data:  53%|███████████████████████████████████████████████▎                                          | 52547/100000 [00:05<00:04, 10607.02it/s]Preprocessing data:  54%|████████████████████████████████████████████████▎                                         | 53623/100000 [00:05<00:04, 10649.91it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▏                                        | 54699/100000 [00:05<00:04, 10681.84it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▏                                       | 55776/100000 [00:05<00:04, 10705.44it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▏                                      | 56857/100000 [00:05<00:04, 10735.03it/s]Preprocessing data:  58%|████████████████████████████████████████████████████▏                                     | 57936/100000 [00:05<00:03, 10750.73it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████                                     | 59014/100000 [00:05<00:03, 10758.52it/s]Preprocessing data:  60%|██████████████████████████████████████████████████████                                    | 60091/100000 [00:05<00:03, 10759.93it/s]Preprocessing data:  61%|███████████████████████████████████████████████████████                                   | 61168/100000 [00:05<00:03, 10665.60it/s]Preprocessing data:  62%|████████████████████████████████████████████████████████                                  | 62247/100000 [00:05<00:03, 10702.57it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▉                                 | 63327/100000 [00:06<00:03, 10729.61it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▉                                | 64401/100000 [00:06<00:03, 10722.64it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▉                               | 65474/100000 [00:06<00:03, 10620.86it/s]Preprocessing data:  67%|███████████████████████████████████████████████████████████▉                              | 66543/100000 [00:06<00:03, 10641.01it/s]Preprocessing data:  68%|████████████████████████████████████████████████████████████▊                             | 67616/100000 [00:06<00:03, 10666.80it/s]Preprocessing data:  69%|█████████████████████████████████████████████████████████████▊                            | 68688/100000 [00:06<00:02, 10680.78it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▊                           | 69759/100000 [00:06<00:02, 10687.48it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▋                          | 70831/100000 [00:06<00:02, 10694.75it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▋                         | 71906/100000 [00:06<00:02, 10708.73it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▋                        | 72977/100000 [00:06<00:02, 10259.43it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▎                       | 74007/100000 [00:07<00:02, 9472.21it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▏                      | 74968/100000 [00:07<00:02, 8988.18it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████                      | 75879/100000 [00:07<00:02, 8688.61it/s]Preprocessing data:  77%|█████████████████████████████████████████████████████████████████████▊                     | 76757/100000 [00:07<00:02, 8484.43it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▋                    | 77611/100000 [00:07<00:02, 8355.52it/s]Preprocessing data:  78%|███████████████████████████████████████████████████████████████████████▍                   | 78450/100000 [00:07<00:02, 8254.02it/s]Preprocessing data:  79%|████████████████████████████████████████████████████████████████████████▏                  | 79278/100000 [00:07<00:02, 8165.48it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▉                  | 80096/100000 [00:07<00:02, 7987.00it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▌                 | 80896/100000 [00:07<00:02, 7874.99it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▌                | 81898/100000 [00:08<00:02, 8486.36it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▎               | 82750/100000 [00:08<00:02, 8372.79it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████               | 83590/100000 [00:08<00:01, 8363.73it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▉              | 84486/100000 [00:08<00:01, 8536.26it/s]Preprocessing data:  86%|█████████████████████████████████████████████████████████████████████████████▊             | 85537/100000 [00:08<00:01, 9115.62it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▋            | 86451/100000 [00:08<00:01, 8884.98it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▍           | 87343/100000 [00:08<00:01, 8743.08it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████▍          | 88326/100000 [00:08<00:01, 9057.51it/s]Preprocessing data:  89%|█████████████████████████████████████████████████████████████████████████████████▎         | 89331/100000 [00:08<00:01, 9347.81it/s]Preprocessing data:  90%|██████████████████████████████████████████████████████████████████████████████████▏        | 90269/100000 [00:08<00:01, 9311.36it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▉        | 91202/100000 [00:09<00:01, 8615.58it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▊       | 92075/100000 [00:09<00:00, 8172.51it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▌      | 92903/100000 [00:09<00:00, 7899.78it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▎     | 93701/100000 [00:09<00:00, 7831.47it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████     | 94632/100000 [00:09<00:00, 8243.64it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████    | 95637/100000 [00:09<00:00, 8756.62it/s]Preprocessing data:  97%|███████████████████████████████████████████████████████████████████████████████████████▊   | 96535/100000 [00:09<00:00, 8820.65it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▋  | 97429/100000 [00:09<00:00, 8853.12it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▌ | 98368/100000 [00:09<00:00, 9010.07it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████▍| 99384/100000 [00:10<00:00, 9349.82it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9904.78it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2969, -1.2344,  0.2373, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.8203, -2.1250, -0.7930, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-2.1562, -0.6641, -0.5820, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1229])
attention_mask shape: torch.Size([4, 1229])
reward: tensor([-1.1094, -2.2031, -0.3203,  1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1610])
attention_mask shape: torch.Size([4, 1610])
reward: tensor([ 0.4805,  0.3262, -0.0378, -0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 737])
attention_mask shape: torch.Size([4, 737])
reward: tensor([-1.1172, -0.6133, -1.6328,  0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1954])
attention_mask shape: torch.Size([4, 1954])
reward: tensor([-1.1094, -2.0938,  0.5586, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([-0.9766, -0.3867,  0.8359,  0.9727], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1079])
attention_mask shape: torch.Size([4, 1079])
reward: tensor([-0.2422, -0.9258, -1.1484, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1498])
attention_mask shape: torch.Size([4, 1498])
reward: tensor([-0.9062,  1.1562, -1.1719, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1076])
attention_mask shape: torch.Size([4, 1076])
reward: tensor([-0.7773, -0.1514,  0.6211, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1700])
attention_mask shape: torch.Size([4, 1700])
reward: tensor([-0.4043, -2.0312, -0.5820,  0.0532], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1504])
attention_mask shape: torch.Size([4, 1504])
reward: tensor([-2.1250, -0.2969, -1.6406, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1838])
attention_mask shape: torch.Size([4, 1838])
reward: tensor([-0.6719, -0.0913,  1.7812, -0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1178])
attention_mask shape: torch.Size([4, 1178])
reward: tensor([-1.4844,  0.2090,  1.0859, -0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.5938, -0.6719, -0.8398, -0.7227], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1918])
attention_mask shape: torch.Size([4, 1918])
reward: tensor([-0.8789, -1.2656, -1.8750, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.8711,  0.1982, -1.9375,  0.5742], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1379])
attention_mask shape: torch.Size([4, 1379])
reward: tensor([ 0.3027, -0.9062, -0.7734, -1.2656], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 871])
attention_mask shape: torch.Size([4, 871])
reward: tensor([ 2.0781, -0.6992, -1.5859, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 852])
attention_mask shape: torch.Size([4, 852])
reward: tensor([-1.4844, -0.3691,  0.4355, -1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1698])
attention_mask shape: torch.Size([4, 1698])
reward: tensor([ 0.0388,  0.6836,  0.6133, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1158])
attention_mask shape: torch.Size([4, 1158])
reward: tensor([-0.3867,  0.6484,  0.4492, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1737])
attention_mask shape: torch.Size([4, 1737])
reward: tensor([-0.5781, -0.9883, -1.0078,  1.3984], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1933])
attention_mask shape: torch.Size([4, 1933])
reward: tensor([ 0.3320, -1.1641, -0.2871,  0.0635], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1414])
attention_mask shape: torch.Size([4, 1414])
reward: tensor([ 0.8594, -0.2354, -0.6211, -0.2852], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.0156,  0.3242, -0.2402, -0.1270], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 564])
attention_mask shape: torch.Size([4, 564])
reward: tensor([ 1.2031, -0.0356, -1.3281, -0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1484])
attention_mask shape: torch.Size([4, 1484])
reward: tensor([ 1.3984, -0.7852, -1.0312,  1.8750], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1704])
attention_mask shape: torch.Size([4, 1704])
reward: tensor([-0.7812,  1.2109,  0.0791, -0.2285], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1786])
attention_mask shape: torch.Size([4, 1786])
reward: tensor([-0.0488,  0.2021, -0.1553, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1587])
attention_mask shape: torch.Size([4, 1587])
reward: tensor([-1.9531,  0.2480, -0.4707, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.1016, -0.3027,  0.5430, -1.9609], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2004])
attention_mask shape: torch.Size([4, 2004])
reward: tensor([ 0.4902, -0.4316, -0.8789,  0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 963])
attention_mask shape: torch.Size([4, 963])
reward: tensor([ 0.3691,  0.0167, -0.4004, -0.0757], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1572])
attention_mask shape: torch.Size([4, 1572])
reward: tensor([ 1.4766,  0.6445,  0.9688, -1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1464])
attention_mask shape: torch.Size([4, 1464])
reward: tensor([ 0.9336, -1.0312, -0.9062,  0.5820], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1720])
attention_mask shape: torch.Size([4, 1720])
reward: tensor([ 0.2246, -0.2910, -0.3652, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([-1.7578, -0.2314, -1.5391, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.0781, -0.6055, -0.4258,  1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([1.0391, 1.0703, 0.8477, 0.8633], device='cuda:0', dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1592])
attention_mask shape: torch.Size([4, 1592])
reward: tensor([ 0.6523,  0.4219, -0.6328, -0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1238])
attention_mask shape: torch.Size([4, 1238])
reward: tensor([-0.7930, -1.9844,  0.0654, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 978])
attention_mask shape: torch.Size([4, 978])
reward: tensor([-1.4766, -0.1064, -0.5742, -0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1568])
attention_mask shape: torch.Size([4, 1568])
reward: tensor([0.0957, 0.7930, 1.3594, 0.6484], device='cuda:0', dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1637])
attention_mask shape: torch.Size([4, 1637])
reward: tensor([-1.5547, -0.7656,  0.4805, -0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1186])
attention_mask shape: torch.Size([4, 1186])
reward: tensor([-0.9414, -0.3770, -0.6719, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.3711, -0.8438, -0.0466,  0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.9336,  0.0133, -1.1172, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([-2.0938, -0.7656, -0.0178,  0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.1719, -1.3750, -1.6641,  0.0432], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.0547,  0.7188, -0.0222, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1619])
attention_mask shape: torch.Size([4, 1619])
reward: tensor([-0.2676,  0.8008,  0.8086, -0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1869])
attention_mask shape: torch.Size([4, 1869])
reward: tensor([-0.1602,  0.8633, -0.6172, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7734, -0.4531,  1.6016, -0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-2.1719, -0.9414, -1.5234, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1723])
attention_mask shape: torch.Size([4, 1723])
reward: tensor([-1.0391, -0.0623, -1.4609, -1.3984], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 991])
attention_mask shape: torch.Size([4, 991])
reward: tensor([-1.7188,  0.7812, -0.0400, -1.3047], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([ 0.4785, -1.5234,  1.2969, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2031,  1.4766, -2.2031,  0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1156])
attention_mask shape: torch.Size([4, 1156])
reward: tensor([ 1.0391, -1.1562, -0.4844, -0.0579], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1450])
attention_mask shape: torch.Size([4, 1450])
reward: tensor([-0.5547, -0.2354,  1.2031,  0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1991])
attention_mask shape: torch.Size([4, 1991])
reward: tensor([ 0.0579, -1.5938, -0.5742, -0.2266], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1362])
attention_mask shape: torch.Size([4, 1362])
reward: tensor([-1.4922,  0.3262, -0.7656, -0.7617], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1825])
attention_mask shape: torch.Size([4, 1825])
reward: tensor([ 0.5078, -0.8906, -0.3906, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 855])
attention_mask shape: torch.Size([4, 855])
reward: tensor([ 0.0566,  1.1406, -1.8906,  0.1953], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1143])
attention_mask shape: torch.Size([4, 1143])
reward: tensor([-0.3555, -0.1621, -1.1562, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1567])
attention_mask shape: torch.Size([4, 1567])
reward: tensor([ 0.2334, -1.0156,  0.2734,  1.4922], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1250])
attention_mask shape: torch.Size([4, 1250])
reward: tensor([-0.1309,  0.0422,  0.5586,  0.3320], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1537])
attention_mask shape: torch.Size([4, 1537])
reward: tensor([-1.1172, -0.6133, -0.1582,  0.6250], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1589])
attention_mask shape: torch.Size([4, 1589])
reward: tensor([ 1.4219, -0.0444, -0.3770,  0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 952])
attention_mask shape: torch.Size([4, 952])
reward: tensor([ 0.6758, -1.3125, -0.2490, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1626])
attention_mask shape: torch.Size([4, 1626])
reward: tensor([-0.7617,  0.7383,  0.0522, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1960])
attention_mask shape: torch.Size([4, 1960])
reward: tensor([-1.9375, -1.0469,  0.0791, -0.8125], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 517])
attention_mask shape: torch.Size([4, 517])
reward: tensor([ 0.5820,  0.1123, -0.4258,  0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1484])
attention_mask shape: torch.Size([4, 1484])
reward: tensor([ 0.6133, -1.7656, -1.6641, -1.1719], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([ 0.9219, -0.7148, -1.2031, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1819])
attention_mask shape: torch.Size([4, 1819])
reward: tensor([ 0.5117,  0.0742, -0.3965, -0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1747])
attention_mask shape: torch.Size([4, 1747])
reward: tensor([-0.4219,  0.9609, -0.9609,  0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1325])
attention_mask shape: torch.Size([4, 1325])
reward: tensor([-0.3340, -0.0356,  0.7148, -1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1003])
attention_mask shape: torch.Size([4, 1003])
reward: tensor([-0.7031, -0.2314,  1.0000, -1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1199])
attention_mask shape: torch.Size([4, 1199])
reward: tensor([ 1.8594, -0.5625, -0.1670, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.3867,  0.4043, -0.8906, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1278])
attention_mask shape: torch.Size([4, 1278])
reward: tensor([-0.4980,  0.8359, -0.6719, -0.1089], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1894])
attention_mask shape: torch.Size([4, 1894])
reward: tensor([-0.8906, -0.3730, -0.3691, -0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2422, -0.5820, -0.4570, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1297])
attention_mask shape: torch.Size([4, 1297])
reward: tensor([-0.5898,  0.1133, -0.3457,  1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1781])
attention_mask shape: torch.Size([4, 1781])
reward: tensor([-0.5898, -0.4453, -0.0466, -0.0200], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1519])
attention_mask shape: torch.Size([4, 1519])
reward: tensor([-0.0444, -1.2344,  0.5391, -0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1203])
attention_mask shape: torch.Size([4, 1203])
reward: tensor([-0.2314,  0.0510,  0.2441, -0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 948])
attention_mask shape: torch.Size([4, 948])
reward: tensor([-0.1426,  1.2109, -0.6484,  1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1907])
attention_mask shape: torch.Size([4, 1907])
reward: tensor([ 0.9609,  1.4766, -1.7812, -0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 832])
attention_mask shape: torch.Size([4, 832])
reward: tensor([ 0.3828, -0.6250,  1.4609, -0.2441], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1800])
attention_mask shape: torch.Size([4, 1800])
reward: tensor([ 1.3047,  0.0334,  0.3320, -0.2539], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1901])
attention_mask shape: torch.Size([4, 1901])
reward: tensor([ 0.6406, -1.1250,  0.6406, -1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 872])
attention_mask shape: torch.Size([4, 872])
reward: tensor([ 0.0222,  1.7031, -0.6484,  0.0045], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1441])
attention_mask shape: torch.Size([4, 1441])
reward: tensor([-0.3027, -0.0913, -0.1157, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-0.9414,  0.2871, -1.2656, -0.4805], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1474])
attention_mask shape: torch.Size([4, 1474])
reward: tensor([ 0.2402, -2.1562, -1.0781, -0.2197], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1779])
attention_mask shape: torch.Size([4, 1779])
reward: tensor([-0.2520, -1.2812, -0.6562, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1677])
attention_mask shape: torch.Size([4, 1677])
reward: tensor([-1.6172, -1.5000, -1.2969,  0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1145])
attention_mask shape: torch.Size([4, 1145])
reward: tensor([-0.1689, -1.3750, -0.3105, -1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1792])
attention_mask shape: torch.Size([4, 1792])
reward: tensor([ 0.1387, -0.4668,  0.5938, -0.0977], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1844])
attention_mask shape: torch.Size([4, 1844])
reward: tensor([ 1.0391, -0.7344, -1.8828,  1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([-0.1270, -0.9492,  1.5547, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.5469, -0.7344, -1.7422, -1.4766], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1779])
attention_mask shape: torch.Size([4, 1779])
reward: tensor([-0.6328, -1.4609, -1.1094, -0.0200], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1347])
attention_mask shape: torch.Size([4, 1347])
reward: tensor([ 0.6836, -1.3438, -0.0557, -0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([-0.5703, -0.4629, -0.0422, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2500, -0.9336,  0.4570, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1927])
attention_mask shape: torch.Size([4, 1927])
reward: tensor([-0.9062,  0.1953, -0.8008, -0.0089], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 302])
attention_mask shape: torch.Size([4, 302])
reward: tensor([ 1.2891, -0.7461,  0.1729, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1664])
attention_mask shape: torch.Size([4, 1664])
reward: tensor([-0.4219,  1.9844, -1.5547,  1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1476])
attention_mask shape: torch.Size([4, 1476])
reward: tensor([-0.7227, -0.1270, -0.4492, -1.8516], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1155])
attention_mask shape: torch.Size([4, 1155])
reward: tensor([-0.6211,  0.3027, -0.1865,  0.0500], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1273])
attention_mask shape: torch.Size([4, 1273])
reward: tensor([-0.6875, -1.0312, -0.8125, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2031, -1.9219,  0.2852, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1667])
attention_mask shape: torch.Size([4, 1667])
reward: tensor([-0.8906,  0.6172,  0.1602, -1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 795])
attention_mask shape: torch.Size([4, 795])
reward: tensor([-0.9258, -0.6680, -0.9414, -1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.5547, -0.2354,  0.2344, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1611])
attention_mask shape: torch.Size([4, 1611])
reward: tensor([-0.6016, -0.2129, -0.6992, -0.3770], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1473])
attention_mask shape: torch.Size([4, 1473])
reward: tensor([ 0.6328, -1.5469, -0.7148, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1663])
attention_mask shape: torch.Size([4, 1663])
reward: tensor([-0.1309,  0.9922, -0.3516, -0.3516], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1641])
attention_mask shape: torch.Size([4, 1641])
reward: tensor([-0.9141, -0.3730, -0.6992, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([ 0.8359, -2.0000, -0.7031, -0.0776], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1416])
attention_mask shape: torch.Size([4, 1416])
reward: tensor([-1.6016, -0.2471, -1.3750,  0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1438])
attention_mask shape: torch.Size([4, 1438])
reward: tensor([ 0.2891, -0.1758,  0.0111, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1917])
attention_mask shape: torch.Size([4, 1917])
reward: tensor([ 0.3574, -0.2090, -1.4609, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.1914, -0.4746, -2.2031,  0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.1758, -1.1719,  0.1689, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.8594,  0.2070, -0.3867, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1594])
attention_mask shape: torch.Size([4, 1594])
reward: tensor([ 0.6406, -1.2500,  0.7188, -1.9375], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2046])
attention_mask shape: torch.Size([4, 2046])
reward: tensor([-0.0156, -0.4141, -1.0938, -1.6641], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 848])
attention_mask shape: torch.Size([4, 848])
reward: tensor([ 0.5078,  0.2236, -0.5117,  0.2871], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 650])
attention_mask shape: torch.Size([4, 650])
reward: tensor([0.4453, 0.4180, 0.3613, 1.0547], device='cuda:0', dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1574])
attention_mask shape: torch.Size([4, 1574])
reward: tensor([-0.6914, -1.8281,  0.1426,  1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.5469, -1.2656,  0.5312, -1.6875], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1610])
attention_mask shape: torch.Size([4, 1610])
reward: tensor([ 1.4375, -1.0781,  1.1875,  0.3320], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1612])
attention_mask shape: torch.Size([4, 1612])
reward: tensor([ 0.5352,  0.5664, -1.7031, -0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.0469, -0.6445, -0.4043, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.6367, -0.5859, -0.7227,  0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-0.8047,  0.5430,  1.3281,  0.0767], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1633])
attention_mask shape: torch.Size([4, 1633])
reward: tensor([-0.9492, -0.6250, -0.9609,  0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1429])
attention_mask shape: torch.Size([4, 1429])
reward: tensor([-0.4570, -1.7422, -0.4980,  0.1777], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1202])
attention_mask shape: torch.Size([4, 1202])
reward: tensor([-0.9883,  0.4004, -0.6562,  0.1387], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.1865, -0.6172, -1.8047, -0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1322])
attention_mask shape: torch.Size([4, 1322])
reward: tensor([0.2129, 0.2070, 0.0698, 0.2695], device='cuda:0', dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 919])
attention_mask shape: torch.Size([4, 919])
reward: tensor([-0.4707, -0.2812,  0.3242, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 958])
attention_mask shape: torch.Size([4, 958])
reward: tensor([ 0.7656, -0.4355,  0.6680, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([-0.5273,  0.5430, -1.4453, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([-0.6992, -1.4375,  0.3457,  0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-0.9961,  0.3203, -1.1484, -0.1729], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-1.0703,  1.1719, -0.9141, -1.8594], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 989])
attention_mask shape: torch.Size([4, 989])
reward: tensor([-0.7109, -0.3555, -1.7031,  1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1345])
attention_mask shape: torch.Size([4, 1345])
reward: tensor([ 0.3008, -1.5000,  0.5000, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 760])
attention_mask shape: torch.Size([4, 760])
reward: tensor([-0.7812, -0.0244, -0.4453, -1.9297], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1647])
attention_mask shape: torch.Size([4, 1647])
reward: tensor([-1.0078, -1.0391, -2.2031,  1.2344], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1827])
attention_mask shape: torch.Size([4, 1827])
reward: tensor([-0.8711, -0.7031, -0.9258,  1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.0078, -1.1016,  0.6836, -0.3164], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1060])
attention_mask shape: torch.Size([4, 1060])
reward: tensor([-0.7773, -0.0111, -0.9766, -0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1194])
attention_mask shape: torch.Size([4, 1194])
reward: tensor([-0.3555, -0.5195, -1.6562, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1566])
attention_mask shape: torch.Size([4, 1566])
reward: tensor([-0.8594, -1.1875,  0.2441, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 886])
attention_mask shape: torch.Size([4, 886])
reward: tensor([-0.7383, -0.1157,  0.7930, -0.1377], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1527])
attention_mask shape: torch.Size([4, 1527])
reward: tensor([-0.0913, -0.8477, -1.0703,  0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1340])
attention_mask shape: torch.Size([4, 1340])
reward: tensor([ 0.4336, -0.7227, -0.5078, -0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.0133, -2.0156,  0.0233, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.8594, -1.3594,  0.8789, -1.4766], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1132])
attention_mask shape: torch.Size([4, 1132])
reward: tensor([ 0.6836, -0.4980, -0.9688, -1.7500], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1343])
attention_mask shape: torch.Size([4, 1343])
reward: tensor([ 1.0781, -1.4844,  0.3105, -0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1568])
attention_mask shape: torch.Size([4, 1568])
reward: tensor([-1.4844, -0.7656, -0.0200, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1857])
attention_mask shape: torch.Size([4, 1857])
reward: tensor([-1.0391,  0.6680, -1.8984,  1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.3672, -1.3594,  1.6875, -0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 893])
attention_mask shape: torch.Size([4, 893])
reward: tensor([-0.3594, -0.5469, -0.3516,  2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1680])
attention_mask shape: torch.Size([4, 1680])
reward: tensor([-0.3770,  0.0688, -1.6562,  1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1159])
attention_mask shape: torch.Size([4, 1159])
reward: tensor([-0.7344,  0.3457,  0.3770, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.0713, -0.8711,  0.2559, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.3652,  0.0300,  0.0889, -1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.2422,  1.2656,  0.3750, -1.8203], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1937])
attention_mask shape: torch.Size([4, 1937])
reward: tensor([-1.2109, -0.4082, -0.7148, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.1035, -2.1406, -0.5742, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1464])
attention_mask shape: torch.Size([4, 1464])
reward: tensor([-0.5898, -1.9844, -1.0078, -0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 897])
attention_mask shape: torch.Size([4, 897])
reward: tensor([-0.9336, -0.9141, -0.7656, -0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1885])
attention_mask shape: torch.Size([4, 1885])
reward: tensor([ 0.0991, -0.7734, -0.6133, -0.0178], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([ 0.7930, -1.8750, -1.6172, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.8750, -0.7812, -1.3984, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1860])
attention_mask shape: torch.Size([4, 1860])
reward: tensor([ 0.2178,  1.5703, -0.1982,  0.0776], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7852, -1.2422,  0.0967, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1783])
attention_mask shape: torch.Size([4, 1783])
reward: tensor([ 0.5234,  0.5430, -1.0547, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.6016, -1.1641,  0.4199, -1.1719], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1341])
attention_mask shape: torch.Size([4, 1341])
reward: tensor([-0.7812,  0.7383, -0.4844, -0.2266], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1182])
attention_mask shape: torch.Size([4, 1182])
reward: tensor([-0.4180, -0.1670,  1.2266, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1669])
attention_mask shape: torch.Size([4, 1669])
reward: tensor([-0.3379,  0.0723,  1.5312,  0.6094], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6133, -0.4746, -0.2158, -0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-1.6953, -1.8594, -0.8711, -0.7539], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1904])
attention_mask shape: torch.Size([4, 1904])
reward: tensor([-2.2031, -0.0178, -0.0422, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1304])
attention_mask shape: torch.Size([4, 1304])
reward: tensor([-0.6836, -2.1562, -0.0356, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1376])
attention_mask shape: torch.Size([4, 1376])
reward: tensor([-0.3965, -0.6211, -0.9766, -0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1038])
attention_mask shape: torch.Size([4, 1038])
reward: tensor([-1.0781, -0.0311,  0.0189, -0.0178], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1852])
attention_mask shape: torch.Size([4, 1852])
reward: tensor([-0.5234,  1.1562, -1.0781, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.3125,  0.1387, -0.5508, -1.7812], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.2598, -1.7969, -0.7031,  0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([-0.8711, -0.9883, -1.7031,  0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-1.1094, -0.4492,  0.2119, -0.6094], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1115])
attention_mask shape: torch.Size([4, 1115])
reward: tensor([-0.5039, -2.1406, -0.6641, -0.5820], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1575])
attention_mask shape: torch.Size([4, 1575])
reward: tensor([-0.8477, -0.0510, -1.0703, -0.2891], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 553])
attention_mask shape: torch.Size([4, 553])
reward: tensor([-0.5625,  0.3164, -0.6680, -0.1846], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1249])
attention_mask shape: torch.Size([4, 1249])
reward: tensor([-1.6953, -1.1797, -0.2422,  0.2285], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1879])
attention_mask shape: torch.Size([4, 1879])
reward: tensor([-0.3730, -0.4043, -1.6172, -1.7891], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.1484, -1.9453, -2.2031,  0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.4609, -0.8984, -0.0400,  0.2236], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1283])
attention_mask shape: torch.Size([4, 1283])
reward: tensor([-0.4453,  0.1211, -1.5859, -1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.0078, -0.5117, -0.4629, -1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-1.7656,  0.5547,  0.3008, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1137])
attention_mask shape: torch.Size([4, 1137])
reward: tensor([-1.1172, -0.7188, -0.9141, -0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.1245,  0.4805, -1.3516,  0.1035], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1661])
attention_mask shape: torch.Size([4, 1661])
reward: tensor([ 1.1016,  0.1836,  0.5508, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([-1.2109,  0.4414,  1.7969, -0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1134])
attention_mask shape: torch.Size([4, 1134])
reward: tensor([-0.3965,  1.8828,  0.0510, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1867])
attention_mask shape: torch.Size([4, 1867])
reward: tensor([ 0.1943, -1.1641, -1.7656, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1306])
attention_mask shape: torch.Size([4, 1306])
reward: tensor([-0.1445, -1.5703,  0.4141, -1.1641], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1175])
attention_mask shape: torch.Size([4, 1175])
reward: tensor([ 1.1094, -1.2500, -2.1094,  0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1845])
attention_mask shape: torch.Size([4, 1845])
reward: tensor([-0.1270, -0.8164, -2.1406, -1.3984], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1799])
attention_mask shape: torch.Size([4, 1799])
reward: tensor([ 0.2471, -0.9492, -0.8281,  1.5547], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1235])
attention_mask shape: torch.Size([4, 1235])
reward: tensor([-1.0391, -1.2422, -0.7930, -1.8672], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 944])
attention_mask shape: torch.Size([4, 944])
reward: tensor([ 0.1465, -0.5898, -0.9688, -0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1356])
attention_mask shape: torch.Size([4, 1356])
reward: tensor([ 0.7930, -1.1797, -0.7695, -0.2910], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1722])
attention_mask shape: torch.Size([4, 1722])
reward: tensor([-2.0156, -0.3281, -0.0422, -0.0776], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 649])
attention_mask shape: torch.Size([4, 649])
reward: tensor([ 0.1201, -0.8438, -0.9609,  0.0378], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.1875,  1.0703,  0.0211, -0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 873])
attention_mask shape: torch.Size([4, 873])
reward: tensor([-0.0178, -1.5469, -0.3418,  1.6172], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1759])
attention_mask shape: torch.Size([4, 1759])
reward: tensor([-1.4922,  0.2441,  0.8008, -1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1568])
attention_mask shape: torch.Size([4, 1568])
reward: tensor([-0.2637, -1.4062, -0.1533, -0.0200], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1484])
attention_mask shape: torch.Size([4, 1484])
reward: tensor([ 0.7852, -0.7695,  2.0938, -1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([ 2.1250, -1.2422, -0.2109,  1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1129])
attention_mask shape: torch.Size([4, 1129])
reward: tensor([0.4004, 0.0356, 1.7344, 0.8125], device='cuda:0', dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6523, -1.1406, -0.5039,  0.0967], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1307])
attention_mask shape: torch.Size([4, 1307])
reward: tensor([-1.8828, -1.0703, -0.0156,  1.4297], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.0547, -0.4531, -1.1016, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1287])
attention_mask shape: torch.Size([4, 1287])
reward: tensor([ 1.5469, -0.2227, -1.5078, -0.4492], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 949])
attention_mask shape: torch.Size([4, 949])
reward: tensor([ 1.2969, -1.7969,  0.7852,  0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1380])
attention_mask shape: torch.Size([4, 1380])
reward: tensor([ 0.2070,  1.4922,  0.1514, -1.1641], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1920])
attention_mask shape: torch.Size([4, 1920])
reward: tensor([-0.2578,  0.3086, -0.1221, -0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1731])
attention_mask shape: torch.Size([4, 1731])
reward: tensor([ 0.2793, -1.2266, -0.9883, -0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.4316,  1.5469, -2.1562,  0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1667])
attention_mask shape: torch.Size([4, 1667])
reward: tensor([-0.6875,  0.7031,  1.8438, -1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1686])
attention_mask shape: torch.Size([4, 1686])
reward: tensor([-1.2188, -1.3672, -1.3047,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1520])
attention_mask shape: torch.Size([4, 1520])
reward: tensor([-1.1719,  0.4082,  0.8945, -1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1371])
attention_mask shape: torch.Size([4, 1371])
reward: tensor([-0.7773, -0.2598, -0.8789, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
[2024-10-21 22:29:14,397] [INFO] [launch.py:351:main] Process 602391 exits successfully.
sequences shape: torch.Size([4, 1987])
attention_mask shape: torch.Size([4, 1987])
reward: tensor([-1.2500, -0.7656, -1.1562,  0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.2490, -0.6055, -0.7852, -1.7188], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1109])
attention_mask shape: torch.Size([4, 1109])
reward: tensor([-2.2031, -1.8125, -0.8789, -1.8750], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1448])
attention_mask shape: torch.Size([4, 1448])
reward: tensor([-0.9141,  0.8711, -0.2002, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1135])
attention_mask shape: torch.Size([4, 1135])
reward: tensor([-0.6016, -0.1533, -1.0703, -1.6641], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 945])
attention_mask shape: torch.Size([4, 945])
reward: tensor([ 0.5820, -0.0089,  1.3438, -1.6094], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1980])
attention_mask shape: torch.Size([4, 1980])
reward: tensor([ 1.7344,  1.3516,  0.7773, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
[2024-10-21 22:31:28,533] [INFO] [launch.py:351:main] Process 602392 exits successfully.
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-0.4453, -1.2188, -2.0312,  0.0244], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-21 22:31:49,554] [INFO] [launch.py:351:main] Process 602389 exits successfully.
[2024-10-21 22:32:19,584] [INFO] [launch.py:351:main] Process 602390 exits successfully.
+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_PPO_512prompt_trivial-1th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_PPO_512prompt_trivial-1th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_PPO_1 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-21 22:32:23,729] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 22:32:25,564] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-21 22:32:25,565] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_PPO_512prompt_trivial-1th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_PPO_1 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-21 22:32:27,516] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 22:32:29,378] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-21 22:32:29,378] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-21 22:32:29,378] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-21 22:32:29,378] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-21 22:32:29,378] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-21 22:32:29,378] [INFO] [launch.py:256:main] process 604216 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_PPO_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_PPO_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 22:32:29,379] [INFO] [launch.py:256:main] process 604217 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_PPO_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_PPO_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 22:32:29,379] [INFO] [launch.py:256:main] process 604218 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_PPO_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_PPO_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 22:32:29,379] [INFO] [launch.py:256:main] process 604219 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_PPO_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_PPO_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 22:32:31,385] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 22:32:31,405] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 22:32:31,418] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 22:32:31,425] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-21 22:32:34,765] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-21 22:32:34,765] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-21 22:32:35,449] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-21 22:32:35,450] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-21 22:32:35,463] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.43it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.35it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.31it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.41it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.36it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.32it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.36it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.39it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.30it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.60it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.50it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.59it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.48it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.48it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.45it/s]
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.48it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.54it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.59it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.84it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.73it/s]
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-21 22:32:38,093] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-21 22:32:38,094] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-21 22:32:38,094] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 22:32:40,096] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 22:32:47,726] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 22:32:48,218] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 22:32:48,675] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 22:32:48,675] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 22:32:48,675] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-21 22:32:48,676] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-21 22:32:48,678] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 22:32:48,829] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-21 22:32:48,830] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-21 22:32:48,830] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.89 GB, percent = 3.1%
[2024-10-21 22:32:48,960] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-21 22:32:48,961] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-21 22:32:48,961] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.89 GB, percent = 3.1%
[2024-10-21 22:32:48,962] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-21 22:32:48,962] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-21 22:32:48,962] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-21 22:32:48,962] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-21 22:32:48,962] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f15c819e4a0>
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-21 22:32:48,963] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-21 22:32:48,964] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-21 22:32:48,964] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-21 22:32:48,965] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-21 22:32:48,965] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-21 22:32:48,965] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 22:32:53,460] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-21 22:32:53,461] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
[2024-10-21 22:32:53,615] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-21 22:32:53,616] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-21 22:32:53,616] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.91 GB, percent = 3.1%
[2024-10-21 22:32:53,753] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-21 22:32:53,754] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-21 22:32:53,754] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.91 GB, percent = 3.1%
[2024-10-21 22:32:53,755] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-21 22:32:53,755] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-21 22:32:53,755] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-21 22:32:53,755] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-21 22:32:53,755] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f15c02ca9b0>
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-21 22:32:53,756] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-21 22:32:53,757] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-21 22:32:53,757] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 639/100000 [00:00<00:15, 6385.86it/s]Preprocessing data:   2%|█▌                                                                                          | 1660/100000 [00:00<00:11, 8628.57it/s]Preprocessing data:   3%|██▍                                                                                         | 2667/100000 [00:00<00:10, 9284.10it/s]Preprocessing data:   4%|███▍                                                                                        | 3673/100000 [00:00<00:10, 9587.29it/s]Preprocessing data:   5%|████▎                                                                                       | 4693/100000 [00:00<00:09, 9807.69it/s]Preprocessing data:   6%|█████▎                                                                                      | 5711/100000 [00:00<00:09, 9933.21it/s]Preprocessing data:   7%|██████                                                                                     | 6729/100000 [00:00<00:09, 10013.03it/s]Preprocessing data:   8%|███████                                                                                    | 7758/100000 [00:00<00:09, 10099.29it/s]Preprocessing data:   9%|███████▉                                                                                   | 8768/100000 [00:00<00:09, 10012.11it/s]Preprocessing data:  10%|████████▉                                                                                  | 9793/100000 [00:01<00:08, 10083.24it/s]Preprocessing data:  11%|█████████▊                                                                                | 10855/100000 [00:01<00:08, 10244.14it/s]Preprocessing data:  12%|██████████▋                                                                               | 11910/100000 [00:01<00:08, 10336.28it/s]Preprocessing data:  13%|███████████▋                                                                              | 12987/100000 [00:01<00:08, 10465.16it/s]Preprocessing data:  14%|████████████▋                                                                             | 14063/100000 [00:01<00:08, 10553.30it/s]Preprocessing data:  15%|█████████████▋                                                                            | 15141/100000 [00:01<00:07, 10618.41it/s]Preprocessing data:  16%|██████████████▌                                                                           | 16222/100000 [00:01<00:07, 10674.36it/s]Preprocessing data:  17%|███████████████▌                                                                          | 17298/100000 [00:01<00:07, 10699.22it/s]Preprocessing data:  18%|████████████████▌                                                                         | 18374/100000 [00:01<00:07, 10715.39it/s]Preprocessing data:  19%|█████████████████▌                                                                        | 19452/100000 [00:01<00:07, 10733.48it/s]Preprocessing data:  21%|██████████████████▍                                                                       | 20544/100000 [00:02<00:07, 10789.11it/s]Preprocessing data:  22%|███████████████████▍                                                                      | 21640/100000 [00:02<00:07, 10839.87it/s]Preprocessing data:  23%|████████████████████▍                                                                     | 22725/100000 [00:02<00:07, 10802.43it/s]Preprocessing data:  24%|█████████████████████▍                                                                    | 23806/100000 [00:02<00:07, 10747.02it/s]Preprocessing data:  25%|██████████████████████▍                                                                   | 24881/100000 [00:02<00:07, 10715.17it/s]Preprocessing data:  26%|███████████████████████▎                                                                  | 25953/100000 [00:02<00:06, 10658.17it/s]Preprocessing data:  27%|████████████████████████▎                                                                 | 27019/100000 [00:02<00:06, 10629.44it/s]Preprocessing data:  28%|█████████████████████████▎                                                                | 28082/100000 [00:02<00:06, 10599.36it/s]Preprocessing data:  29%|██████████████████████████▏                                                               | 29142/100000 [00:02<00:06, 10576.76it/s]Preprocessing data:  30%|███████████████████████████▏                                                              | 30200/100000 [00:02<00:06, 10572.24it/s]Preprocessing data:  31%|████████████████████████████▏                                                             | 31258/100000 [00:03<00:06, 10566.90it/s]Preprocessing data:  32%|█████████████████████████████                                                             | 32315/100000 [00:03<00:06, 10552.89it/s]Preprocessing data:  33%|██████████████████████████████                                                            | 33371/100000 [00:03<00:06, 10523.24it/s]Preprocessing data:  34%|██████████████████████████████▉                                                           | 34424/100000 [00:03<00:06, 10503.51it/s]Preprocessing data:  35%|███████████████████████████████▉                                                          | 35475/100000 [00:03<00:06, 10406.97it/s]Preprocessing data:  37%|████████████████████████████████▊                                                         | 36519/100000 [00:03<00:06, 10414.43it/s]Preprocessing data:  38%|█████████████████████████████████▊                                                        | 37568/100000 [00:03<00:05, 10434.35it/s]Preprocessing data:  39%|██████████████████████████████████▊                                                       | 38620/100000 [00:03<00:05, 10457.11it/s]Preprocessing data:  40%|███████████████████████████████████▋                                                      | 39675/100000 [00:03<00:05, 10483.74it/s]Preprocessing data:  41%|████████████████████████████████████▋                                                     | 40730/100000 [00:03<00:05, 10503.45it/s]Preprocessing data:  42%|█████████████████████████████████████▌                                                    | 41783/100000 [00:04<00:05, 10509.95it/s]Preprocessing data:  43%|██████████████████████████████████████▌                                                   | 42837/100000 [00:04<00:05, 10517.71it/s]Preprocessing data:  44%|███████████████████████████████████████▌                                                  | 43916/100000 [00:04<00:05, 10597.78it/s]Preprocessing data:  45%|████████████████████████████████████████▍                                                 | 44997/100000 [00:04<00:05, 10661.15it/s]Preprocessing data:  46%|█████████████████████████████████████████▍                                                | 46078/100000 [00:04<00:05, 10704.61it/s]Preprocessing data:  47%|██████████████████████████████████████████▍                                               | 47157/100000 [00:04<00:04, 10728.59it/s]Preprocessing data:  48%|███████████████████████████████████████████▍                                              | 48235/100000 [00:04<00:04, 10741.31it/s]Preprocessing data:  49%|████████████████████████████████████████████▍                                             | 49315/100000 [00:04<00:04, 10758.10it/s]Preprocessing data:  50%|█████████████████████████████████████████████▎                                            | 50396/100000 [00:04<00:04, 10772.55it/s]Preprocessing data:  51%|██████████████████████████████████████████████▎                                           | 51475/100000 [00:04<00:04, 10775.90it/s]Preprocessing data:  53%|███████████████████████████████████████████████▎                                          | 52553/100000 [00:05<00:04, 10616.23it/s]Preprocessing data:  54%|████████████████████████████████████████████████▎                                         | 53632/100000 [00:05<00:04, 10667.03it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▏                                        | 54716/100000 [00:05<00:04, 10716.16it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▏                                       | 55799/100000 [00:05<00:04, 10747.96it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▏                                      | 56882/100000 [00:05<00:04, 10770.12it/s]Preprocessing data:  58%|████████████████████████████████████████████████████▏                                     | 57964/100000 [00:05<00:03, 10782.56it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████▏                                    | 59048/100000 [00:05<00:03, 10799.26it/s]Preprocessing data:  60%|██████████████████████████████████████████████████████                                    | 60129/100000 [00:05<00:03, 10796.43it/s]Preprocessing data:  61%|███████████████████████████████████████████████████████                                   | 61211/100000 [00:05<00:03, 10801.88it/s]Preprocessing data:  62%|████████████████████████████████████████████████████████                                  | 62298/100000 [00:05<00:03, 10820.55it/s]Preprocessing data:  63%|█████████████████████████████████████████████████████████                                 | 63382/100000 [00:06<00:03, 10824.75it/s]Preprocessing data:  64%|██████████████████████████████████████████████████████████                                | 64465/100000 [00:06<00:03, 10810.37it/s]Preprocessing data:  66%|██████████████████████████████████████████████████████████▉                               | 65547/100000 [00:06<00:03, 10785.35it/s]Preprocessing data:  67%|███████████████████████████████████████████████████████████▉                              | 66626/100000 [00:06<00:03, 10752.49it/s]Preprocessing data:  68%|████████████████████████████████████████████████████████████▉                             | 67702/100000 [00:06<00:03, 10722.40it/s]Preprocessing data:  69%|█████████████████████████████████████████████████████████████▉                            | 68775/100000 [00:06<00:02, 10702.24it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▊                           | 69846/100000 [00:06<00:02, 10694.13it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▊                          | 70916/100000 [00:06<00:02, 10681.46it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▊                         | 71985/100000 [00:06<00:02, 10671.40it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▋                        | 73053/100000 [00:06<00:02, 10147.77it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▍                       | 74074/100000 [00:07<00:02, 8996.81it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▎                      | 75000/100000 [00:07<00:02, 8668.76it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████                      | 75885/100000 [00:07<00:02, 8459.43it/s]Preprocessing data:  77%|█████████████████████████████████████████████████████████████████████▊                     | 76743/100000 [00:07<00:02, 8314.54it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▌                    | 77582/100000 [00:07<00:02, 8208.31it/s]Preprocessing data:  78%|███████████████████████████████████████████████████████████████████████▎                   | 78408/100000 [00:07<00:02, 8133.16it/s]Preprocessing data:  79%|████████████████████████████████████████████████████████████████████████                   | 79225/100000 [00:07<00:02, 8067.49it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▊                  | 80034/100000 [00:07<00:02, 7919.46it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▌                 | 80828/100000 [00:07<00:02, 7780.98it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▍                | 81852/100000 [00:08<00:02, 8482.34it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▎               | 82705/100000 [00:08<00:02, 8354.63it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████               | 83544/100000 [00:08<00:01, 8330.18it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▊              | 84415/100000 [00:08<00:01, 8440.31it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▊             | 85463/100000 [00:08<00:01, 9039.08it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▌            | 86370/100000 [00:08<00:01, 8880.26it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▍           | 87261/100000 [00:08<00:01, 8676.46it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████▎          | 88280/100000 [00:08<00:01, 9114.55it/s]Preprocessing data:  89%|█████████████████████████████████████████████████████████████████████████████████▎         | 89292/100000 [00:08<00:01, 9407.18it/s]Preprocessing data:  90%|██████████████████████████████████████████████████████████████████████████████████         | 90238/100000 [00:08<00:01, 9422.40it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▉        | 91183/100000 [00:09<00:01, 8674.92it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▊       | 92064/100000 [00:09<00:00, 8196.55it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▌      | 92896/100000 [00:09<00:00, 7894.12it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▎     | 93695/100000 [00:09<00:00, 7812.89it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████     | 94624/100000 [00:09<00:00, 8222.14it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████    | 95642/100000 [00:09<00:00, 8777.44it/s]Preprocessing data:  97%|███████████████████████████████████████████████████████████████████████████████████████▊   | 96537/100000 [00:09<00:00, 8824.75it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▋  | 97428/100000 [00:09<00:00, 8847.76it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▌ | 98360/100000 [00:09<00:00, 8984.62it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████▍| 99380/100000 [00:10<00:00, 9341.69it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9889.57it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3730, -1.1250,  0.0122, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.0312, -1.0703,  0.7773, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.0078, -1.8984, -1.1562, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 485])
attention_mask shape: torch.Size([4, 485])
reward: tensor([-0.0089, -0.6328,  0.2930,  1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1669])
attention_mask shape: torch.Size([4, 1669])
reward: tensor([0.1387, 0.0200, 0.3164, 0.1377], device='cuda:0', dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1374])
attention_mask shape: torch.Size([4, 1374])
reward: tensor([ 0.0145, -1.6875, -0.0957, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1954])
attention_mask shape: torch.Size([4, 1954])
reward: tensor([-0.2871, -0.6328,  0.7734, -0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([ 0.6250, -1.4219,  0.0255,  0.0078], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1158])
attention_mask shape: torch.Size([4, 1158])
reward: tensor([ 0.5703,  0.6914, -0.0244,  0.0289], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1691])
attention_mask shape: torch.Size([4, 1691])
reward: tensor([-1.3828,  0.1670, -0.6328, -0.7539], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1619])
attention_mask shape: torch.Size([4, 1619])
reward: tensor([-0.8281, -0.6641, -1.5859, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1418])
attention_mask shape: torch.Size([4, 1418])
reward: tensor([ 0.7617, -0.4258,  0.1455,  0.6719], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1355])
attention_mask shape: torch.Size([4, 1355])
reward: tensor([-0.7695, -0.6055,  0.1064, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1721])
attention_mask shape: torch.Size([4, 1721])
reward: tensor([-0.6836, -0.4805,  1.9922,  0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1033])
attention_mask shape: torch.Size([4, 1033])
reward: tensor([-0.7148,  0.8594, -0.2754, -0.1045], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1880])
attention_mask shape: torch.Size([4, 1880])
reward: tensor([ 1.1172, -0.3516, -0.4395, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1918])
attention_mask shape: torch.Size([4, 1918])
reward: tensor([-0.4629, -0.6133, -1.9922,  0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.3516,  0.2051, -1.5703,  0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1379])
attention_mask shape: torch.Size([4, 1379])
reward: tensor([ 0.6914, -0.4570, -0.5430, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 911])
attention_mask shape: torch.Size([4, 911])
reward: tensor([ 2.0312,  0.2314, -0.8906, -0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1093])
attention_mask shape: torch.Size([4, 1093])
reward: tensor([-1.5234,  0.3652,  0.1396, -0.4395], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1698])
attention_mask shape: torch.Size([4, 1698])
reward: tensor([-0.3555,  0.9062, -0.8008, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1499])
attention_mask shape: torch.Size([4, 1499])
reward: tensor([ 0.0510,  0.7109,  0.1143, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1737])
attention_mask shape: torch.Size([4, 1737])
reward: tensor([ 0.6367,  0.1455, -1.3281,  0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.9883, -0.3340, -0.0334, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([ 0.6562, -0.8984,  0.9375, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1747])
attention_mask shape: torch.Size([4, 1747])
reward: tensor([-0.3457,  0.3242,  0.8828, -1.3750], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 425])
attention_mask shape: torch.Size([4, 425])
reward: tensor([ 1.2812, -0.4668, -0.3379, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1484])
attention_mask shape: torch.Size([4, 1484])
reward: tensor([ 0.6328, -0.8164, -1.7578, -1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1763])
attention_mask shape: torch.Size([4, 1763])
reward: tensor([-0.5898,  1.0391,  0.1426, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.8281,  0.4492, -1.9141, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1075])
attention_mask shape: torch.Size([4, 1075])
reward: tensor([-1.2031,  0.2402,  0.1338, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.0781, -0.2334,  0.6406, -1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2812,  0.3809, -0.3594,  0.2598], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1078])
attention_mask shape: torch.Size([4, 1078])
reward: tensor([-0.3203,  1.3672, -0.7109, -0.0776], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.6953, -0.6641,  0.8008, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1631])
attention_mask shape: torch.Size([4, 1631])
reward: tensor([ 0.2695, -0.3594, -0.7227,  0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1517])
attention_mask shape: torch.Size([4, 1517])
reward: tensor([ 0.9141, -1.5938,  0.3574, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([ 0.3438,  0.5352,  1.4141, -1.6641], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1868])
attention_mask shape: torch.Size([4, 1868])
reward: tensor([ 1.6641,  0.0278, -0.6836,  1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1480])
attention_mask shape: torch.Size([4, 1480])
reward: tensor([1.4453, 0.9766, 1.9844, 1.3594], device='cuda:0', dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([ 1.3125,  1.6797, -0.7070,  0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 648])
attention_mask shape: torch.Size([4, 648])
reward: tensor([-0.1426,  0.6484, -0.1484,  0.1689], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 769])
attention_mask shape: torch.Size([4, 769])
reward: tensor([-0.4453,  0.1123, -0.3027, -0.2354], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1692])
attention_mask shape: torch.Size([4, 1692])
reward: tensor([-0.6484,  2.0312, -0.0623,  1.4297], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1182])
attention_mask shape: torch.Size([4, 1182])
reward: tensor([ 1.1328e+00, -7.4219e-01, -3.5645e-02,  1.1139e-03], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 875])
attention_mask shape: torch.Size([4, 875])
reward: tensor([0.1177, 0.2852, 1.5391, 0.2773], device='cuda:0', dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3730,  0.6875, -0.6367,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1712])
attention_mask shape: torch.Size([4, 1712])
reward: tensor([-0.9961, -0.6992,  0.7852, -1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([-1.4141, -1.4375,  0.7812,  1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1566])
attention_mask shape: torch.Size([4, 1566])
reward: tensor([-1.2891, -0.7812,  0.8203, -0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.5469,  1.0781, -1.2969,  1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1459])
attention_mask shape: torch.Size([4, 1459])
reward: tensor([ 0.2754, -0.7227,  1.1875, -0.7539], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1869])
attention_mask shape: torch.Size([4, 1869])
reward: tensor([ 0.6133,  0.6328, -0.7188,  0.0388], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.7422,  0.5273, -0.9258, -0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-0.9062, -1.7969,  0.9961, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1723])
attention_mask shape: torch.Size([4, 1723])
reward: tensor([-0.4941,  0.2949,  0.0801, -0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1087])
attention_mask shape: torch.Size([4, 1087])
reward: tensor([-0.8047, -0.2002,  0.6133, -0.8398], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([ 0.0432, -0.4980,  1.0234, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1458])
attention_mask shape: torch.Size([4, 1458])
reward: tensor([-2.1250,  1.5859, -0.5195, -0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1931])
attention_mask shape: torch.Size([4, 1931])
reward: tensor([-0.8477, -1.6094, -0.7070, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1754])
attention_mask shape: torch.Size([4, 1754])
reward: tensor([-0.9766, -0.2441,  0.6328,  0.8828], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1683])
attention_mask shape: torch.Size([4, 1683])
reward: tensor([ 0.0122, -0.6836, -1.2188,  0.2109], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1362])
attention_mask shape: torch.Size([4, 1362])
reward: tensor([-0.6719, -0.7695, -0.9492,  0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1604])
attention_mask shape: torch.Size([4, 1604])
reward: tensor([ 0.8828, -0.3555,  1.6562, -1.7891], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 982])
attention_mask shape: torch.Size([4, 982])
reward: tensor([-0.9688,  0.6406, -1.5469,  0.2432], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1109])
attention_mask shape: torch.Size([4, 1109])
reward: tensor([-0.1157, -0.4453, -1.4922, -0.3770], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1419])
attention_mask shape: torch.Size([4, 1419])
reward: tensor([-0.3730,  0.3203, -0.6641,  1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1250])
attention_mask shape: torch.Size([4, 1250])
reward: tensor([1.2891, 0.2002, 1.7266, 0.3320], device='cuda:0', dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1776])
attention_mask shape: torch.Size([4, 1776])
reward: tensor([-1.3984, -0.5781,  0.5625,  0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2030])
attention_mask shape: torch.Size([4, 2030])
reward: tensor([ 1.6875, -0.0864, -1.0156, -0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1277])
attention_mask shape: torch.Size([4, 1277])
reward: tensor([ 1.1719, -0.5703, -1.0938,  0.8945], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1208])
attention_mask shape: torch.Size([4, 1208])
reward: tensor([-0.8359,  1.1719, -0.4707, -0.1582], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1362])
attention_mask shape: torch.Size([4, 1362])
reward: tensor([ 0.3770,  0.3867, -0.1177, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-1.1797, -0.0444, -1.4141, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1096])
attention_mask shape: torch.Size([4, 1096])
reward: tensor([ 0.7383,  0.4316, -0.1270,  0.3574], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1458])
attention_mask shape: torch.Size([4, 1458])
reward: tensor([ 1.7656, -0.3105, -1.1719, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1604])
attention_mask shape: torch.Size([4, 1604])
reward: tensor([ 0.5234,  0.4746, -0.5273, -0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1747])
attention_mask shape: torch.Size([4, 1747])
reward: tensor([-1.2734,  1.6328,  1.2812,  0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1430])
attention_mask shape: torch.Size([4, 1430])
reward: tensor([-1.1406, -0.4531,  0.0356, -1.3594], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1588])
attention_mask shape: torch.Size([4, 1588])
reward: tensor([ 1.4453,  0.4180, -0.3594, -1.3594], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1090])
attention_mask shape: torch.Size([4, 1090])
reward: tensor([ 2.1875,  0.0854,  0.7109, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.4707,  0.6797,  0.4844, -0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 865])
attention_mask shape: torch.Size([4, 865])
reward: tensor([0.1602, 0.8047, 0.0864, 0.1318], device='cuda:0', dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1592])
attention_mask shape: torch.Size([4, 1592])
reward: tensor([-0.6758, -0.7070,  0.5625,  0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1479])
attention_mask shape: torch.Size([4, 1479])
reward: tensor([-0.1484, -0.9688, -0.2812,  0.2197], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1252])
attention_mask shape: torch.Size([4, 1252])
reward: tensor([0.1055, 0.0776, 1.2422, 1.6094], device='cuda:0', dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7812, -0.8906,  0.5703, -0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.7969, -1.3281, -0.0089, -0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1203])
attention_mask shape: torch.Size([4, 1203])
reward: tensor([ 1.0234,  0.1621, -0.8906,  0.0522], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 830])
attention_mask shape: torch.Size([4, 830])
reward: tensor([-0.0801,  1.4766, -0.3027,  1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1907])
attention_mask shape: torch.Size([4, 1907])
reward: tensor([ 1.4844,  1.5469, -2.0156, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 668])
attention_mask shape: torch.Size([4, 668])
reward: tensor([ 1.2344, -1.4844,  1.4141,  0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1447])
attention_mask shape: torch.Size([4, 1447])
reward: tensor([-0.0045,  1.5312, -0.1602,  0.2129], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1689])
attention_mask shape: torch.Size([4, 1689])
reward: tensor([ 0.9219, -1.1250,  0.8008,  1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1248])
attention_mask shape: torch.Size([4, 1248])
reward: tensor([-0.3203,  1.4609, -0.8711, -1.4844], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1716])
attention_mask shape: torch.Size([4, 1716])
reward: tensor([-1.3359,  0.0864,  1.4062,  0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-1.5000,  0.1826,  2.3750,  0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1474])
attention_mask shape: torch.Size([4, 1474])
reward: tensor([ 0.0133, -1.8906, -0.1245, -1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1779])
attention_mask shape: torch.Size([4, 1779])
reward: tensor([ 0.0933, -0.0334, -0.3730, -0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1473])
attention_mask shape: torch.Size([4, 1473])
reward: tensor([-1.6094,  0.4844, -1.1719,  0.2441], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 524])
attention_mask shape: torch.Size([4, 524])
reward: tensor([ 0.1582, -0.9141,  0.6836,  0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1406])
attention_mask shape: torch.Size([4, 1406])
reward: tensor([ 0.4258, -0.6250, -0.5898,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1490])
attention_mask shape: torch.Size([4, 1490])
reward: tensor([ 1.4375, -0.5703, -0.1445,  0.2969], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([ 1.0547, -1.4375,  0.0623,  0.3184], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1850])
attention_mask shape: torch.Size([4, 1850])
reward: tensor([-0.7734,  0.2041, -0.1484,  0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1779])
attention_mask shape: torch.Size([4, 1779])
reward: tensor([-1.2344, -1.0859, -1.0938,  1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1023])
attention_mask shape: torch.Size([4, 1023])
reward: tensor([ 0.8047, -0.6172,  0.3809, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([-1.0391,  0.0000, -0.0466,  0.0311], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.0742, -2.1250,  1.4688, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1389])
attention_mask shape: torch.Size([4, 1389])
reward: tensor([ 0.2930,  0.2812, -0.1357,  0.1826], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 519])
attention_mask shape: torch.Size([4, 519])
reward: tensor([-0.5117, -0.6914, -1.1562,  0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1934])
attention_mask shape: torch.Size([4, 1934])
reward: tensor([ 0.1245, -0.7148, -0.0078,  1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 826])
attention_mask shape: torch.Size([4, 826])
reward: tensor([-0.6562, -0.5859, -1.7188,  0.2227], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1121])
attention_mask shape: torch.Size([4, 1121])
reward: tensor([-1.0156,  0.8672,  0.2412,  0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1273])
attention_mask shape: torch.Size([4, 1273])
reward: tensor([-0.5039, -0.0977,  0.6836,  0.0422], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1548])
attention_mask shape: torch.Size([4, 1548])
reward: tensor([-1.4766,  0.5938,  1.6172, -0.0532], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([-0.8633,  0.3438,  0.5859, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1358])
attention_mask shape: torch.Size([4, 1358])
reward: tensor([-0.3965, -0.0820, -0.5547, -1.7188], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.2969, -0.2930,  0.6055, -1.4844], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1611])
attention_mask shape: torch.Size([4, 1611])
reward: tensor([-0.8711, -1.2031,  0.7109,  0.1357], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1553])
attention_mask shape: torch.Size([4, 1553])
reward: tensor([ 0.6406, -1.3594, -1.3438, -0.1602], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1522])
attention_mask shape: torch.Size([4, 1522])
reward: tensor([ 0.4746,  1.3047,  1.5391, -0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1641])
attention_mask shape: torch.Size([4, 1641])
reward: tensor([-0.0557, -1.1172, -0.2129, -0.2617], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([-0.4570, -1.2891, -1.6953, -0.2578], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 676])
attention_mask shape: torch.Size([4, 676])
reward: tensor([-0.5117,  0.9414, -1.0781, -0.2373], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1868])
attention_mask shape: torch.Size([4, 1868])
reward: tensor([-0.6719, -0.1021, -1.2031,  1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1917])
attention_mask shape: torch.Size([4, 1917])
reward: tensor([-0.8164, -0.0111, -0.8711, -0.2109], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.4941, -0.7852, -1.2891,  0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1779])
attention_mask shape: torch.Size([4, 1779])
reward: tensor([ 0.4746,  0.5781,  1.1562, -0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1715])
attention_mask shape: torch.Size([4, 1715])
reward: tensor([-0.7422,  0.1602,  0.1309, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1594])
attention_mask shape: torch.Size([4, 1594])
reward: tensor([-0.1338, -1.5703, -0.4492, -1.8516], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1493])
attention_mask shape: torch.Size([4, 1493])
reward: tensor([ 0.1079, -0.3066, -0.7734, -1.9141], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1136])
attention_mask shape: torch.Size([4, 1136])
reward: tensor([-0.5195, -0.1758, -0.3164,  1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 523])
attention_mask shape: torch.Size([4, 523])
reward: tensor([ 0.1963,  0.2617, -0.4043,  1.0000], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7773, -2.1094, -0.4531, -0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1777])
attention_mask shape: torch.Size([4, 1777])
reward: tensor([-0.9062, -1.3984,  0.2021, -0.1553], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1856])
attention_mask shape: torch.Size([4, 1856])
reward: tensor([ 1.8047, -1.5938, -1.4141, -0.0200], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1066])
attention_mask shape: torch.Size([4, 1066])
reward: tensor([-0.4219,  0.6562,  0.3398, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1320])
attention_mask shape: torch.Size([4, 1320])
reward: tensor([-0.3418, -0.1885, -0.9414, -0.7617], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1251])
attention_mask shape: torch.Size([4, 1251])
reward: tensor([-0.5156, -0.8398, -0.3418,  0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1430])
attention_mask shape: torch.Size([4, 1430])
reward: tensor([-0.0222, -0.6094,  1.3828,  1.7891], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1633])
attention_mask shape: torch.Size([4, 1633])
reward: tensor([ 1.9531,  0.3008, -0.4668,  0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1090])
attention_mask shape: torch.Size([4, 1090])
reward: tensor([-0.4453, -1.4375,  0.2656, -0.0400], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 995])
attention_mask shape: torch.Size([4, 995])
reward: tensor([-0.1777,  0.6914, -1.6016, -0.0378], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1870])
attention_mask shape: torch.Size([4, 1870])
reward: tensor([ 0.3105, -1.1875, -0.5938, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1113])
attention_mask shape: torch.Size([4, 1113])
reward: tensor([-0.4629, -0.2871,  0.7031,  0.1777], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1497])
attention_mask shape: torch.Size([4, 1497])
reward: tensor([-0.0645,  0.4141, -1.5469, -0.2041], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([ 1.5234, -0.3242,  0.8789, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([-0.1201,  0.8438, -1.5078,  0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([-0.2520, -1.9844, -0.3340,  1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-0.9336,  0.3906,  0.0767, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-1.2422,  2.2500, -0.8906, -1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1003])
attention_mask shape: torch.Size([4, 1003])
reward: tensor([-1.0703, -0.1426, -0.8164,  0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1125])
attention_mask shape: torch.Size([4, 1125])
reward: tensor([ 0.8594, -1.7500,  1.4141,  0.4023], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 723])
attention_mask shape: torch.Size([4, 723])
reward: tensor([ 0.4395, -0.3066, -0.2314, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1544])
attention_mask shape: torch.Size([4, 1544])
reward: tensor([-0.4316, -0.5586, -1.2656,  1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1604])
attention_mask shape: torch.Size([4, 1604])
reward: tensor([-0.8711, -1.2422,  0.3379,  1.6016], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.0078,  0.0322, -0.2910,  0.1797], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1369])
attention_mask shape: torch.Size([4, 1369])
reward: tensor([0.5312, 1.3672, 0.4395, 0.2393], device='cuda:0', dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1194])
attention_mask shape: torch.Size([4, 1194])
reward: tensor([-0.7188,  1.7969, -1.4609, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1037])
attention_mask shape: torch.Size([4, 1037])
reward: tensor([ 0.2734, -1.4219,  0.5117,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 884])
attention_mask shape: torch.Size([4, 884])
reward: tensor([0.0033, 0.6562, 1.2109, 0.9258], device='cuda:0', dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1898])
attention_mask shape: torch.Size([4, 1898])
reward: tensor([-0.1338, -1.6328, -0.1377, -1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 987])
attention_mask shape: torch.Size([4, 987])
reward: tensor([ 0.8477, -0.6172, -0.2227, -0.0133], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.6133, -2.0781,  0.4492, -1.5234], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1390])
attention_mask shape: torch.Size([4, 1390])
reward: tensor([ 1.2266, -0.8711,  0.7344, -0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 654])
attention_mask shape: torch.Size([4, 654])
reward: tensor([ 0.9062, -0.1357, -0.7656,  0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.2969, -0.7109,  1.0703, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1394])
attention_mask shape: torch.Size([4, 1394])
reward: tensor([ 1.2109, -0.4531,  0.5898,  0.1133], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1857])
attention_mask shape: torch.Size([4, 1857])
reward: tensor([-1.7891,  0.5195, -1.8516, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1543])
attention_mask shape: torch.Size([4, 1543])
reward: tensor([ 0.0579,  0.0457,  1.8047, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 599])
attention_mask shape: torch.Size([4, 599])
reward: tensor([-0.7930, -0.9688, -0.5430,  0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1680])
attention_mask shape: torch.Size([4, 1680])
reward: tensor([ 0.6680, -1.7656, -1.9141,  1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([ 0.0977,  0.2373, -0.0776,  0.7617], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1698])
attention_mask shape: torch.Size([4, 1698])
reward: tensor([-0.2793, -0.6523,  1.5078, -1.2656], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.2471, -1.7969, -0.0957, -0.3906], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.1270,  1.2656,  0.6797, -1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1651])
attention_mask shape: torch.Size([4, 1651])
reward: tensor([-0.4180,  1.1016,  0.6367, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.0820, -1.1172, -0.4258, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 994])
attention_mask shape: torch.Size([4, 994])
reward: tensor([ 0.4082,  0.9727, -1.0156, -0.0557], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 973])
attention_mask shape: torch.Size([4, 973])
reward: tensor([ 0.3691, -0.5742, -0.7734, -0.3516], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3066, -0.0713, -1.9453, -0.0244], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([ 1.6719,  0.2539, -1.7188, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.9336, -0.8164,  1.4688,  0.0388], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1908])
attention_mask shape: torch.Size([4, 1908])
reward: tensor([ 0.3262,  1.5312,  0.5898, -0.2871], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1335])
attention_mask shape: torch.Size([4, 1335])
reward: tensor([ 0.2471, -0.4883,  0.4473, -0.6328], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.2598, -0.7500, -0.8086,  1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1837])
attention_mask shape: torch.Size([4, 1837])
reward: tensor([ 0.7852,  0.2031,  0.7461, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 2.0312,  0.3457,  0.2480, -0.0645], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1088])
attention_mask shape: torch.Size([4, 1088])
reward: tensor([-0.4531, -0.1670,  1.3125,  0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.8789,  0.5117,  1.4844, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7930, -1.6953, -0.1064, -0.5508], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-0.8438, -1.1484, -0.5234, -0.0400], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1755])
attention_mask shape: torch.Size([4, 1755])
reward: tensor([0.6484, 0.1846, 0.4980, 1.2812], device='cuda:0', dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 976])
attention_mask shape: torch.Size([4, 976])
reward: tensor([-0.8594, -1.6406,  0.8711,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1818])
attention_mask shape: torch.Size([4, 1818])
reward: tensor([-0.6172, -0.9414, -1.3047,  0.0635], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 846])
attention_mask shape: torch.Size([4, 846])
reward: tensor([-0.1201, -0.4180,  0.0623, -0.1826], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1852])
attention_mask shape: torch.Size([4, 1852])
reward: tensor([-2.1094,  2.2344,  0.6133, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1684])
attention_mask shape: torch.Size([4, 1684])
reward: tensor([-0.7227, -0.0266,  0.4102,  0.5820], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1536])
attention_mask shape: torch.Size([4, 1536])
reward: tensor([-0.6914, -0.5469, -0.3418,  0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([-0.0713, -1.7500, -1.2812,  0.9297], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-0.0532, -0.1396, -0.4883,  0.0913], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 967])
attention_mask shape: torch.Size([4, 967])
reward: tensor([ 0.6484, -2.0156, -0.0444,  0.0879], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1575])
attention_mask shape: torch.Size([4, 1575])
reward: tensor([ 0.8906,  0.8516, -2.1406,  0.2412], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 541])
attention_mask shape: torch.Size([4, 541])
reward: tensor([-0.3555,  0.2012, -0.5781, -0.6250], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1249])
attention_mask shape: torch.Size([4, 1249])
reward: tensor([-0.2734, -0.5234, -0.3516,  0.1846], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1879])
attention_mask shape: torch.Size([4, 1879])
reward: tensor([ 0.4395, -0.6055,  0.4355, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1632])
attention_mask shape: torch.Size([4, 1632])
reward: tensor([ 0.9727,  0.6914, -1.8203,  0.9648], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1563])
attention_mask shape: torch.Size([4, 1563])
reward: tensor([-1.8047, -1.0547,  1.6875,  0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1283])
attention_mask shape: torch.Size([4, 1283])
reward: tensor([ 0.1826,  0.2812, -1.0781, -1.2891], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.8086, -0.2227, -1.2812, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-1.2656,  0.2852,  1.4297,  0.3984], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 805])
attention_mask shape: torch.Size([4, 805])
reward: tensor([-0.4883, -1.3359, -0.5117,  0.4785], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.0156,  1.0156,  0.0566,  0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1718])
attention_mask shape: torch.Size([4, 1718])
reward: tensor([ 1.0781,  1.5938,  0.3926, -0.4492], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([ 0.3398, -0.8320,  2.2500, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1134])
attention_mask shape: torch.Size([4, 1134])
reward: tensor([-1.6797,  1.6562,  1.0156, -1.9922], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1412])
attention_mask shape: torch.Size([4, 1412])
reward: tensor([ 0.7734,  0.6133, -0.4141,  0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 793])
attention_mask shape: torch.Size([4, 793])
reward: tensor([ 0.0854,  0.1982,  0.9453, -0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1175])
attention_mask shape: torch.Size([4, 1175])
reward: tensor([ 0.9375, -1.1719, -1.2422,  1.4922], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1445])
attention_mask shape: torch.Size([4, 1445])
reward: tensor([ 0.3223, -0.3516, -2.1250, -0.1582], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.8203, -0.6758, -0.5742,  1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1235])
attention_mask shape: torch.Size([4, 1235])
reward: tensor([-0.1621,  0.5586, -0.8398, -1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1080])
attention_mask shape: torch.Size([4, 1080])
reward: tensor([-0.3594, -0.7031, -1.6250, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1545])
attention_mask shape: torch.Size([4, 1545])
reward: tensor([ 1.6094, -0.9766,  0.0378,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1473])
attention_mask shape: torch.Size([4, 1473])
reward: tensor([ 1.8516, -1.1484,  0.0145,  0.9922], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 659])
attention_mask shape: torch.Size([4, 659])
reward: tensor([ 0.0334, -1.0469, -0.0089, -0.0356], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1954])
attention_mask shape: torch.Size([4, 1954])
reward: tensor([-0.3555,  0.3867, -0.3457,  0.0610], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1608])
attention_mask shape: torch.Size([4, 1608])
reward: tensor([-0.1709, -1.3594, -1.2891,  0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1352])
attention_mask shape: torch.Size([4, 1352])
reward: tensor([ 0.9258,  0.5352,  0.2773, -0.3691], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1455])
attention_mask shape: torch.Size([4, 1455])
reward: tensor([ 0.2480, -0.2578, -0.0579,  0.2852], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1673])
attention_mask shape: torch.Size([4, 1673])
reward: tensor([ 1.0156, -0.7500,  1.2969,  0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1542])
attention_mask shape: torch.Size([4, 1542])
reward: tensor([ 1.3984, -1.4922, -0.1934,  1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1013])
attention_mask shape: torch.Size([4, 1013])
reward: tensor([-0.2490,  0.1484,  1.0859,  0.9023], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.9023, -2.2031, -0.0244,  0.1143], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1307])
attention_mask shape: torch.Size([4, 1307])
reward: tensor([ 0.2178, -1.1406, -1.5234, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6367,  0.1279, -0.9141, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1517])
attention_mask shape: torch.Size([4, 1517])
reward: tensor([ 1.1016,  0.1680, -0.6016,  0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 975])
attention_mask shape: torch.Size([4, 975])
reward: tensor([-1.0391, -1.2266,  1.0391,  0.0645], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1343])
attention_mask shape: torch.Size([4, 1343])
reward: tensor([ 0.1982,  2.0156,  0.6562, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7188, -2.2031,  0.5391,  0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1731])
attention_mask shape: torch.Size([4, 1731])
reward: tensor([-0.4258, -0.8359,  0.6875, -1.2031], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
[2024-10-21 23:41:50,306] [INFO] [launch.py:351:main] Process 604219 exits successfully.
sequences shape: torch.Size([4, 1722])
attention_mask shape: torch.Size([4, 1722])
reward: tensor([ 2.0469,  0.4805, -1.5000,  0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7656, -1.1484,  1.9922, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 792])
attention_mask shape: torch.Size([4, 792])
reward: tensor([-1.2109, -0.1953, -1.2344,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1520])
attention_mask shape: torch.Size([4, 1520])
reward: tensor([-0.4844, -1.1484,  0.5859,  0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
[2024-10-21 23:42:50,367] [INFO] [launch.py:351:main] Process 604218 exits successfully.
sequences shape: torch.Size([4, 1615])
attention_mask shape: torch.Size([4, 1615])
reward: tensor([-0.8320, -0.6133, -0.9414, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1987])
attention_mask shape: torch.Size([4, 1987])
reward: tensor([-0.3027, -0.6836, -1.3125,  0.0178], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
[2024-10-21 23:43:35,411] [INFO] [launch.py:351:main] Process 604217 exits successfully.
sequences shape: torch.Size([4, 1813])
attention_mask shape: torch.Size([4, 1813])
reward: tensor([ 0.6797, -0.8398, -0.6523,  0.2285], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 911])
attention_mask shape: torch.Size([4, 911])
reward: tensor([ 0.4551,  1.5391, -0.4082,  0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1532])
attention_mask shape: torch.Size([4, 1532])
reward: tensor([ 0.5078,  1.2266,  0.2715, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1135])
attention_mask shape: torch.Size([4, 1135])
reward: tensor([-0.4570, -0.1045, -0.7773, -1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 871])
attention_mask shape: torch.Size([4, 871])
reward: tensor([ 0.1133, -1.0234,  1.2969, -1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.7422,  1.3281,  1.8594, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-0.5820,  0.2148,  1.7578,  0.8750], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-21 23:45:29,527] [INFO] [launch.py:351:main] Process 604216 exits successfully.
+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-1th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-1th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_1 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-21 23:45:33,680] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 23:45:35,603] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-21 23:45:35,603] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-1th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_1 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-21 23:45:37,615] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 23:45:39,516] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-21 23:45:39,516] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-21 23:45:39,516] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-21 23:45:39,516] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-21 23:45:39,516] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-21 23:45:39,516] [INFO] [launch.py:256:main] process 605465 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 23:45:39,517] [INFO] [launch.py:256:main] process 605466 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 23:45:39,517] [INFO] [launch.py:256:main] process 605467 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 23:45:39,517] [INFO] [launch.py:256:main] process 605468 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-21 23:45:41,315] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 23:45:41,342] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 23:45:41,373] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-21 23:45:41,381] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-21 23:45:44,929] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-21 23:45:44,929] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-21 23:45:45,760] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-21 23:45:45,779] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-21 23:45:45,780] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.24it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.24it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.26it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.50it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.40it/s]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.52it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.52it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.37it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.52it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.53it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.58it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.71it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.57it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  6.00it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.20it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.97it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.72it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.65it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.45it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.16it/s]
[2024-10-21 23:45:57,975] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 23:46:07,981] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 23:46:07,997] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-21 23:46:08,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-21 23:46:08,036] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-21 23:46:08,037] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 23:46:08,831] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-21 23:46:08,832] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-21 23:46:08,833] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 23:46:08,833] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 23:46:08,833] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 23:46:08,978] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-21 23:46:08,979] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-21 23:46:08,979] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.89 GB, percent = 3.6%
[2024-10-21 23:46:09,120] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-21 23:46:09,121] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-21 23:46:09,121] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.91 GB, percent = 3.6%
[2024-10-21 23:46:09,122] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-21 23:46:09,122] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-21 23:46:09,122] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-21 23:46:09,122] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-21 23:46:09,122] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc349d3d0c0>
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-21 23:46:09,123] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-21 23:46:09,124] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-21 23:46:09,124] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-21 23:46:09,124] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-21 23:46:09,124] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-21 23:46:09,125] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-21 23:46:13,138] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-21 23:46:13,140] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-21 23:46:13,334] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-21 23:46:13,335] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-21 23:46:13,335] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.78 GB, percent = 3.6%
[2024-10-21 23:46:13,464] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-21 23:46:13,464] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-21 23:46:13,465] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.78 GB, percent = 3.6%
[2024-10-21 23:46:13,466] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-21 23:46:13,466] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-21 23:46:13,466] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-21 23:46:13,466] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-21 23:46:13,466] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-21 23:46:13,466] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-21 23:46:13,466] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-21 23:46:13,466] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-21 23:46:13,466] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-21 23:46:13,466] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc34812a9b0>
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-21 23:46:13,467] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-21 23:46:13,468] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-21 23:46:13,468] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 638/100000 [00:00<00:15, 6377.38it/s]Preprocessing data:   2%|█▌                                                                                          | 1656/100000 [00:00<00:11, 8612.93it/s]Preprocessing data:   3%|██▍                                                                                         | 2679/100000 [00:00<00:10, 9347.62it/s]Preprocessing data:   4%|███▍                                                                                        | 3697/100000 [00:00<00:09, 9672.87it/s]Preprocessing data:   5%|████▎                                                                                       | 4723/100000 [00:00<00:09, 9883.92it/s]Preprocessing data:   6%|█████▏                                                                                     | 5749/100000 [00:00<00:09, 10008.84it/s]Preprocessing data:   7%|██████▏                                                                                    | 6772/100000 [00:00<00:09, 10079.52it/s]Preprocessing data:   8%|███████                                                                                    | 7801/100000 [00:00<00:09, 10145.63it/s]Preprocessing data:   9%|████████                                                                                   | 8820/100000 [00:00<00:08, 10158.58it/s]Preprocessing data:  10%|████████▉                                                                                  | 9836/100000 [00:01<00:08, 10063.92it/s]Preprocessing data:  11%|█████████▊                                                                                | 10901/100000 [00:01<00:08, 10240.21it/s]Preprocessing data:  12%|██████████▊                                                                               | 11976/100000 [00:01<00:08, 10391.99it/s]Preprocessing data:  13%|███████████▋                                                                              | 13054/100000 [00:01<00:08, 10508.49it/s]Preprocessing data:  14%|████████████▋                                                                             | 14123/100000 [00:01<00:08, 10560.36it/s]Preprocessing data:  15%|█████████████▋                                                                            | 15199/100000 [00:01<00:07, 10617.55it/s]Preprocessing data:  16%|██████████████▋                                                                           | 16275/100000 [00:01<00:07, 10658.86it/s]Preprocessing data:  17%|███████████████▌                                                                          | 17341/100000 [00:01<00:07, 10653.98it/s]Preprocessing data:  18%|████████████████▌                                                                         | 18418/100000 [00:01<00:07, 10686.07it/s]Preprocessing data:  19%|█████████████████▌                                                                        | 19497/100000 [00:01<00:07, 10714.65it/s]Preprocessing data:  21%|██████████████████▌                                                                       | 20590/100000 [00:02<00:07, 10777.73it/s]Preprocessing data:  22%|███████████████████▌                                                                      | 21671/100000 [00:02<00:07, 10785.78it/s]Preprocessing data:  23%|████████████████████▍                                                                     | 22750/100000 [00:02<00:07, 10758.76it/s]Preprocessing data:  24%|█████████████████████▍                                                                    | 23826/100000 [00:02<00:07, 10723.20it/s]Preprocessing data:  25%|██████████████████████▍                                                                   | 24899/100000 [00:02<00:07, 10607.65it/s]Preprocessing data:  26%|███████████████████████▎                                                                  | 25961/100000 [00:02<00:06, 10587.16it/s]Preprocessing data:  27%|████████████████████████▎                                                                 | 27020/100000 [00:02<00:06, 10577.89it/s]Preprocessing data:  28%|█████████████████████████▎                                                                | 28078/100000 [00:02<00:06, 10559.89it/s]Preprocessing data:  29%|██████████████████████████▏                                                               | 29135/100000 [00:02<00:06, 10544.61it/s]Preprocessing data:  30%|███████████████████████████▏                                                              | 30190/100000 [00:02<00:06, 10536.97it/s]Preprocessing data:  31%|████████████████████████████                                                              | 31247/100000 [00:03<00:06, 10544.57it/s]Preprocessing data:  32%|█████████████████████████████                                                             | 32302/100000 [00:03<00:06, 10523.74it/s]Preprocessing data:  33%|██████████████████████████████                                                            | 33357/100000 [00:03<00:06, 10530.14it/s]Preprocessing data:  34%|██████████████████████████████▉                                                           | 34411/100000 [00:03<00:06, 10529.66it/s]Preprocessing data:  35%|███████████████████████████████▉                                                          | 35464/100000 [00:03<00:06, 10527.70it/s]Preprocessing data:  37%|████████████████████████████████▊                                                         | 36517/100000 [00:03<00:06, 10514.09it/s]Preprocessing data:  38%|█████████████████████████████████▊                                                        | 37572/100000 [00:03<00:05, 10524.75it/s]Preprocessing data:  39%|██████████████████████████████████▊                                                       | 38625/100000 [00:03<00:05, 10512.98it/s]Preprocessing data:  40%|███████████████████████████████████▋                                                      | 39677/100000 [00:03<00:05, 10498.85it/s]Preprocessing data:  41%|████████████████████████████████████▋                                                     | 40727/100000 [00:03<00:05, 10454.00it/s]Preprocessing data:  42%|█████████████████████████████████████▌                                                    | 41775/100000 [00:04<00:05, 10460.62it/s]Preprocessing data:  43%|██████████████████████████████████████▌                                                   | 42822/100000 [00:04<00:05, 10425.00it/s]Preprocessing data:  44%|███████████████████████████████████████▌                                                  | 43892/100000 [00:04<00:05, 10506.49it/s]Preprocessing data:  45%|████████████████████████████████████████▍                                                 | 44943/100000 [00:04<00:05, 10443.08it/s]Preprocessing data:  46%|█████████████████████████████████████████▍                                                | 46020/100000 [00:04<00:05, 10539.32it/s]Preprocessing data:  47%|██████████████████████████████████████████▍                                               | 47101/100000 [00:04<00:04, 10619.73it/s]Preprocessing data:  48%|███████████████████████████████████████████▎                                              | 48178/100000 [00:04<00:04, 10662.49it/s]Preprocessing data:  49%|████████████████████████████████████████████▎                                             | 49255/100000 [00:04<00:04, 10692.27it/s]Preprocessing data:  50%|█████████████████████████████████████████████▎                                            | 50332/100000 [00:04<00:04, 10713.42it/s]Preprocessing data:  51%|██████████████████████████████████████████████▎                                           | 51408/100000 [00:04<00:04, 10724.81it/s]Preprocessing data:  52%|███████████████████████████████████████████████▏                                          | 52481/100000 [00:05<00:04, 10658.90it/s]Preprocessing data:  54%|████████████████████████████████████████████████▏                                         | 53548/100000 [00:05<00:04, 10649.99it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▏                                        | 54625/100000 [00:05<00:04, 10684.38it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▏                                       | 55701/100000 [00:05<00:04, 10706.57it/s]Preprocessing data:  57%|███████████████████████████████████████████████████                                       | 56786/100000 [00:05<00:04, 10746.56it/s]Preprocessing data:  58%|████████████████████████████████████████████████████                                      | 57868/100000 [00:05<00:03, 10766.15it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████                                     | 58945/100000 [00:05<00:03, 10737.12it/s]Preprocessing data:  60%|██████████████████████████████████████████████████████                                    | 60022/100000 [00:05<00:03, 10746.40it/s]Preprocessing data:  61%|██████████████████████████████████████████████████████▉                                   | 61104/100000 [00:05<00:03, 10766.41it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▉                                  | 62181/100000 [00:05<00:03, 10763.78it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▉                                 | 63260/100000 [00:06<00:03, 10770.52it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▉                                | 64338/100000 [00:06<00:03, 10750.87it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▊                               | 65414/100000 [00:06<00:03, 10735.59it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▊                              | 66488/100000 [00:06<00:03, 10514.62it/s]Preprocessing data:  68%|████████████████████████████████████████████████████████████▊                             | 67541/100000 [00:06<00:03, 10395.79it/s]Preprocessing data:  69%|█████████████████████████████████████████████████████████████▋                            | 68604/100000 [00:06<00:03, 10463.02it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▋                           | 69667/100000 [00:06<00:02, 10511.27it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▋                          | 70727/100000 [00:06<00:02, 10537.47it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▌                         | 71782/100000 [00:06<00:02, 10526.72it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▌                        | 72835/100000 [00:06<00:02, 10121.68it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▏                       | 73851/100000 [00:07<00:02, 9021.05it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████                       | 74776/100000 [00:07<00:03, 8265.14it/s]Preprocessing data:  76%|████████████████████████████████████████████████████████████████████▊                      | 75627/100000 [00:07<00:03, 7867.43it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████▌                     | 76431/100000 [00:07<00:03, 7618.51it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▎                    | 77204/100000 [00:07<00:03, 7441.76it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▉                    | 77955/100000 [00:07<00:02, 7375.61it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▌                   | 78697/100000 [00:07<00:02, 7262.79it/s]Preprocessing data:  79%|████████████████████████████████████████████████████████████████████████▎                  | 79426/100000 [00:07<00:02, 7223.66it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▉                  | 80150/100000 [00:08<00:02, 6975.79it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▋                 | 80935/100000 [00:08<00:02, 7220.16it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▌                | 81934/100000 [00:08<00:02, 8010.75it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▎               | 82741/100000 [00:08<00:02, 7742.42it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████               | 83521/100000 [00:08<00:02, 7718.45it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▋              | 84297/100000 [00:08<00:02, 7660.23it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▌             | 85275/100000 [00:08<00:01, 8274.35it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▎            | 86113/100000 [00:08<00:01, 8303.66it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████            | 86947/100000 [00:08<00:01, 7962.52it/s]Preprocessing data:  88%|███████████████████████████████████████████████████████████████████████████████▉           | 87878/100000 [00:08<00:01, 8349.14it/s]Preprocessing data:  89%|████████████████████████████████████████████████████████████████████████████████▉          | 88874/100000 [00:09<00:01, 8816.99it/s]Preprocessing data:  90%|█████████████████████████████████████████████████████████████████████████████████▊         | 89868/100000 [00:09<00:01, 9145.91it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▌        | 90787/100000 [00:09<00:01, 8477.44it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▍       | 91647/100000 [00:09<00:01, 8083.97it/s]Preprocessing data:  92%|████████████████████████████████████████████████████████████████████████████████████▏      | 92466/100000 [00:09<00:00, 7814.88it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▊      | 93256/100000 [00:09<00:00, 7684.17it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▌     | 94030/100000 [00:09<00:00, 7609.96it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▍    | 94920/100000 [00:09<00:00, 7972.60it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▏   | 95825/100000 [00:09<00:00, 8279.48it/s]Preprocessing data:  97%|███████████████████████████████████████████████████████████████████████████████████████▉   | 96658/100000 [00:10<00:00, 8110.91it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▋  | 97476/100000 [00:10<00:00, 8130.31it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▍ | 98327/100000 [00:10<00:00, 8240.97it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████▎| 99258/100000 [00:10<00:00, 8553.96it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9620.77it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.2168, -1.5234, -1.9375, -0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.0859, -0.5234, -0.7695, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1144])
attention_mask shape: torch.Size([4, 1144])
reward: tensor([-1.9141, -0.8086, -1.6328, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 691])
attention_mask shape: torch.Size([4, 691])
reward: tensor([-1.1172, -1.8672, -0.6328,  1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1518])
attention_mask shape: torch.Size([4, 1518])
reward: tensor([ 0.3066,  0.2109,  0.0679, -0.2158], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1045])
attention_mask shape: torch.Size([4, 1045])
reward: tensor([ 1.6797, -1.1406, -0.8281,  0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1954])
attention_mask shape: torch.Size([4, 1954])
reward: tensor([-0.3203,  0.1689, -1.7266, -1.7266], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([ 0.4453, -0.9961,  0.9766,  0.1309], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 897])
attention_mask shape: torch.Size([4, 897])
reward: tensor([-1.0547,  0.1157, -0.4492, -0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1139])
attention_mask shape: torch.Size([4, 1139])
reward: tensor([-0.3027,  1.1562, -0.9766, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1619])
attention_mask shape: torch.Size([4, 1619])
reward: tensor([-1.4219,  0.0278, -1.0547,  0.0089], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1700])
attention_mask shape: torch.Size([4, 1700])
reward: tensor([ 0.7461, -1.4141, -0.4141,  0.2012], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1105])
attention_mask shape: torch.Size([4, 1105])
reward: tensor([-1.6641, -0.5820,  0.6211, -0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1564])
attention_mask shape: torch.Size([4, 1564])
reward: tensor([-0.3203, -0.1045,  1.7812, -0.1045], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 415])
attention_mask shape: torch.Size([4, 415])
reward: tensor([-0.6211, -0.4746, -0.1621,  0.0510], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1630])
attention_mask shape: torch.Size([4, 1630])
reward: tensor([-0.9766, -0.5703, -0.3242, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1261])
attention_mask shape: torch.Size([4, 1261])
reward: tensor([-1.4375, -1.7969, -0.5234,  1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1912])
attention_mask shape: torch.Size([4, 1912])
reward: tensor([ 0.1157,  0.0977, -0.3828, -0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 802])
attention_mask shape: torch.Size([4, 802])
reward: tensor([ 0.4492, -1.3125, -1.3359, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1225])
attention_mask shape: torch.Size([4, 1225])
reward: tensor([ 1.2031, -1.2031, -1.7500, -0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1678])
attention_mask shape: torch.Size([4, 1678])
reward: tensor([-2.1719, -0.4316,  0.2676, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 893])
attention_mask shape: torch.Size([4, 893])
reward: tensor([ 0.2676, -0.0444, -0.1729, -0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1499])
attention_mask shape: torch.Size([4, 1499])
reward: tensor([ 0.1318, -0.4629,  0.6016, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1737])
attention_mask shape: torch.Size([4, 1737])
reward: tensor([-1.8047,  0.0233, -1.3359,  1.6875], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1435])
attention_mask shape: torch.Size([4, 1435])
reward: tensor([-0.3066, -1.0859, -0.0977, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1568])
attention_mask shape: torch.Size([4, 1568])
reward: tensor([ 0.5156, -0.8164,  0.5156, -1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1798])
attention_mask shape: torch.Size([4, 1798])
reward: tensor([-0.6055, -0.5508,  0.3105, -1.2344], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 412])
attention_mask shape: torch.Size([4, 412])
reward: tensor([ 0.7930, -0.6992, -0.0688, -1.3047], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1361])
attention_mask shape: torch.Size([4, 1361])
reward: tensor([ 1.0391, -1.8672, -0.3066,  0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1562])
attention_mask shape: torch.Size([4, 1562])
reward: tensor([-1.0859,  1.3281, -0.1309, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1491])
attention_mask shape: torch.Size([4, 1491])
reward: tensor([-0.8164,  0.5156, -0.0311, -0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1587])
attention_mask shape: torch.Size([4, 1587])
reward: tensor([-2.0156,  0.1914, -1.1641, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1726])
attention_mask shape: torch.Size([4, 1726])
reward: tensor([-1.4062, -0.1689,  0.5547, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.4629, -0.1001, -0.8906, -1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 812])
attention_mask shape: torch.Size([4, 812])
reward: tensor([-0.9961, -0.8125, -1.1172, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.1094, -1.5312,  1.1875, -0.8008], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1212])
attention_mask shape: torch.Size([4, 1212])
reward: tensor([-0.2422, -1.2422, -0.8906, -1.8750], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1450])
attention_mask shape: torch.Size([4, 1450])
reward: tensor([ 0.9414, -1.5391, -1.4766, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 979])
attention_mask shape: torch.Size([4, 979])
reward: tensor([-1.7891,  0.8828,  0.4023, -1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.4922, -0.8633, -1.1484,  0.8359], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1777])
attention_mask shape: torch.Size([4, 1777])
reward: tensor([ 1.5547, -1.3984,  0.4453,  0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([ 1.6875,  1.5469, -1.2031,  0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 321])
attention_mask shape: torch.Size([4, 321])
reward: tensor([-0.8789, -1.4297, -0.6836, -0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 723])
attention_mask shape: torch.Size([4, 723])
reward: tensor([-1.5625,  0.0889, -0.8906, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2045])
attention_mask shape: torch.Size([4, 2045])
reward: tensor([-0.0200, -0.0957, -0.2793,  1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1028])
attention_mask shape: torch.Size([4, 1028])
reward: tensor([ 0.4531, -1.1172,  0.1543, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 797])
attention_mask shape: torch.Size([4, 797])
reward: tensor([-0.7500, -2.0938,  1.8125, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3691, -0.6055, -0.2334, -0.0444], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1552])
attention_mask shape: torch.Size([4, 1552])
reward: tensor([-1.2031, -0.4570,  0.6055, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([-1.0234, -1.4609,  0.1602,  1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1416])
attention_mask shape: torch.Size([4, 1416])
reward: tensor([-0.7852, -0.3828,  0.3691, -0.1729], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1201])
attention_mask shape: torch.Size([4, 1201])
reward: tensor([-1.2031,  1.5859, -1.8438,  0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1676])
attention_mask shape: torch.Size([4, 1676])
reward: tensor([-0.2246, -1.4297,  1.0312, -1.3750], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1021])
attention_mask shape: torch.Size([4, 1021])
reward: tensor([-1.3672, -1.8906,  0.2559, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1514])
attention_mask shape: torch.Size([4, 1514])
reward: tensor([ 0.1533,  0.1602,  0.8359, -0.0977], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 695])
attention_mask shape: torch.Size([4, 695])
reward: tensor([-0.4805, -0.8203, -1.6094, -1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1147])
attention_mask shape: torch.Size([4, 1147])
reward: tensor([-0.9336, -0.3828, -1.1641, -1.9219], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 557])
attention_mask shape: torch.Size([4, 557])
reward: tensor([-0.8203,  0.3867, -0.9141, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([ 0.7969, -1.0703, -1.1250, -0.0557], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.2031,  1.5547, -1.6172, -0.1064], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1167])
attention_mask shape: torch.Size([4, 1167])
reward: tensor([ 0.1211, -1.1797, -1.4766, -0.1089], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1570])
attention_mask shape: torch.Size([4, 1570])
reward: tensor([-0.0532, -1.1172, -0.4746, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1355])
attention_mask shape: torch.Size([4, 1355])
reward: tensor([ 0.0189,  0.0579, -0.9492,  0.2969], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 725])
attention_mask shape: torch.Size([4, 725])
reward: tensor([ 0.4062,  0.1245,  0.7930, -1.3594], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1825])
attention_mask shape: torch.Size([4, 1825])
reward: tensor([ 1.0391, -1.4609,  1.1875, -1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 837])
attention_mask shape: torch.Size([4, 837])
reward: tensor([-1.2656,  1.1875, -1.2500, -0.2617], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 952])
attention_mask shape: torch.Size([4, 952])
reward: tensor([-0.6484, -0.9141,  0.0879, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1528])
attention_mask shape: torch.Size([4, 1528])
reward: tensor([-1.4609, -0.6367, -0.0732,  1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 608])
attention_mask shape: torch.Size([4, 608])
reward: tensor([-0.2002,  0.0610,  0.1533, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1535])
attention_mask shape: torch.Size([4, 1535])
reward: tensor([-0.6719, -1.4766, -0.9141,  0.2734], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1767])
attention_mask shape: torch.Size([4, 1767])
reward: tensor([-1.1016, -0.2002, -0.4141,  0.9102], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1364])
attention_mask shape: torch.Size([4, 1364])
reward: tensor([-0.4746, -1.3594, -0.8281, -1.8984], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1115])
attention_mask shape: torch.Size([4, 1115])
reward: tensor([0.6094, 0.5469, 0.7461, 0.7461], device='cuda:0', dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([-0.7734,  1.2266, -0.1245,  0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-1.4375, -0.4180, -0.3457, -0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 841])
attention_mask shape: torch.Size([4, 841])
reward: tensor([ 0.2891,  0.1982, -1.4375, -1.5469], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1354])
attention_mask shape: torch.Size([4, 1354])
reward: tensor([ 1.8828, -1.0703,  0.6055, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1212])
attention_mask shape: torch.Size([4, 1212])
reward: tensor([ 0.7734, -0.7773, -0.7070, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1597])
attention_mask shape: torch.Size([4, 1597])
reward: tensor([-1.4766, -0.1133, -1.2266,  0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 461])
attention_mask shape: torch.Size([4, 461])
reward: tensor([ 0.1523, -2.1406, -1.7891, -1.3750], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1588])
attention_mask shape: torch.Size([4, 1588])
reward: tensor([-0.9336,  0.0977, -1.2266, -0.2021], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 467])
attention_mask shape: torch.Size([4, 467])
reward: tensor([ 0.1465,  0.2109,  0.7031, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1176])
attention_mask shape: torch.Size([4, 1176])
reward: tensor([-0.1177,  0.2539, -0.9883, -0.1089], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1053])
attention_mask shape: torch.Size([4, 1053])
reward: tensor([-0.0933, -0.4746, -0.7109,  0.2715], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1894])
attention_mask shape: torch.Size([4, 1894])
reward: tensor([ 0.3984, -0.3730,  0.0732, -1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.6172, -0.6758,  0.2480,  0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1057])
attention_mask shape: torch.Size([4, 1057])
reward: tensor([-0.9336, -0.6836, -0.6211,  0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2266,  0.9102, -0.4941, -0.2598], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1367])
attention_mask shape: torch.Size([4, 1367])
reward: tensor([ 0.1641, -0.9766,  0.6914, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 936])
attention_mask shape: torch.Size([4, 936])
reward: tensor([ 1.5625, -0.8164,  1.2109, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 663])
attention_mask shape: torch.Size([4, 663])
reward: tensor([-0.8320,  0.5352, -0.3906,  0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1537])
attention_mask shape: torch.Size([4, 1537])
reward: tensor([ 1.3359,  0.0645, -0.7305, -0.8359], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 585])
attention_mask shape: torch.Size([4, 585])
reward: tensor([0.8750, 0.0278, 1.3672, 0.6133], device='cuda:0', dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1617])
attention_mask shape: torch.Size([4, 1617])
reward: tensor([ 1.9844,  1.4141, -0.0033,  0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1705])
attention_mask shape: torch.Size([4, 1705])
reward: tensor([ 1.0156, -1.1250,  0.2402, -1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 494])
attention_mask shape: torch.Size([4, 494])
reward: tensor([-0.4980, -1.6953, -0.8398, -0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1494])
attention_mask shape: torch.Size([4, 1494])
reward: tensor([-0.6367, -0.3906,  1.1562, -1.1719], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1055])
attention_mask shape: torch.Size([4, 1055])
reward: tensor([-0.3281,  0.7227,  1.9141,  0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 740])
attention_mask shape: torch.Size([4, 740])
reward: tensor([-0.4141, -0.8438,  0.4141, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1404])
attention_mask shape: torch.Size([4, 1404])
reward: tensor([-2.1406, -0.0334,  1.5703, -1.8203], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1086])
attention_mask shape: torch.Size([4, 1086])
reward: tensor([-1.4375, -1.3750, -0.6992, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 747])
attention_mask shape: torch.Size([4, 747])
reward: tensor([-1.2188, -0.9062, -1.1016, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1085])
attention_mask shape: torch.Size([4, 1085])
reward: tensor([ 0.2617, -0.6484, -0.2246, -0.0289], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1456])
attention_mask shape: torch.Size([4, 1456])
reward: tensor([ 0.6758, -1.5234, -0.1089,  1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([ 0.6445, -0.5859,  1.2266, -0.0356], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1430])
attention_mask shape: torch.Size([4, 1430])
reward: tensor([-0.7188, -0.8594,  0.3086,  1.6875], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1181])
attention_mask shape: torch.Size([4, 1181])
reward: tensor([-1.7422,  0.2412, -1.5625,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1215])
attention_mask shape: torch.Size([4, 1215])
reward: tensor([ 0.8633, -1.4766, -0.8086, -0.7539], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 819])
attention_mask shape: torch.Size([4, 819])
reward: tensor([-1.0703, -0.5156,  0.1475, -1.7500], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.8320, -1.0391,  0.8281,  0.0977], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1744])
attention_mask shape: torch.Size([4, 1744])
reward: tensor([-1.2500,  0.1133,  0.5938, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 363])
attention_mask shape: torch.Size([4, 363])
reward: tensor([ 0.9766, -0.6680, -0.6094, -1.9141], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7070, -1.0391, -1.7812,  1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 729])
attention_mask shape: torch.Size([4, 729])
reward: tensor([-0.6523, -0.0645, -1.6172,  0.4453], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1913])
attention_mask shape: torch.Size([4, 1913])
reward: tensor([-0.3105, -1.4688, -0.2266,  0.1387], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-0.1133, -0.2695,  0.9883,  0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1398])
attention_mask shape: torch.Size([4, 1398])
reward: tensor([-1.5234,  0.1123,  0.9102, -0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1380])
attention_mask shape: torch.Size([4, 1380])
reward: tensor([ 0.7695,  0.0145,  0.7383, -0.6719], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 818])
attention_mask shape: torch.Size([4, 818])
reward: tensor([ 0.0500, -0.6641,  0.7617, -0.6367], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1586])
attention_mask shape: torch.Size([4, 1586])
reward: tensor([-1.3438, -0.3203,  0.0713, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1184])
attention_mask shape: torch.Size([4, 1184])
reward: tensor([-0.6211, -0.7031,  0.3594, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.9336, -1.2812, -1.9219, -0.2578], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1160])
attention_mask shape: torch.Size([4, 1160])
reward: tensor([-0.2158, -0.3418, -0.1582, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1068])
attention_mask shape: torch.Size([4, 1068])
reward: tensor([-0.4746,  0.7344, -0.5508, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([-1.1172, -0.7305, -1.7031, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 671])
attention_mask shape: torch.Size([4, 671])
reward: tensor([-0.3379,  1.0703,  0.4629, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1651])
attention_mask shape: torch.Size([4, 1651])
reward: tensor([-0.1426, -0.2812, -0.4668, -1.3281], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1598])
attention_mask shape: torch.Size([4, 1598])
reward: tensor([-0.8984, -1.0312,  0.3340, -0.1001], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.0078, -0.1045, -1.9844,  0.1826], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1652])
attention_mask shape: torch.Size([4, 1652])
reward: tensor([-0.1045,  0.8398, -0.0776, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.0156,  0.1729, -0.1064, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1594])
attention_mask shape: torch.Size([4, 1594])
reward: tensor([ 0.0579, -1.8594, -0.9766, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.1426, -1.9297, -2.0156, -1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 725])
attention_mask shape: torch.Size([4, 725])
reward: tensor([ 0.5938, -0.4395, -0.4258, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 632])
attention_mask shape: torch.Size([4, 632])
reward: tensor([ 0.3340,  0.5352, -0.3516,  1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2500, -2.1562,  0.8516,  0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1313])
attention_mask shape: torch.Size([4, 1313])
reward: tensor([-1.3516, -1.2344, -1.0547, -0.6445], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1856])
attention_mask shape: torch.Size([4, 1856])
reward: tensor([ 0.5781, -0.8633,  0.5273, -1.4766], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 642])
attention_mask shape: torch.Size([4, 642])
reward: tensor([-0.9961, -0.6406, -0.2930,  0.1729], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1307])
attention_mask shape: torch.Size([4, 1307])
reward: tensor([ 0.2520, -1.7969, -0.8906, -1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.7539, -0.0133, -1.3047, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1408])
attention_mask shape: torch.Size([4, 1408])
reward: tensor([-1.5078, -0.2158,  1.6875, -1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1633])
attention_mask shape: torch.Size([4, 1633])
reward: tensor([-0.8203, -0.3027, -2.0938,  0.2314], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 895])
attention_mask shape: torch.Size([4, 895])
reward: tensor([ 0.5117, -1.3359,  0.4961, -0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 886])
attention_mask shape: torch.Size([4, 886])
reward: tensor([-1.4141, -0.2969, -0.4043,  0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1329])
attention_mask shape: torch.Size([4, 1329])
reward: tensor([ 0.2969, -0.8086, -0.5820, -0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 816])
attention_mask shape: torch.Size([4, 816])
reward: tensor([-1.0781,  0.4844, -0.2041, -0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([ 0.7070,  1.1719, -1.5000, -0.1426], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 701])
attention_mask shape: torch.Size([4, 701])
reward: tensor([ 1.3438, -0.6328, -1.2891, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([-0.9414,  0.3086, -0.1602, -0.1797], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1024])
attention_mask shape: torch.Size([4, 1024])
reward: tensor([-0.1514, -1.3125, -0.7383,  1.3984], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 807])
attention_mask shape: torch.Size([4, 807])
reward: tensor([-1.1172, -0.5117,  0.3867, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1281])
attention_mask shape: torch.Size([4, 1281])
reward: tensor([-1.3594,  1.8828, -0.7109, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1014])
attention_mask shape: torch.Size([4, 1014])
reward: tensor([-0.3164,  0.6055, -1.9297,  0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 752])
attention_mask shape: torch.Size([4, 752])
reward: tensor([-0.2754, -1.7891, -0.1641, -0.6445], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 657])
attention_mask shape: torch.Size([4, 657])
reward: tensor([-0.2354, -1.4453, -0.4141,  1.1328], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1098])
attention_mask shape: torch.Size([4, 1098])
reward: tensor([ 0.1846, -0.7148, -1.2500,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1531])
attention_mask shape: torch.Size([4, 1531])
reward: tensor([-1.0312, -0.5156,  1.0469,  1.5938], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.8984, -1.0391, -0.8477, -0.7227], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1268])
attention_mask shape: torch.Size([4, 1268])
reward: tensor([-1.3750,  0.8594, -0.4141, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 554])
attention_mask shape: torch.Size([4, 554])
reward: tensor([-0.5430,  0.6133, -0.3730, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1091])
attention_mask shape: torch.Size([4, 1091])
reward: tensor([ 0.8086, -0.7656, -0.1826, -0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 820])
attention_mask shape: torch.Size([4, 820])
reward: tensor([ 0.2539,  0.6211, -0.2969, -0.7773], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1636])
attention_mask shape: torch.Size([4, 1636])
reward: tensor([-1.7969, -0.4043, -1.1016, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1164])
attention_mask shape: torch.Size([4, 1164])
reward: tensor([-2.0469, -0.1982, -1.7422,  0.1777], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1296])
attention_mask shape: torch.Size([4, 1296])
reward: tensor([ 0.6094, -0.5625, -0.6680, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1308])
attention_mask shape: torch.Size([4, 1308])
reward: tensor([ 1.2266, -0.7930, -0.3379,  0.2422], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 287])
attention_mask shape: torch.Size([4, 287])
reward: tensor([ 0.0610, -1.6875, -0.6172, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1155])
attention_mask shape: torch.Size([4, 1155])
reward: tensor([ 2.2500, -1.9453,  0.3574, -0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1007])
attention_mask shape: torch.Size([4, 1007])
reward: tensor([ 1.0391, -0.7812, -0.2178, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1383])
attention_mask shape: torch.Size([4, 1383])
reward: tensor([-0.2852, -0.2520, -0.9883,  0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1528])
attention_mask shape: torch.Size([4, 1528])
reward: tensor([ 1.2656, -0.2969,  1.2109, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 741])
attention_mask shape: torch.Size([4, 741])
reward: tensor([-1.1641, -1.8516, -0.6562,  2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1482])
attention_mask shape: torch.Size([4, 1482])
reward: tensor([-1.2422, -1.0391, -1.2266,  0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1224])
attention_mask shape: torch.Size([4, 1224])
reward: tensor([-2.1250,  0.0388,  1.1250,  0.2656], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1588])
attention_mask shape: torch.Size([4, 1588])
reward: tensor([-0.4043, -0.9336,  0.6914, -0.5156], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2020])
attention_mask shape: torch.Size([4, 2020])
reward: tensor([ 1.0391, -0.0977,  0.3555, -0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1568])
attention_mask shape: torch.Size([4, 1568])
reward: tensor([ 0.0923, -2.2188,  0.9883,  0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1404])
attention_mask shape: torch.Size([4, 1404])
reward: tensor([-0.5469,  0.3457,  0.4199, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1644])
attention_mask shape: torch.Size([4, 1644])
reward: tensor([-0.3164, -1.1875, -0.2910, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1191])
attention_mask shape: torch.Size([4, 1191])
reward: tensor([-0.5938,  0.5508, -0.4531, -1.8672], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 800])
attention_mask shape: torch.Size([4, 800])
reward: tensor([-1.0078, -1.2500, -1.2734, -0.2812], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1433])
attention_mask shape: torch.Size([4, 1433])
reward: tensor([ 0.0089, -0.6484, -1.2500, -0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([ 1.6875, -1.7500, -0.9688, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.8789, -2.1719,  1.8281, -1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1557])
attention_mask shape: torch.Size([4, 1557])
reward: tensor([ 0.0300, -0.1133, -0.1982, -0.1709], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1433])
attention_mask shape: torch.Size([4, 1433])
reward: tensor([ 0.4043,  0.2832,  0.0967, -1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1596])
attention_mask shape: torch.Size([4, 1596])
reward: tensor([ 0.4570, -0.7969, -0.8633, -0.1377], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.9609,  0.7656, -0.4258, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 758])
attention_mask shape: torch.Size([4, 758])
reward: tensor([ 0.7539,  0.1089,  0.0422, -1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 864])
attention_mask shape: torch.Size([4, 864])
reward: tensor([-0.8125, -0.3730,  0.9727,  0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1596])
attention_mask shape: torch.Size([4, 1596])
reward: tensor([-0.8477,  0.7812,  1.1719, -0.1426], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6016, -2.2031, -0.2354, -0.2354], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-1.1406, -1.1484,  0.1123,  0.2070], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1434])
attention_mask shape: torch.Size([4, 1434])
reward: tensor([ 0.6562,  0.1309,  0.4492, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 528])
attention_mask shape: torch.Size([4, 528])
reward: tensor([-1.1250, -1.7578, -1.2109,  0.3047], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1088])
attention_mask shape: torch.Size([4, 1088])
reward: tensor([-0.7344, -0.9062,  0.9062, -0.1709], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 958])
attention_mask shape: torch.Size([4, 958])
reward: tensor([-0.2441, -0.6367, -1.1094, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1319])
attention_mask shape: torch.Size([4, 1319])
reward: tensor([ 0.2637, -0.3457,  0.5000, -1.1641], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.0334,  0.1113, -0.9258, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2003])
attention_mask shape: torch.Size([4, 2003])
reward: tensor([-1.4141, -1.8594, -0.3281,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1036])
attention_mask shape: torch.Size([4, 1036])
reward: tensor([-1.0781, -1.9531,  0.8047,  0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-0.2637,  0.0178, -0.3027,  0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 677])
attention_mask shape: torch.Size([4, 677])
reward: tensor([-0.3867, -2.1719, -1.4766, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1553])
attention_mask shape: torch.Size([4, 1553])
reward: tensor([ 0.9805,  0.4355, -1.0391, -0.8359], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 498])
attention_mask shape: torch.Size([4, 498])
reward: tensor([-0.4219,  0.0344, -1.3359, -0.1797], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1249])
attention_mask shape: torch.Size([4, 1249])
reward: tensor([-0.3730, -0.6641,  0.4023,  0.1865], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1879])
attention_mask shape: torch.Size([4, 1879])
reward: tensor([-0.7227, -0.2539,  0.6719, -0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1330])
attention_mask shape: torch.Size([4, 1330])
reward: tensor([ 0.9062,  0.3516, -1.3359,  0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1203])
attention_mask shape: torch.Size([4, 1203])
reward: tensor([-0.9414, -1.2656,  0.4043,  0.5000], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1283])
attention_mask shape: torch.Size([4, 1283])
reward: tensor([-2.1562, -1.1094, -1.7812, -1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1316])
attention_mask shape: torch.Size([4, 1316])
reward: tensor([-0.1484, -0.3457, -2.0312, -1.5859], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-1.9219, -0.1885,  0.6836, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1137])
attention_mask shape: torch.Size([4, 1137])
reward: tensor([-1.2188, -0.9062, -1.2188, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7695,  0.5547,  0.0957, -0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1613])
attention_mask shape: torch.Size([4, 1613])
reward: tensor([ 0.6445,  1.4609,  0.1318, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 965])
attention_mask shape: torch.Size([4, 965])
reward: tensor([-0.2852,  0.0444,  1.8047, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1134])
attention_mask shape: torch.Size([4, 1134])
reward: tensor([-0.3379, -0.3457,  0.7695, -1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1513])
attention_mask shape: torch.Size([4, 1513])
reward: tensor([ 0.4609, -0.5195, -0.7227,  0.1797], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 592])
attention_mask shape: torch.Size([4, 592])
reward: tensor([-0.2969,  0.5078, -0.2734, -0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1175])
attention_mask shape: torch.Size([4, 1175])
reward: tensor([ 0.5469, -1.5000, -1.8281,  1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1845])
attention_mask shape: torch.Size([4, 1845])
reward: tensor([-0.3418, -2.1719, -0.9688, -0.0557], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1622])
attention_mask shape: torch.Size([4, 1622])
reward: tensor([-0.2314, -0.1309, -0.3730,  1.8438], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1235])
attention_mask shape: torch.Size([4, 1235])
reward: tensor([-1.0391, -1.6328, -0.9492, -1.5234], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1357])
attention_mask shape: torch.Size([4, 1357])
reward: tensor([-0.1445, -0.5781, -1.3047, -1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1067])
attention_mask shape: torch.Size([4, 1067])
reward: tensor([ 0.3555, -0.7852, -0.3770,  1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1378])
attention_mask shape: torch.Size([4, 1378])
reward: tensor([-0.6992, -0.4805,  0.3574, -0.2246], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 545])
attention_mask shape: torch.Size([4, 545])
reward: tensor([-0.1641, -0.8984,  0.1367,  0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.3516,  0.5391, -0.2402, -1.5547], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 797])
attention_mask shape: torch.Size([4, 797])
reward: tensor([-1.3516, -1.6328, -1.1094,  1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1759])
attention_mask shape: torch.Size([4, 1759])
reward: tensor([ 1.0234, -0.7109,  0.1177, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1258])
attention_mask shape: torch.Size([4, 1258])
reward: tensor([ 0.1201, -0.9961, -0.0713,  0.0688], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1295])
attention_mask shape: torch.Size([4, 1295])
reward: tensor([ 1.0547, -1.1250,  2.1875, -0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1349])
attention_mask shape: torch.Size([4, 1349])
reward: tensor([ 1.3281, -1.4453, -0.9766,  1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1218])
attention_mask shape: torch.Size([4, 1218])
reward: tensor([-1.0469, -0.2109,  1.7656,  1.7266], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1335])
attention_mask shape: torch.Size([4, 1335])
reward: tensor([ 0.3203, -0.4746, -0.4492, -0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 877])
attention_mask shape: torch.Size([4, 877])
reward: tensor([-0.9609, -0.9258, -0.3164,  0.1719], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6094, -1.6797, -0.9141, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1088])
attention_mask shape: torch.Size([4, 1088])
reward: tensor([ 0.8164, -0.4570, -1.4609, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 896])
attention_mask shape: torch.Size([4, 896])
reward: tensor([ 1.1875, -2.1719,  0.3750, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1291])
attention_mask shape: torch.Size([4, 1291])
reward: tensor([-0.3691,  1.0391,  0.4980, -1.6562], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.0938, -2.1406, -2.0312, -0.3906], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1731])
attention_mask shape: torch.Size([4, 1731])
reward: tensor([-0.0200, -0.5898,  0.6680, -1.8516], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1571])
attention_mask shape: torch.Size([4, 1571])
reward: tensor([ 1.7031,  0.8594, -0.9414,  0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1699])
attention_mask shape: torch.Size([4, 1699])
reward: tensor([-0.1377, -1.6172,  1.9844, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 691])
attention_mask shape: torch.Size([4, 691])
reward: tensor([-0.8203, -0.6758, -1.6875,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 810])
attention_mask shape: torch.Size([4, 810])
reward: tensor([-0.2539, -0.4141,  1.2656,  0.2373], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1121])
attention_mask shape: torch.Size([4, 1121])
reward: tensor([ 0.1621, -0.8594,  0.5703, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1365])
attention_mask shape: torch.Size([4, 1365])
reward: tensor([-0.1001, -1.3438, -0.1396, -0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1145])
attention_mask shape: torch.Size([4, 1145])
reward: tensor([ 0.3613, -1.4062, -0.6523, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 791])
attention_mask shape: torch.Size([4, 791])
reward: tensor([ 0.0942,  0.7461, -0.6016,  0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1374])
attention_mask shape: torch.Size([4, 1374])
reward: tensor([-1.2188,  0.9219,  0.7969, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 562])
attention_mask shape: torch.Size([4, 562])
reward: tensor([-1.4609, -0.3203, -0.4258, -1.9375], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
[2024-10-22 00:43:43,789] [INFO] [launch.py:351:main] Process 605466 exits successfully.
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([-1.1719, -0.3516, -0.3691, -1.7578], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1629])
attention_mask shape: torch.Size([4, 1629])
reward: tensor([-0.2227, -0.8164,  1.2266, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([ 0.1089, -1.4062,  1.1484,  0.0566], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-22 00:44:39,846] [INFO] [launch.py:351:main] Process 605465 exits successfully.
[2024-10-22 00:46:52,982] [INFO] [launch.py:351:main] Process 605467 exits successfully.
[2024-10-22 00:48:43,095] [INFO] [launch.py:351:main] Process 605468 exits successfully.
+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-1th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-1th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_1 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-22 00:48:48,942] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 00:48:50,788] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-22 00:48:50,788] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-1th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_1 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-22 00:48:52,255] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 00:48:55,312] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-22 00:48:55,313] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-22 00:48:55,313] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-22 00:48:55,313] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-22 00:48:55,313] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-22 00:48:55,313] [INFO] [launch.py:256:main] process 607435 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 00:48:55,313] [INFO] [launch.py:256:main] process 607436 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 00:48:55,314] [INFO] [launch.py:256:main] process 607437 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 00:48:55,314] [INFO] [launch.py:256:main] process 607438 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-1th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_1', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 00:48:56,760] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 00:48:56,916] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 00:48:56,928] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 00:48:56,936] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-22 00:48:59,333] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 00:48:59,343] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 00:48:59,343] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-22 00:48:59,633] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-10-22 00:48:59,698] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.76it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.29it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.20it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.70it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.28it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.20it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.38it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.66it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.29it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.22it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.39it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.91it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.83it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.52it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.43it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.45it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.52it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.84it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.69it/s]
[2024-10-22 00:49:03,552] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 00:49:03,677] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-22 00:49:03,703] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-22 00:49:03,704] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-22 00:49:03,704] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 00:49:03,735] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 00:49:04,435] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-22 00:49:04,436] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-22 00:49:04,437] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 00:49:04,437] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 00:49:04,437] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 00:49:04,556] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-22 00:49:04,557] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-22 00:49:04,557] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.94 GB, percent = 2.5%
[2024-10-22 00:49:04,685] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-22 00:49:04,686] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-22 00:49:04,686] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.94 GB, percent = 2.5%
[2024-10-22 00:49:04,687] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-22 00:49:04,687] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-22 00:49:04,687] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-22 00:49:04,687] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-22 00:49:04,687] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fde5bd26740>
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-22 00:49:04,688] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-22 00:49:04,689] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-22 00:49:04,689] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-22 00:49:04,690] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-22 00:49:04,690] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-22 00:49:04,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
[2024-10-22 00:49:09,292] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-22 00:49:09,294] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-22 00:49:09,425] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-22 00:49:09,426] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-22 00:49:09,426] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.96 GB, percent = 2.5%
[2024-10-22 00:49:09,541] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-22 00:49:09,541] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-22 00:49:09,541] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.96 GB, percent = 2.5%
[2024-10-22 00:49:09,543] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fde49656830>
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-22 00:49:09,543] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-22 00:49:09,544] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-22 00:49:09,545] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-22 00:49:09,545] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 649/100000 [00:00<00:15, 6484.52it/s]Preprocessing data:   2%|█▌                                                                                          | 1667/100000 [00:00<00:11, 8653.53it/s]Preprocessing data:   3%|██▍                                                                                         | 2700/100000 [00:00<00:10, 9415.52it/s]Preprocessing data:   4%|███▍                                                                                        | 3720/100000 [00:00<00:09, 9723.33it/s]Preprocessing data:   5%|████▎                                                                                       | 4746/100000 [00:00<00:09, 9915.32it/s]Preprocessing data:   6%|█████▎                                                                                      | 5758/100000 [00:00<00:09, 9983.13it/s]Preprocessing data:   7%|██████▏                                                                                    | 6768/100000 [00:00<00:09, 10018.79it/s]Preprocessing data:   8%|███████                                                                                    | 7786/100000 [00:00<00:09, 10068.66it/s]Preprocessing data:   9%|████████                                                                                   | 8799/100000 [00:00<00:09, 10085.94it/s]Preprocessing data:  10%|████████▉                                                                                  | 9822/100000 [00:01<00:08, 10129.78it/s]Preprocessing data:  11%|█████████▊                                                                                | 10867/100000 [00:01<00:08, 10226.57it/s]Preprocessing data:  12%|██████████▋                                                                               | 11941/100000 [00:01<00:08, 10380.57it/s]Preprocessing data:  13%|███████████▋                                                                              | 13018/100000 [00:01<00:08, 10497.16it/s]Preprocessing data:  14%|████████████▋                                                                             | 14082/100000 [00:01<00:08, 10537.32it/s]Preprocessing data:  15%|█████████████▋                                                                            | 15159/100000 [00:01<00:07, 10607.31it/s]Preprocessing data:  16%|██████████████▌                                                                           | 16233/100000 [00:01<00:07, 10644.31it/s]Preprocessing data:  17%|███████████████▌                                                                          | 17309/100000 [00:01<00:07, 10677.99it/s]Preprocessing data:  18%|████████████████▌                                                                         | 18377/100000 [00:01<00:07, 10610.31it/s]Preprocessing data:  19%|█████████████████▌                                                                        | 19450/100000 [00:01<00:07, 10643.72it/s]Preprocessing data:  21%|██████████████████▍                                                                       | 20541/100000 [00:02<00:07, 10721.56it/s]Preprocessing data:  22%|███████████████████▍                                                                      | 21625/100000 [00:02<00:07, 10754.42it/s]Preprocessing data:  23%|████████████████████▍                                                                     | 22701/100000 [00:02<00:07, 10737.66it/s]Preprocessing data:  24%|█████████████████████▍                                                                    | 23775/100000 [00:02<00:07, 10668.77it/s]Preprocessing data:  25%|██████████████████████▎                                                                   | 24842/100000 [00:02<00:07, 10643.41it/s]Preprocessing data:  26%|███████████████████████▎                                                                  | 25907/100000 [00:02<00:07, 10538.90it/s]Preprocessing data:  27%|████████████████████████▎                                                                 | 26962/100000 [00:02<00:06, 10488.51it/s]Preprocessing data:  28%|█████████████████████████▏                                                                | 28012/100000 [00:02<00:06, 10485.56it/s]Preprocessing data:  29%|██████████████████████████▏                                                               | 29061/100000 [00:02<00:06, 10452.87it/s]Preprocessing data:  30%|███████████████████████████                                                               | 30111/100000 [00:02<00:06, 10464.46it/s]Preprocessing data:  31%|████████████████████████████                                                              | 31160/100000 [00:03<00:06, 10471.50it/s]Preprocessing data:  32%|████████████████████████████▉                                                             | 32212/100000 [00:03<00:06, 10484.79it/s]Preprocessing data:  33%|█████████████████████████████▉                                                            | 33262/100000 [00:03<00:06, 10487.10it/s]Preprocessing data:  34%|██████████████████████████████▉                                                           | 34311/100000 [00:03<00:06, 10388.74it/s]Preprocessing data:  35%|███████████████████████████████▊                                                          | 35357/100000 [00:03<00:06, 10407.45it/s]Preprocessing data:  36%|████████████████████████████████▊                                                         | 36402/100000 [00:03<00:06, 10417.97it/s]Preprocessing data:  37%|█████████████████████████████████▋                                                        | 37448/100000 [00:03<00:05, 10427.85it/s]Preprocessing data:  39%|██████████████████████████████████▋                                                       | 38501/100000 [00:03<00:05, 10455.29it/s]Preprocessing data:  40%|███████████████████████████████████▌                                                      | 39547/100000 [00:03<00:05, 10430.37it/s]Preprocessing data:  41%|████████████████████████████████████▌                                                     | 40601/100000 [00:03<00:05, 10461.34it/s]Preprocessing data:  42%|█████████████████████████████████████▍                                                    | 41650/100000 [00:04<00:05, 10468.48it/s]Preprocessing data:  43%|██████████████████████████████████████▍                                                   | 42697/100000 [00:04<00:05, 10447.33it/s]Preprocessing data:  44%|███████████████████████████████████████▍                                                  | 43771/100000 [00:04<00:05, 10534.30it/s]Preprocessing data:  45%|████████████████████████████████████████▎                                                 | 44846/100000 [00:04<00:05, 10596.52it/s]Preprocessing data:  46%|█████████████████████████████████████████▎                                                | 45923/100000 [00:04<00:05, 10647.34it/s]Preprocessing data:  47%|██████████████████████████████████████████▎                                               | 46997/100000 [00:04<00:04, 10674.05it/s]Preprocessing data:  48%|███████████████████████████████████████████▎                                              | 48069/100000 [00:04<00:04, 10687.52it/s]Preprocessing data:  49%|████████████████████████████████████████████▏                                             | 49138/100000 [00:04<00:04, 10600.70it/s]Preprocessing data:  50%|█████████████████████████████████████████████▏                                            | 50210/100000 [00:04<00:04, 10635.91it/s]Preprocessing data:  51%|██████████████████████████████████████████████▏                                           | 51285/100000 [00:04<00:04, 10668.85it/s]Preprocessing data:  52%|███████████████████████████████████████████████                                           | 52352/100000 [00:05<00:04, 10646.97it/s]Preprocessing data:  53%|████████████████████████████████████████████████                                          | 53429/100000 [00:05<00:04, 10681.71it/s]Preprocessing data:  55%|█████████████████████████████████████████████████                                         | 54501/100000 [00:05<00:04, 10691.99it/s]Preprocessing data:  56%|██████████████████████████████████████████████████                                        | 55571/100000 [00:05<00:04, 10683.32it/s]Preprocessing data:  57%|██████████████████████████████████████████████████▉                                       | 56650/100000 [00:05<00:04, 10712.29it/s]Preprocessing data:  58%|███████████████████████████████████████████████████▉                                      | 57727/100000 [00:05<00:03, 10728.99it/s]Preprocessing data:  59%|████████████████████████████████████████████████████▉                                     | 58800/100000 [00:05<00:03, 10635.64it/s]Preprocessing data:  60%|█████████████████████████████████████████████████████▉                                    | 59876/100000 [00:05<00:03, 10670.84it/s]Preprocessing data:  61%|██████████████████████████████████████████████████████▊                                   | 60948/100000 [00:05<00:03, 10684.89it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▊                                  | 62020/100000 [00:05<00:03, 10694.22it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▊                                 | 63107/100000 [00:06<00:03, 10745.64it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▊                                | 64182/100000 [00:06<00:03, 10739.74it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▋                               | 65257/100000 [00:06<00:03, 10705.40it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▋                              | 66328/100000 [00:06<00:03, 10695.95it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████▋                             | 67398/100000 [00:06<00:03, 10692.38it/s]Preprocessing data:  68%|█████████████████████████████████████████████████████████████▌                            | 68468/100000 [00:06<00:02, 10668.18it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▌                           | 69535/100000 [00:06<00:02, 10587.70it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▌                          | 70598/100000 [00:06<00:02, 10599.68it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▍                         | 71660/100000 [00:06<00:02, 10602.81it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▍                        | 72721/100000 [00:06<00:02, 10370.66it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████                        | 73760/100000 [00:07<00:02, 9533.25it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████                       | 74727/100000 [00:07<00:02, 9009.19it/s]Preprocessing data:  76%|████████████████████████████████████████████████████████████████████▊                      | 75641/100000 [00:07<00:02, 8689.33it/s]Preprocessing data:  77%|█████████████████████████████████████████████████████████████████████▋                     | 76519/100000 [00:07<00:02, 8483.00it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▍                    | 77373/100000 [00:07<00:02, 8338.49it/s]Preprocessing data:  78%|███████████████████████████████████████████████████████████████████████▏                   | 78211/100000 [00:07<00:02, 8234.34it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▉                   | 79037/100000 [00:07<00:02, 8130.64it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▋                  | 79852/100000 [00:07<00:02, 8013.48it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▍                 | 80654/100000 [00:07<00:02, 7828.21it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▎                | 81640/100000 [00:08<00:02, 8405.40it/s]Preprocessing data:  82%|███████████████████████████████████████████████████████████████████████████                | 82484/100000 [00:08<00:02, 8393.59it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▊               | 83326/100000 [00:08<00:01, 8366.76it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▌              | 84165/100000 [00:08<00:01, 8243.84it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▌             | 85217/100000 [00:08<00:01, 8908.23it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▍            | 86138/100000 [00:08<00:01, 8995.03it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▏           | 87040/100000 [00:08<00:01, 8566.42it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████           | 88031/100000 [00:08<00:01, 8950.16it/s]Preprocessing data:  89%|████████████████████████████████████████████████████████████████████████████████▉          | 88990/100000 [00:08<00:01, 9135.26it/s]Preprocessing data:  90%|█████████████████████████████████████████████████████████████████████████████████▊         | 89953/100000 [00:08<00:01, 9279.40it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▋        | 90885/100000 [00:09<00:01, 8654.70it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▌       | 91761/100000 [00:09<00:01, 8188.60it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▎      | 92591/100000 [00:09<00:00, 7854.12it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▉      | 93385/100000 [00:09<00:00, 7719.00it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▋     | 94216/100000 [00:09<00:00, 7880.84it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▋    | 95238/100000 [00:09<00:00, 8541.01it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▌   | 96187/100000 [00:09<00:00, 8811.50it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▎  | 97075/100000 [00:09<00:00, 8728.12it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▏ | 97979/100000 [00:09<00:00, 8818.55it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████ | 98974/100000 [00:10<00:00, 9149.36it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████▉| 99984/100000 [00:10<00:00, 9427.82it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9852.56it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1335])
attention_mask shape: torch.Size([4, 1335])
reward: tensor([-0.6484, -0.9258, -1.3516, -0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1625])
attention_mask shape: torch.Size([4, 1625])
reward: tensor([ 0.7812, -0.6758, -0.0466, -1.8828], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1211])
attention_mask shape: torch.Size([4, 1211])
reward: tensor([-1.8438, -0.4883, -0.7930, -0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 408])
attention_mask shape: torch.Size([4, 408])
reward: tensor([-1.2266, -2.0938, -0.6250,  1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([ 0.4180,  0.3398, -0.3027, -0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 987])
attention_mask shape: torch.Size([4, 987])
reward: tensor([-1.1094, -1.2656, -0.4570, -0.2852], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1954])
attention_mask shape: torch.Size([4, 1954])
reward: tensor([-1.4609, -0.4082, -0.0488, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1049])
attention_mask shape: torch.Size([4, 1049])
reward: tensor([-0.1245, -0.1553,  0.1768,  0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1832])
attention_mask shape: torch.Size([4, 1832])
reward: tensor([-0.2676, -0.7031, -1.6172, -0.1885], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([-0.4668,  0.1865, -1.4375, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 961])
attention_mask shape: torch.Size([4, 961])
reward: tensor([-2.0938, -0.8711,  1.4219, -0.0601], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1023])
attention_mask shape: torch.Size([4, 1023])
reward: tensor([ 0.8711, -0.1201, -0.3828,  0.1631], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1274])
attention_mask shape: torch.Size([4, 1274])
reward: tensor([-1.6094, -0.9609,  0.4355, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1791])
attention_mask shape: torch.Size([4, 1791])
reward: tensor([-1.1016, -0.6719,  1.1797, -1.9766], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1045])
attention_mask shape: torch.Size([4, 1045])
reward: tensor([-0.7031,  0.2256, -0.1064, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1589])
attention_mask shape: torch.Size([4, 1589])
reward: tensor([ 0.8164, -0.7422, -0.3652, -1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1918])
attention_mask shape: torch.Size([4, 1918])
reward: tensor([-1.4375, -1.4922, -1.6953,  0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.9062,  0.1011, -1.9141, -0.2617], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 921])
attention_mask shape: torch.Size([4, 921])
reward: tensor([ 1.0391, -1.2734, -0.8906, -0.2266], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1225])
attention_mask shape: torch.Size([4, 1225])
reward: tensor([ 1.8203, -1.6953,  0.1719, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1399])
attention_mask shape: torch.Size([4, 1399])
reward: tensor([-0.8633,  0.3809, -0.3418, -0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1123])
attention_mask shape: torch.Size([4, 1123])
reward: tensor([-0.3867,  0.9883,  0.5195,  0.0732], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1071])
attention_mask shape: torch.Size([4, 1071])
reward: tensor([ 0.0532,  0.6445, -0.5625, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1105])
attention_mask shape: torch.Size([4, 1105])
reward: tensor([ 0.1914,  0.0178, -0.4805, -0.1001], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1416])
attention_mask shape: torch.Size([4, 1416])
reward: tensor([ 0.9414, -1.1562, -0.0977, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1591])
attention_mask shape: torch.Size([4, 1591])
reward: tensor([-0.0178, -0.4883,  0.5859, -0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.8789,  0.3418, -0.3203,  0.0300], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 315])
attention_mask shape: torch.Size([4, 315])
reward: tensor([-0.5820, -0.5820, -0.8164, -0.5508], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 932])
attention_mask shape: torch.Size([4, 932])
reward: tensor([-0.1982, -1.3359,  0.0422, -0.2539], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1678])
attention_mask shape: torch.Size([4, 1678])
reward: tensor([-0.6992,  1.1094, -0.0732, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2188,  0.4102, -2.2031, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1587])
attention_mask shape: torch.Size([4, 1587])
reward: tensor([-1.6562, -0.3555, -1.1250, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.4707, -0.5586, -2.0469, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1415])
attention_mask shape: torch.Size([4, 1415])
reward: tensor([ 0.2930,  0.9766, -1.0312, -0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 821])
attention_mask shape: torch.Size([4, 821])
reward: tensor([-0.1445, -1.1719, -1.0312, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.1953, -1.9844,  0.8828, -1.5469], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1110])
attention_mask shape: torch.Size([4, 1110])
reward: tensor([ 0.7695, -1.5469,  0.1533,  0.3223], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1547])
attention_mask shape: torch.Size([4, 1547])
reward: tensor([ 0.3438, -1.5469, -0.4531, -0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([-1.4609, -0.2676,  1.0156, -1.9922], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1770])
attention_mask shape: torch.Size([4, 1770])
reward: tensor([ 2.0625, -0.6562,  0.2109,  1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1777])
attention_mask shape: torch.Size([4, 1777])
reward: tensor([ 1.3359, -0.5430, -0.7305,  1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([-1.4453,  0.1641, -0.9336,  0.4473], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 579])
attention_mask shape: torch.Size([4, 579])
reward: tensor([-0.4941, -0.4004, -0.8711, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 871])
attention_mask shape: torch.Size([4, 871])
reward: tensor([-1.2812, -0.2598, -1.2656, -1.5234], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7148,  0.3281, -1.5000,  0.9727], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1133])
attention_mask shape: torch.Size([4, 1133])
reward: tensor([ 1.3828, -1.3359, -0.3730, -0.1777], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1186])
attention_mask shape: torch.Size([4, 1186])
reward: tensor([ 0.8008, -2.0938,  1.6406, -1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.9141, -0.8047, -0.2471,  0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.5508, -1.2734, -1.2188, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1138])
attention_mask shape: torch.Size([4, 1138])
reward: tensor([-0.6484, -1.6016, -0.5156,  0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1497])
attention_mask shape: torch.Size([4, 1497])
reward: tensor([-1.1406, -1.0312,  0.7734, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.5586,  0.8672, -1.2188, -1.8672], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1982])
attention_mask shape: torch.Size([4, 1982])
reward: tensor([-0.8008, -1.8438,  0.3770, -1.4141], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1350])
attention_mask shape: torch.Size([4, 1350])
reward: tensor([ 0.2070, -1.8594,  0.2559, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1554])
attention_mask shape: torch.Size([4, 1554])
reward: tensor([ 0.0845, -1.4844,  0.4238, -0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 849])
attention_mask shape: torch.Size([4, 849])
reward: tensor([-0.5391, -1.0078, -1.5312, -1.6172], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1401])
attention_mask shape: torch.Size([4, 1401])
reward: tensor([-0.8516, -0.5469,  0.3496,  0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 736])
attention_mask shape: torch.Size([4, 736])
reward: tensor([-1.5469,  0.5117,  0.9414, -1.5234], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([ 0.9375,  0.1885,  0.3184, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.3672,  1.4766, -1.2891, -0.0776], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1133])
attention_mask shape: torch.Size([4, 1133])
reward: tensor([-0.3379, -0.7227, -1.8281, -0.1533], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1886])
attention_mask shape: torch.Size([4, 1886])
reward: tensor([-0.3418, -1.0469,  1.1094,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1645])
attention_mask shape: torch.Size([4, 1645])
reward: tensor([-0.0579, -0.7305, -1.0391, -0.0623], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 755])
attention_mask shape: torch.Size([4, 755])
reward: tensor([-1.9453,  0.1377,  0.7930, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1825])
attention_mask shape: torch.Size([4, 1825])
reward: tensor([ 0.8633, -0.8047, -0.4570, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 842])
attention_mask shape: torch.Size([4, 842])
reward: tensor([-0.3203,  0.5430, -1.9766, -0.1465], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 755])
attention_mask shape: torch.Size([4, 755])
reward: tensor([-0.7773, -0.5469, -0.5117, -0.7305], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1451])
attention_mask shape: torch.Size([4, 1451])
reward: tensor([-0.5234, -1.2969,  0.0776,  1.5859], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1089])
attention_mask shape: torch.Size([4, 1089])
reward: tensor([ 1.2188, -0.0510,  0.1079, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.0312, -0.4570, -2.1719,  0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1543])
attention_mask shape: torch.Size([4, 1543])
reward: tensor([-0.6016, -0.1113, -0.4004, -0.0688], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 859])
attention_mask shape: torch.Size([4, 859])
reward: tensor([-0.3379, -1.1172, -0.9609, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1681])
attention_mask shape: torch.Size([4, 1681])
reward: tensor([-1.2500, -0.5352, -0.4844, -0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1960])
attention_mask shape: torch.Size([4, 1960])
reward: tensor([-2.0312, -0.8906, -0.3340, -0.8008], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-1.5938, -0.0977, -0.1157, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1065])
attention_mask shape: torch.Size([4, 1065])
reward: tensor([ 0.3828, -0.4219,  0.2002, -0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1464])
attention_mask shape: torch.Size([4, 1464])
reward: tensor([ 1.8203, -0.7969, -1.2266, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1304])
attention_mask shape: torch.Size([4, 1304])
reward: tensor([ 0.7109, -1.2969, -0.5586, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1321])
attention_mask shape: torch.Size([4, 1321])
reward: tensor([-0.4492,  1.0781, -0.3652,  0.4238], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 819])
attention_mask shape: torch.Size([4, 819])
reward: tensor([-1.1406, -1.7031, -0.1689, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1588])
attention_mask shape: torch.Size([4, 1588])
reward: tensor([-1.0469,  0.2637, -1.5391, -1.8125], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1199])
attention_mask shape: torch.Size([4, 1199])
reward: tensor([ 1.7812, -0.2773, -1.5000, -0.0400], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1387])
attention_mask shape: torch.Size([4, 1387])
reward: tensor([-0.6328,  0.6992, -0.1289, -0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 527])
attention_mask shape: torch.Size([4, 527])
reward: tensor([-0.6758, -1.8516, -0.5273, -0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1518])
attention_mask shape: torch.Size([4, 1518])
reward: tensor([-0.3770, -0.3730,  0.2158, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1372])
attention_mask shape: torch.Size([4, 1372])
reward: tensor([-0.5859, -0.7656, -0.0156,  0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1133])
attention_mask shape: torch.Size([4, 1133])
reward: tensor([-0.9492,  0.0776,  1.3516, -0.8008], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.4531, -2.1719, -0.7070, -0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1195])
attention_mask shape: torch.Size([4, 1195])
reward: tensor([-0.4805, -1.2266,  0.6484, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1203])
attention_mask shape: torch.Size([4, 1203])
reward: tensor([-1.2344, -0.7539, -1.8906, -1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 953])
attention_mask shape: torch.Size([4, 953])
reward: tensor([-1.6172,  0.9414, -0.3691, -0.1729], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1635])
attention_mask shape: torch.Size([4, 1635])
reward: tensor([ 1.5078,  0.6094, -0.7500, -1.4141], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 600])
attention_mask shape: torch.Size([4, 600])
reward: tensor([ 0.0991, -1.0547,  1.3438,  0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1581])
attention_mask shape: torch.Size([4, 1581])
reward: tensor([ 1.3438,  0.9336, -0.2559,  0.4453], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1775])
attention_mask shape: torch.Size([4, 1775])
reward: tensor([ 0.7656, -0.2002,  0.2773, -0.3770], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1407])
attention_mask shape: torch.Size([4, 1407])
reward: tensor([-1.0469,  1.1875, -1.0859, -0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1288])
attention_mask shape: torch.Size([4, 1288])
reward: tensor([-0.6367,  0.0400,  0.5664, -0.3906], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-2.2031,  0.1309, -0.2334, -0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 865])
attention_mask shape: torch.Size([4, 865])
reward: tensor([-0.0133,  0.0078,  0.0488, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1525])
attention_mask shape: torch.Size([4, 1525])
reward: tensor([-0.1885, -0.0334, -0.1729, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1677])
attention_mask shape: torch.Size([4, 1677])
reward: tensor([-0.7070, -0.6680, -2.1094, -0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 768])
attention_mask shape: torch.Size([4, 768])
reward: tensor([-0.8906, -1.1797,  0.4492,  0.1553], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 988])
attention_mask shape: torch.Size([4, 988])
reward: tensor([ 0.1396, -0.3906, -0.9609, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1084])
attention_mask shape: torch.Size([4, 1084])
reward: tensor([-1.2656, -1.4141,  0.1777,  0.1045], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([ 0.9023, -1.7891, -1.4766, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1857])
attention_mask shape: torch.Size([4, 1857])
reward: tensor([-0.5781,  0.6484, -0.1064,  0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1779])
attention_mask shape: torch.Size([4, 1779])
reward: tensor([-1.5859, -1.3125,  0.9414,  0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1347])
attention_mask shape: torch.Size([4, 1347])
reward: tensor([-0.1289,  0.1934, -0.6250, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 849])
attention_mask shape: torch.Size([4, 849])
reward: tensor([-0.8281, -1.0938, -1.3438, -0.3418], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1546])
attention_mask shape: torch.Size([4, 1546])
reward: tensor([ 0.1387, -1.1016,  0.8320, -0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1542])
attention_mask shape: torch.Size([4, 1542])
reward: tensor([-0.8047,  0.0189,  1.1250,  0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 212])
attention_mask shape: torch.Size([4, 212])
reward: tensor([-1.8047, -1.1875, -1.2188, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1415])
attention_mask shape: torch.Size([4, 1415])
reward: tensor([-0.7109, -1.6719,  0.5430,  1.8516], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 676])
attention_mask shape: torch.Size([4, 676])
reward: tensor([-1.3281, -0.2930, -1.6797, -0.2090], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 941])
attention_mask shape: torch.Size([4, 941])
reward: tensor([-1.7188, -0.3770, -0.2637,  0.0732], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1273])
attention_mask shape: torch.Size([4, 1273])
reward: tensor([-0.9883,  0.1729, -0.7148,  0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1448])
attention_mask shape: torch.Size([4, 1448])
reward: tensor([-0.3965, -0.7422,  0.4570, -0.5820], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1136])
attention_mask shape: torch.Size([4, 1136])
reward: tensor([-0.5586,  0.4316,  0.5859, -0.3516], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 973])
attention_mask shape: torch.Size([4, 973])
reward: tensor([ 0.0500, -1.4688, -0.0033, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1735])
attention_mask shape: torch.Size([4, 1735])
reward: tensor([-1.6562, -0.2314, -0.0156, -1.6016], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1224])
attention_mask shape: torch.Size([4, 1224])
reward: tensor([-0.1484, -0.5820, -2.0312, -0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1523])
attention_mask shape: torch.Size([4, 1523])
reward: tensor([ 1.4766, -1.5000, -1.8594, -0.7773], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1326])
attention_mask shape: torch.Size([4, 1326])
reward: tensor([-0.9141,  0.5625,  0.5859,  0.0532], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1134])
attention_mask shape: torch.Size([4, 1134])
reward: tensor([ 0.1445,  1.3828, -1.1172, -0.5820], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 861])
attention_mask shape: torch.Size([4, 861])
reward: tensor([ 0.9492, -0.8789, -1.0469, -0.2734], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 567])
attention_mask shape: torch.Size([4, 567])
reward: tensor([-1.0469,  1.1484, -1.4062, -0.0889], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1536])
attention_mask shape: torch.Size([4, 1536])
reward: tensor([-0.0378, -0.4629, -0.4707, -0.2676], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1266])
attention_mask shape: torch.Size([4, 1266])
reward: tensor([-1.0234, -0.6523,  1.7266, -0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.1094, -0.5469, -2.1406,  0.6719], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1533])
attention_mask shape: torch.Size([4, 1533])
reward: tensor([ 0.3574, -0.0820, -0.2773, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.4141,  0.2100, -0.5742, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1594])
attention_mask shape: torch.Size([4, 1594])
reward: tensor([-0.8438, -1.4844, -0.4395, -1.6562], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1982])
attention_mask shape: torch.Size([4, 1982])
reward: tensor([-0.1777, -1.3984, -0.6680, -1.5938], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 606])
attention_mask shape: torch.Size([4, 606])
reward: tensor([-0.0488, -0.7656, -0.5508,  0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 700])
attention_mask shape: torch.Size([4, 700])
reward: tensor([ 0.7227, -0.7773, -0.3652,  0.9648], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.8125, -2.1719,  0.0588, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1491])
attention_mask shape: torch.Size([4, 1491])
reward: tensor([-1.0781, -1.2344, -0.3828,  0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1559])
attention_mask shape: torch.Size([4, 1559])
reward: tensor([ 1.4062, -0.8086,  0.1157, -0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 857])
attention_mask shape: torch.Size([4, 857])
reward: tensor([-0.6797, -0.2373,  0.2773, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1493])
attention_mask shape: torch.Size([4, 1493])
reward: tensor([-0.1045, -0.7617, -0.6406, -1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.7148, -1.3672, -2.0000, -1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 865])
attention_mask shape: torch.Size([4, 865])
reward: tensor([-0.0378, -1.1172,  1.2734,  0.5898], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1633])
attention_mask shape: torch.Size([4, 1633])
reward: tensor([ 0.9805, -0.6406, -1.5859,  0.1797], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1529])
attention_mask shape: torch.Size([4, 1529])
reward: tensor([ 0.5117, -2.2031, -0.1445, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1030])
attention_mask shape: torch.Size([4, 1030])
reward: tensor([-0.4844,  0.9414, -1.5234, -0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1426])
attention_mask shape: torch.Size([4, 1426])
reward: tensor([ 0.3340, -0.7344, -0.7773, -0.4355], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1031])
attention_mask shape: torch.Size([4, 1031])
reward: tensor([-0.7695,  0.4707,  0.6055, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1497])
attention_mask shape: torch.Size([4, 1497])
reward: tensor([ 0.7773, -2.1719, -0.7305, -0.1729], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 632])
attention_mask shape: torch.Size([4, 632])
reward: tensor([ 1.4219, -0.0532, -0.3164, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([ 0.4219, -0.3828, -2.0156,  0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([-0.8320, -1.3125, -0.7617,  1.4453], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-0.9141,  0.3340, -0.7031,  0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-2.0312,  1.4219, -0.8984,  0.1221], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 746])
attention_mask shape: torch.Size([4, 746])
reward: tensor([-0.3164, -0.5859, -1.9219,  1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1345])
attention_mask shape: torch.Size([4, 1345])
reward: tensor([-0.3164, -2.2031, -0.5625, -0.2490], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 569])
attention_mask shape: torch.Size([4, 569])
reward: tensor([ 0.8359, -0.4258, -0.6328, -0.2197], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([-0.3027, -1.0938, -0.6992,  0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1827])
attention_mask shape: torch.Size([4, 1827])
reward: tensor([-1.3359, -0.1221,  1.4141, -0.2676], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1589])
attention_mask shape: torch.Size([4, 1589])
reward: tensor([ 0.4902, -1.4453, -0.9961, -0.5742], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1189])
attention_mask shape: torch.Size([4, 1189])
reward: tensor([ 0.0742,  0.9219, -1.4375, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1194])
attention_mask shape: torch.Size([4, 1194])
reward: tensor([-2.1250, -1.1172,  0.0913, -1.5859], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1015])
attention_mask shape: torch.Size([4, 1015])
reward: tensor([ 0.1406, -0.8633, -0.3691,  0.0713], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 953])
attention_mask shape: torch.Size([4, 953])
reward: tensor([ 0.6367,  0.7109, -0.5781, -1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1265])
attention_mask shape: torch.Size([4, 1265])
reward: tensor([-0.0200,  0.6758, -0.5078,  0.2812], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1340])
attention_mask shape: torch.Size([4, 1340])
reward: tensor([-2.1406, -1.1406, -0.2402, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1435])
attention_mask shape: torch.Size([4, 1435])
reward: tensor([ 0.5938,  0.3457,  1.0859, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([ 1.1562, -1.4219,  0.6328,  0.3691], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 803])
attention_mask shape: torch.Size([4, 803])
reward: tensor([ 0.6797, -0.9883, -0.7461, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([ 1.5859, -0.9961, -1.4062, -0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1568])
attention_mask shape: torch.Size([4, 1568])
reward: tensor([ 0.9375, -1.2344, -0.2227, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1857])
attention_mask shape: torch.Size([4, 1857])
reward: tensor([-0.5547, -0.6562, -1.1406,  1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1435])
attention_mask shape: torch.Size([4, 1435])
reward: tensor([ 1.3828,  0.2109,  1.6016, -0.0913], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1120])
attention_mask shape: torch.Size([4, 1120])
reward: tensor([-0.6172, -0.5898, -2.1406,  2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1680])
attention_mask shape: torch.Size([4, 1680])
reward: tensor([-0.7383, -0.8047, -1.6562,  0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1017])
attention_mask shape: torch.Size([4, 1017])
reward: tensor([-0.8906, -1.0781,  0.1396, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([-0.0977, -0.5859,  0.5039, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.8203,  0.0991, -0.0713, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1600])
attention_mask shape: torch.Size([4, 1600])
reward: tensor([-0.8164, -2.2031,  1.1875, -0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1360])
attention_mask shape: torch.Size([4, 1360])
reward: tensor([-0.8086,  0.9688,  0.0732, -0.6367], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1491])
attention_mask shape: torch.Size([4, 1491])
reward: tensor([-0.9766, -1.2266, -0.6445, -0.4629], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 878])
attention_mask shape: torch.Size([4, 878])
reward: tensor([ 0.6172,  0.1416, -0.9766, -1.8984], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 646])
attention_mask shape: torch.Size([4, 646])
reward: tensor([ 0.1270, -1.1562, -0.7773, -0.6328], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1412])
attention_mask shape: torch.Size([4, 1412])
reward: tensor([-0.3418, -0.4883, -1.3594,  1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 689])
attention_mask shape: torch.Size([4, 689])
reward: tensor([ 1.1094,  1.4688, -1.9297, -0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.4922, -1.5469,  1.2266, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1714])
attention_mask shape: torch.Size([4, 1714])
reward: tensor([-0.2021, -0.8125, -0.1709, -0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1169])
attention_mask shape: torch.Size([4, 1169])
reward: tensor([ 0.5586, -0.0732,  0.3438, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1918])
attention_mask shape: torch.Size([4, 1918])
reward: tensor([ 0.1187,  0.1357, -1.0469,  0.1592], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.5234, -1.2656, -0.3457, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1341])
attention_mask shape: torch.Size([4, 1341])
reward: tensor([ 1.3438e+00, -4.4531e-01, -1.8281e+00, -1.1139e-03], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1098])
attention_mask shape: torch.Size([4, 1098])
reward: tensor([-0.4395, -0.1670,  0.4043, -1.4844], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1431])
attention_mask shape: torch.Size([4, 1431])
reward: tensor([-1.8672,  0.1338,  0.6836,  0.2734], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.5781, -1.6094, -0.3730, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-1.2344, -0.7500, -0.8203, -0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1462])
attention_mask shape: torch.Size([4, 1462])
reward: tensor([ 0.0811,  0.0067,  0.5586, -0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 934])
attention_mask shape: torch.Size([4, 934])
reward: tensor([-1.5703,  0.3965, -0.3730,  0.3926], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1135])
attention_mask shape: torch.Size([4, 1135])
reward: tensor([-0.5625, -0.6133,  0.7383, -0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1014])
attention_mask shape: torch.Size([4, 1014])
reward: tensor([-0.1885, -0.6406,  0.3984, -0.0776], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1402])
attention_mask shape: torch.Size([4, 1402])
reward: tensor([ 0.2559,  1.8047, -0.0532,  0.0078], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.0334,  0.2559, -0.3730, -1.6953], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1657])
attention_mask shape: torch.Size([4, 1657])
reward: tensor([-1.1172, -0.7812, -0.3281,  0.7773], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 830])
attention_mask shape: torch.Size([4, 830])
reward: tensor([-0.4805, -1.7422,  0.6484, -0.2812], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-0.7852, -1.1562, -1.5938, -0.1514], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 961])
attention_mask shape: torch.Size([4, 961])
reward: tensor([ 0.4355, -2.2031, -0.6211, -0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1432])
attention_mask shape: torch.Size([4, 1432])
reward: tensor([ 0.4609, -0.1089, -0.7852, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 499])
attention_mask shape: torch.Size([4, 499])
reward: tensor([-0.4219,  0.3027, -1.3125,  0.1387], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1249])
attention_mask shape: torch.Size([4, 1249])
reward: tensor([-1.1250, -0.1089, -0.6250, -1.1797], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1879])
attention_mask shape: torch.Size([4, 1879])
reward: tensor([-0.1426, -0.7227, -2.1562,  0.2363], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1723])
attention_mask shape: torch.Size([4, 1723])
reward: tensor([ 0.6992,  0.6562, -1.0781,  1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1964])
attention_mask shape: torch.Size([4, 1964])
reward: tensor([-1.9844, -1.7969,  0.1235,  1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1283])
attention_mask shape: torch.Size([4, 1283])
reward: tensor([-1.5391, -0.8398, -1.9219, -0.1533], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1207])
attention_mask shape: torch.Size([4, 1207])
reward: tensor([-1.6250, -0.6172, -1.4609, -1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-1.3516,  0.3984,  0.2754, -0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1137])
attention_mask shape: torch.Size([4, 1137])
reward: tensor([-0.8906, -1.8125, -1.0938, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.1719,  0.5000, -0.3594, -1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1518])
attention_mask shape: torch.Size([4, 1518])
reward: tensor([ 0.9766,  0.4395, -0.1885, -1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 904])
attention_mask shape: torch.Size([4, 904])
reward: tensor([ 1.3672, -0.6172,  0.9375, -0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 789])
attention_mask shape: torch.Size([4, 789])
reward: tensor([ 0.0669,  1.0312,  0.5234, -1.6016], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1867])
attention_mask shape: torch.Size([4, 1867])
reward: tensor([ 0.7734, -1.3672, -2.0000, -0.1533], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 996])
attention_mask shape: torch.Size([4, 996])
reward: tensor([ 0.3750, -0.7773, -0.7539, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1139])
attention_mask shape: torch.Size([4, 1139])
reward: tensor([ 0.3398, -1.4062, -1.9375,  1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1466])
attention_mask shape: torch.Size([4, 1466])
reward: tensor([-0.6562,  0.7188, -2.0156,  0.1709], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.3750,  0.4004, -0.6094,  1.8281], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1235])
attention_mask shape: torch.Size([4, 1235])
reward: tensor([-0.9062, -1.1797, -0.5820, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([ 0.7383, -1.4375, -0.9961,  1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1741])
attention_mask shape: torch.Size([4, 1741])
reward: tensor([-0.0688, -0.3965, -0.5195, -0.1113], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1341])
attention_mask shape: torch.Size([4, 1341])
reward: tensor([-0.0378, -1.7188,  0.1221, -0.2852], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 495])
attention_mask shape: torch.Size([4, 495])
reward: tensor([-1.1016, -0.5742,  0.2266,  0.4453], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1301])
attention_mask shape: torch.Size([4, 1301])
reward: tensor([-1.3516,  0.7227,  0.3613, -0.1309], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 882])
attention_mask shape: torch.Size([4, 882])
reward: tensor([-1.0469, -1.8906, -1.0391,  1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1622])
attention_mask shape: torch.Size([4, 1622])
reward: tensor([ 0.8633, -0.9961,  0.5938, -0.2676], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1585])
attention_mask shape: torch.Size([4, 1585])
reward: tensor([ 0.4629,  0.2852, -1.5547,  0.0156], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1481])
attention_mask shape: torch.Size([4, 1481])
reward: tensor([ 0.2373, -0.9961,  1.3047, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1673])
attention_mask shape: torch.Size([4, 1673])
reward: tensor([ 2.3125, -1.1797, -0.8125,  0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 918])
attention_mask shape: torch.Size([4, 918])
reward: tensor([-0.6641, -0.4980,  0.5742,  0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.5078, -1.0156,  0.0854, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1307])
attention_mask shape: torch.Size([4, 1307])
reward: tensor([-2.0781, -1.1484, -0.0156,  0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1143])
attention_mask shape: torch.Size([4, 1143])
reward: tensor([-0.6797, -1.1094,  0.0067, -0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1133])
attention_mask shape: torch.Size([4, 1133])
reward: tensor([ 1.5625, -0.2490, -1.4141, -0.1045], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 773])
attention_mask shape: torch.Size([4, 773])
reward: tensor([ 0.6367, -2.0938, -0.0801, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1509])
attention_mask shape: torch.Size([4, 1509])
reward: tensor([-1.9922,  1.7578, -0.8711, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1501])
attention_mask shape: torch.Size([4, 1501])
reward: tensor([-0.3770, -1.9219, -0.5625, -1.1719], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1252])
attention_mask shape: torch.Size([4, 1252])
reward: tensor([-1.0391, -0.8359,  0.9414,  0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1591])
attention_mask shape: torch.Size([4, 1591])
reward: tensor([ 1.5938,  1.2812, -0.6016,  0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1640])
attention_mask shape: torch.Size([4, 1640])
reward: tensor([-0.3594, -1.8125,  1.7969, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
[2024-10-22 01:44:29,430] [INFO] [launch.py:351:main] Process 607437 exits successfully.
sequences shape: torch.Size([4, 1333])
attention_mask shape: torch.Size([4, 1333])
reward: tensor([-0.3965, -0.7930, -1.7031,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 878])
attention_mask shape: torch.Size([4, 878])
reward: tensor([-0.3066,  0.3457,  0.4746, -0.2021], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1615])
attention_mask shape: torch.Size([4, 1615])
reward: tensor([-0.2178, -0.1445, -0.1533, -1.9297], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1987])
attention_mask shape: torch.Size([4, 1987])
reward: tensor([-0.7461, -1.2344, -1.5703,  0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1425])
attention_mask shape: torch.Size([4, 1425])
reward: tensor([ 0.4883, -0.7617, -0.7070, -0.0078], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 661])
attention_mask shape: torch.Size([4, 661])
reward: tensor([-0.7773, -0.0732, -0.4395,  0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1511])
attention_mask shape: torch.Size([4, 1511])
reward: tensor([ 0.2754,  0.1021, -0.5781, -1.2344], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 440])
attention_mask shape: torch.Size([4, 440])
reward: tensor([-0.9141, -0.3203,  0.0933, -1.8047], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
[2024-10-22 01:46:07,532] [INFO] [launch.py:351:main] Process 607436 exits successfully.
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([ 0.0366, -2.1562, -0.8906, -0.8320], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.6328, -1.2109,  0.7188,  0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-0.0557, -1.2188,  1.3359,  0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-22 01:47:01,588] [INFO] [launch.py:351:main] Process 607435 exits successfully.
[2024-10-22 01:47:51,640] [INFO] [launch.py:351:main] Process 607438 exits successfully.
+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-2th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-2th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_2 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-22 01:47:56,853] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 01:47:58,701] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-22 01:47:58,701] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-2th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_2 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-22 01:48:00,081] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 01:48:03,368] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-22 01:48:03,369] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-22 01:48:03,369] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-22 01:48:03,369] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-22 01:48:03,369] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-22 01:48:03,369] [INFO] [launch.py:256:main] process 608691 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-2th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_2', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 01:48:03,369] [INFO] [launch.py:256:main] process 608692 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-2th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_2', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 01:48:03,370] [INFO] [launch.py:256:main] process 608693 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-2th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_2', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 01:48:03,370] [INFO] [launch.py:256:main] process 608694 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-2th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_2', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 01:48:04,873] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 01:48:04,978] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 01:48:04,991] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 01:48:04,999] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-22 01:48:07,418] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 01:48:07,418] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-22 01:48:07,549] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-10-22 01:48:07,719] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 01:48:07,721] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.44it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.54it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.36it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.47it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.47it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.47it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.32it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.46it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.45it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.49it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.53it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.46it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.48it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.54it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.73it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.64it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.83it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.69it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.88it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.73it/s]
[2024-10-22 01:48:11,645] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 01:48:11,723] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-22 01:48:11,752] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-22 01:48:11,752] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-22 01:48:11,752] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 01:48:11,755] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 01:48:12,444] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-22 01:48:12,445] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-22 01:48:12,445] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 01:48:12,446] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 01:48:12,447] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 01:48:12,575] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-22 01:48:12,575] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-22 01:48:12,575] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.9 GB, percent = 2.5%
[2024-10-22 01:48:12,707] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-22 01:48:12,708] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-22 01:48:12,708] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.9 GB, percent = 2.5%
[2024-10-22 01:48:12,709] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-22 01:48:12,709] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-22 01:48:12,709] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-22 01:48:12,709] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-22 01:48:12,709] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe2181f8fa0>
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-22 01:48:12,710] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-22 01:48:12,711] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-22 01:48:12,711] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-22 01:48:12,712] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-22 01:48:12,712] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-22 01:48:12,712] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 01:48:17,466] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-22 01:48:17,468] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-22 01:48:17,587] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-22 01:48:17,587] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-22 01:48:17,588] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.93 GB, percent = 2.5%
[2024-10-22 01:48:17,706] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-22 01:48:17,707] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-22 01:48:17,707] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.93 GB, percent = 2.5%
[2024-10-22 01:48:17,708] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe209d02890>
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-22 01:48:17,709] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-22 01:48:17,710] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-22 01:48:17,711] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-22 01:48:17,711] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 653/100000 [00:00<00:15, 6528.05it/s]Preprocessing data:   2%|█▌                                                                                          | 1665/100000 [00:00<00:11, 8640.10it/s]Preprocessing data:   3%|██▍                                                                                         | 2699/100000 [00:00<00:10, 9415.11it/s]Preprocessing data:   4%|███▍                                                                                        | 3717/100000 [00:00<00:09, 9716.21it/s]Preprocessing data:   5%|████▎                                                                                       | 4734/100000 [00:00<00:09, 9876.79it/s]Preprocessing data:   6%|█████▎                                                                                      | 5747/100000 [00:00<00:09, 9962.14it/s]Preprocessing data:   7%|██████▏                                                                                    | 6758/100000 [00:00<00:09, 10009.42it/s]Preprocessing data:   8%|███████                                                                                    | 7790/100000 [00:00<00:09, 10108.03it/s]Preprocessing data:   9%|████████                                                                                   | 8818/100000 [00:00<00:08, 10159.58it/s]Preprocessing data:  10%|████████▉                                                                                  | 9837/100000 [00:01<00:08, 10167.87it/s]Preprocessing data:  11%|█████████▊                                                                                | 10903/100000 [00:01<00:08, 10318.02it/s]Preprocessing data:  12%|██████████▊                                                                               | 11981/100000 [00:01<00:08, 10456.31it/s]Preprocessing data:  13%|███████████▋                                                                              | 13038/100000 [00:01<00:08, 10488.60it/s]Preprocessing data:  14%|████████████▋                                                                             | 14112/100000 [00:01<00:08, 10563.21it/s]Preprocessing data:  15%|█████████████▋                                                                            | 15185/100000 [00:01<00:07, 10611.26it/s]Preprocessing data:  16%|██████████████▋                                                                           | 16263/100000 [00:01<00:07, 10661.90it/s]Preprocessing data:  17%|███████████████▌                                                                          | 17337/100000 [00:01<00:07, 10683.69it/s]Preprocessing data:  18%|████████████████▌                                                                         | 18411/100000 [00:01<00:07, 10700.24it/s]Preprocessing data:  19%|█████████████████▌                                                                        | 19487/100000 [00:01<00:07, 10717.47it/s]Preprocessing data:  21%|██████████████████▌                                                                       | 20578/100000 [00:02<00:07, 10773.44it/s]Preprocessing data:  22%|███████████████████▌                                                                      | 21667/100000 [00:02<00:07, 10806.69it/s]Preprocessing data:  23%|████████████████████▍                                                                     | 22748/100000 [00:02<00:07, 10761.82it/s]Preprocessing data:  24%|█████████████████████▍                                                                    | 23825/100000 [00:02<00:07, 10713.31it/s]Preprocessing data:  25%|██████████████████████▍                                                                   | 24897/100000 [00:02<00:07, 10687.05it/s]Preprocessing data:  26%|███████████████████████▎                                                                  | 25966/100000 [00:02<00:06, 10640.54it/s]Preprocessing data:  27%|████████████████████████▎                                                                 | 27031/100000 [00:02<00:06, 10612.50it/s]Preprocessing data:  28%|█████████████████████████▎                                                                | 28093/100000 [00:02<00:06, 10547.24it/s]Preprocessing data:  29%|██████████████████████████▏                                                               | 29148/100000 [00:02<00:06, 10532.56it/s]Preprocessing data:  30%|███████████████████████████▏                                                              | 30202/100000 [00:02<00:06, 10534.45it/s]Preprocessing data:  31%|████████████████████████████▏                                                             | 31258/100000 [00:03<00:06, 10540.92it/s]Preprocessing data:  32%|█████████████████████████████                                                             | 32313/100000 [00:03<00:06, 10540.51it/s]Preprocessing data:  33%|██████████████████████████████                                                            | 33368/100000 [00:03<00:06, 10527.18it/s]Preprocessing data:  34%|██████████████████████████████▉                                                           | 34421/100000 [00:03<00:06, 10517.84it/s]Preprocessing data:  35%|███████████████████████████████▉                                                          | 35473/100000 [00:03<00:06, 10506.78it/s]Preprocessing data:  37%|████████████████████████████████▊                                                         | 36524/100000 [00:03<00:06, 10490.70it/s]Preprocessing data:  38%|█████████████████████████████████▊                                                        | 37574/100000 [00:03<00:05, 10482.09it/s]Preprocessing data:  39%|██████████████████████████████████▊                                                       | 38623/100000 [00:03<00:05, 10482.83it/s]Preprocessing data:  40%|███████████████████████████████████▋                                                      | 39677/100000 [00:03<00:05, 10498.76it/s]Preprocessing data:  41%|████████████████████████████████████▋                                                     | 40732/100000 [00:03<00:05, 10511.74it/s]Preprocessing data:  42%|█████████████████████████████████████▌                                                    | 41784/100000 [00:04<00:05, 10499.49it/s]Preprocessing data:  43%|██████████████████████████████████████▌                                                   | 42834/100000 [00:04<00:05, 10488.15it/s]Preprocessing data:  44%|███████████████████████████████████████▌                                                  | 43907/100000 [00:04<00:05, 10558.12it/s]Preprocessing data:  45%|████████████████████████████████████████▍                                                 | 44980/100000 [00:04<00:05, 10608.21it/s]Preprocessing data:  46%|█████████████████████████████████████████▍                                                | 46058/100000 [00:04<00:05, 10657.33it/s]Preprocessing data:  47%|██████████████████████████████████████████▍                                               | 47131/100000 [00:04<00:04, 10676.41it/s]Preprocessing data:  48%|███████████████████████████████████████████▍                                              | 48201/100000 [00:04<00:04, 10682.73it/s]Preprocessing data:  49%|████████████████████████████████████████████▎                                             | 49275/100000 [00:04<00:04, 10699.35it/s]Preprocessing data:  50%|█████████████████████████████████████████████▎                                            | 50350/100000 [00:04<00:04, 10713.07it/s]Preprocessing data:  51%|██████████████████████████████████████████████▎                                           | 51425/100000 [00:04<00:04, 10723.38it/s]Preprocessing data:  52%|███████████████████████████████████████████████▏                                          | 52498/100000 [00:05<00:04, 10723.47it/s]Preprocessing data:  54%|████████████████████████████████████████████████▏                                         | 53573/100000 [00:05<00:04, 10730.08it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▏                                        | 54647/100000 [00:05<00:04, 10676.48it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▏                                       | 55721/100000 [00:05<00:04, 10694.30it/s]Preprocessing data:  57%|███████████████████████████████████████████████████                                       | 56797/100000 [00:05<00:04, 10711.79it/s]Preprocessing data:  58%|████████████████████████████████████████████████████                                      | 57869/100000 [00:05<00:03, 10610.24it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████                                     | 58931/100000 [00:05<00:03, 10587.34it/s]Preprocessing data:  60%|█████████████████████████████████████████████████████▉                                    | 59992/100000 [00:05<00:03, 10593.79it/s]Preprocessing data:  61%|██████████████████████████████████████████████████████▉                                   | 61052/100000 [00:05<00:03, 10540.47it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▉                                  | 62128/100000 [00:05<00:03, 10604.50it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▉                                 | 63209/100000 [00:06<00:03, 10663.14it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▊                                | 64278/100000 [00:06<00:03, 10670.00it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▊                               | 65346/100000 [00:06<00:03, 10670.30it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▊                              | 66414/100000 [00:06<00:03, 10665.72it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████▋                             | 67482/100000 [00:06<00:03, 10668.18it/s]Preprocessing data:  69%|█████████████████████████████████████████████████████████████▋                            | 68549/100000 [00:06<00:02, 10653.82it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▋                           | 69615/100000 [00:06<00:02, 10639.01it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▌                          | 70679/100000 [00:06<00:02, 10601.21it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▌                         | 71740/100000 [00:06<00:02, 10556.19it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▌                        | 72796/100000 [00:06<00:02, 10275.17it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▏                       | 73826/100000 [00:07<00:02, 9465.52it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████                       | 74786/100000 [00:07<00:02, 8958.50it/s]Preprocessing data:  76%|████████████████████████████████████████████████████████████████████▉                      | 75694/100000 [00:07<00:02, 8645.76it/s]Preprocessing data:  77%|█████████████████████████████████████████████████████████████████████▋                     | 76567/100000 [00:07<00:02, 8440.40it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▍                    | 77417/100000 [00:07<00:02, 8291.68it/s]Preprocessing data:  78%|███████████████████████████████████████████████████████████████████████▏                   | 78250/100000 [00:07<00:02, 8179.44it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▉                   | 79070/100000 [00:07<00:02, 8085.93it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▋                  | 79880/100000 [00:07<00:02, 7919.58it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▍                 | 80673/100000 [00:07<00:02, 7752.98it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▎                | 81656/100000 [00:08<00:02, 8343.10it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████                | 82503/100000 [00:08<00:02, 8376.57it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▊               | 83344/100000 [00:08<00:01, 8349.08it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▌              | 84181/100000 [00:08<00:01, 8235.12it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▌             | 85230/100000 [00:08<00:01, 8893.30it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▍            | 86154/100000 [00:08<00:01, 8992.68it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▏           | 87056/100000 [00:08<00:01, 8584.12it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████▏          | 88079/100000 [00:08<00:01, 9056.97it/s]Preprocessing data:  89%|█████████████████████████████████████████████████████████████████████████████████          | 89082/100000 [00:08<00:01, 9339.35it/s]Preprocessing data:  90%|█████████████████████████████████████████████████████████████████████████████████▉         | 90090/100000 [00:08<00:01, 9556.40it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▊        | 91050/100000 [00:09<00:01, 8744.22it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▋       | 91940/100000 [00:09<00:00, 8234.61it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▍      | 92779/100000 [00:09<00:00, 7908.87it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▏     | 93581/100000 [00:09<00:00, 7793.58it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▉     | 94476/100000 [00:09<00:00, 8109.79it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▉    | 95494/100000 [00:09<00:00, 8692.07it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▋   | 96420/100000 [00:09<00:00, 8854.79it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▌  | 97313/100000 [00:09<00:00, 8848.49it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▍ | 98222/100000 [00:09<00:00, 8916.91it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████▎| 99244/100000 [00:10<00:00, 9300.57it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9879.25it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2500, -1.0938, -1.6797, -0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.7188, -1.7812, -0.3203, -0.2227], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-2.1406, -0.6641, -0.7969, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1229])
attention_mask shape: torch.Size([4, 1229])
reward: tensor([-1.1094, -2.2031, -0.3203, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.7812,  0.0654,  0.2100, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1287])
attention_mask shape: torch.Size([4, 1287])
reward: tensor([ 1.5312, -1.0391, -1.2969,  1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1954])
attention_mask shape: torch.Size([4, 1954])
reward: tensor([-1.9453, -0.3340,  0.1670, -0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([ 0.4141, -0.9492,  0.5273,  1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1083])
attention_mask shape: torch.Size([4, 1083])
reward: tensor([-0.6523,  1.4844, -0.3594, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.9414,  1.1562, -1.2031, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1619])
attention_mask shape: torch.Size([4, 1619])
reward: tensor([ 0.1865, -0.2812, -1.2812, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1271])
attention_mask shape: torch.Size([4, 1271])
reward: tensor([ 0.0579, -0.4141, -0.1641,  0.1807], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1146])
attention_mask shape: torch.Size([4, 1146])
reward: tensor([-1.9375, -0.7500, -0.2676, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2007])
attention_mask shape: torch.Size([4, 2007])
reward: tensor([-0.2070,  0.1157,  1.8906, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1178])
attention_mask shape: torch.Size([4, 1178])
reward: tensor([-1.8125, -2.1719,  1.7969, -0.0601], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.2354,  1.1562, -1.1406,  0.0233], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1918])
attention_mask shape: torch.Size([4, 1918])
reward: tensor([-0.3594, -1.3281, -0.5430, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.4453, -0.5508, -1.7812, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1379])
attention_mask shape: torch.Size([4, 1379])
reward: tensor([-0.2178, -0.7070, -2.1406, -1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 625])
attention_mask shape: torch.Size([4, 625])
reward: tensor([ 0.5898,  0.5781, -1.8281,  0.0344], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 858])
attention_mask shape: torch.Size([4, 858])
reward: tensor([-1.4766,  0.1914,  0.6992, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1343])
attention_mask shape: torch.Size([4, 1343])
reward: tensor([ 0.5469,  0.9570, -0.6367, -0.0067], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1054])
attention_mask shape: torch.Size([4, 1054])
reward: tensor([ 0.2314, -0.0645,  0.2256, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1737])
attention_mask shape: torch.Size([4, 1737])
reward: tensor([ 0.2793, -1.2734, -0.9609,  1.5703], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.8008, -0.5352,  0.0089, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1492])
attention_mask shape: torch.Size([4, 1492])
reward: tensor([0.6484, 0.4258, 0.5469, 0.6562], device='cuda:0', dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.6016,  0.3418,  0.5273,  0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 459])
attention_mask shape: torch.Size([4, 459])
reward: tensor([ 0.9766, -0.5820,  0.0033,  0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1484])
attention_mask shape: torch.Size([4, 1484])
reward: tensor([-0.0757, -0.9609, -0.8281,  1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1627])
attention_mask shape: torch.Size([4, 1627])
reward: tensor([-0.3770,  0.9258,  0.1416, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7070,  0.2832, -2.1250, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1587])
attention_mask shape: torch.Size([4, 1587])
reward: tensor([-2.0156,  0.2021,  0.3125, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.2354, -0.2471,  0.4941, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.5273,  0.3359, -0.7422, -0.2539], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1746])
attention_mask shape: torch.Size([4, 1746])
reward: tensor([ 0.7695, -1.8125, -0.9961, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1939])
attention_mask shape: torch.Size([4, 1939])
reward: tensor([ 1.7422, -1.8516,  0.8594, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1631])
attention_mask shape: torch.Size([4, 1631])
reward: tensor([-0.9414, -0.8711, -0.9414,  1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1477])
attention_mask shape: torch.Size([4, 1477])
reward: tensor([ 0.4570, -0.8906, -0.4180, -0.5820], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([-0.8789, -1.9297,  1.3125, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.0045, -0.5781,  0.0791,  1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1777])
attention_mask shape: torch.Size([4, 1777])
reward: tensor([ 1.0547,  0.5156, -0.0713, -1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([ 1.6875,  1.1641, -0.9062, -0.3691], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 587])
attention_mask shape: torch.Size([4, 587])
reward: tensor([-0.9062,  0.0854, -0.8047, -0.0133], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 763])
attention_mask shape: torch.Size([4, 763])
reward: tensor([ 0.1934,  0.3418, -0.1758, -0.8320], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1930])
attention_mask shape: torch.Size([4, 1930])
reward: tensor([0.5586, 2.2188, 1.2656, 0.9805], device='cuda:0', dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 983])
attention_mask shape: torch.Size([4, 983])
reward: tensor([ 0.0000, -1.0703,  0.1455, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 848])
attention_mask shape: torch.Size([4, 848])
reward: tensor([0.4336, 0.2402, 1.2344, 0.5703], device='cuda:0', dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.5078, -0.8086, -1.2891,  0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.9062, -0.1157, -1.6094, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([-0.8125, -1.8047,  0.0457,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6992, -2.1562,  0.5312,  0.0544], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.7188,  1.6406, -2.1719, -1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1715])
attention_mask shape: torch.Size([4, 1715])
reward: tensor([-0.5547, -0.4980,  1.2422, -1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1869])
attention_mask shape: torch.Size([4, 1869])
reward: tensor([-1.1562, -1.6797, -0.7227, -0.5898], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1598])
attention_mask shape: torch.Size([4, 1598])
reward: tensor([-0.4355,  0.0898,  0.1660, -0.2246], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-0.4629, -1.6719, -1.7500, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1723])
attention_mask shape: torch.Size([4, 1723])
reward: tensor([-0.5781,  0.1001, -0.2334, -1.3984], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1462])
attention_mask shape: torch.Size([4, 1462])
reward: tensor([-2.0469,  0.3125,  0.1514, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([-0.0200,  0.3770,  0.1592, -1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.1484,  1.2812, -2.1250, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1171])
attention_mask shape: torch.Size([4, 1171])
reward: tensor([-0.5820, -1.1797, -1.5000,  0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7031,  0.4453, -0.4180,  0.6445], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1647])
attention_mask shape: torch.Size([4, 1647])
reward: tensor([ 0.3379,  0.3594, -0.6562,  0.0111], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 976])
attention_mask shape: torch.Size([4, 976])
reward: tensor([ 1.5859,  0.5039, -1.4453, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1825])
attention_mask shape: torch.Size([4, 1825])
reward: tensor([ 1.0625, -0.8984,  1.8750, -1.9297], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 992])
attention_mask shape: torch.Size([4, 992])
reward: tensor([-0.6133,  0.1846, -0.8398,  0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1572])
attention_mask shape: torch.Size([4, 1572])
reward: tensor([-1.0156, -0.0601, -0.8789, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.2754,  0.2393,  0.5156, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1250])
attention_mask shape: torch.Size([4, 1250])
reward: tensor([ 0.4883, -0.4082, -0.8203,  0.3320], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.5703, -1.0547,  1.2812,  0.8750], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1641])
attention_mask shape: torch.Size([4, 1641])
reward: tensor([ 0.4473, -0.5117, -0.4004,  0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 882])
attention_mask shape: torch.Size([4, 882])
reward: tensor([ 0.4004, -1.2031, -0.5938,  0.1001], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1681])
attention_mask shape: torch.Size([4, 1681])
reward: tensor([-0.4941,  1.4219, -0.2695, -1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1960])
attention_mask shape: torch.Size([4, 1960])
reward: tensor([-1.2109, -0.5078,  0.1055,  0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 404])
attention_mask shape: torch.Size([4, 404])
reward: tensor([-1.5703,  0.0479, -0.5938, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1017])
attention_mask shape: torch.Size([4, 1017])
reward: tensor([ 0.1357,  1.2031,  0.3867, -0.1934], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.0234, -1.0234, -1.4375, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1531])
attention_mask shape: torch.Size([4, 1531])
reward: tensor([ 0.8047, -0.3867,  0.1689, -1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1488])
attention_mask shape: torch.Size([4, 1488])
reward: tensor([-0.8047,  1.1016,  1.9453,  0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 864])
attention_mask shape: torch.Size([4, 864])
reward: tensor([-1.1641, -1.5391, -0.2852, -1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1588])
attention_mask shape: torch.Size([4, 1588])
reward: tensor([ 0.7109,  0.8359, -1.1875, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1199])
attention_mask shape: torch.Size([4, 1199])
reward: tensor([ 1.5547, -0.7148, -1.2422, -0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1601])
attention_mask shape: torch.Size([4, 1601])
reward: tensor([ 0.0991, -0.9961, -0.1934, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1278])
attention_mask shape: torch.Size([4, 1278])
reward: tensor([-0.5234,  0.5078, -1.8438, -1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1894])
attention_mask shape: torch.Size([4, 1894])
reward: tensor([-0.7695, -1.4141,  0.5703, -0.6094], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1578])
attention_mask shape: torch.Size([4, 1578])
reward: tensor([-0.3594, -0.6133, -0.6523,  0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1304])
attention_mask shape: torch.Size([4, 1304])
reward: tensor([-0.4316,  0.1133,  1.1797, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.0400,  0.2754, -0.4707, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1499])
attention_mask shape: torch.Size([4, 1499])
reward: tensor([-1.1016, -0.8711, -0.1289,  0.2178], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1203])
attention_mask shape: torch.Size([4, 1203])
reward: tensor([ 1.0234,  0.0767,  0.8477, -1.7266], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1349])
attention_mask shape: torch.Size([4, 1349])
reward: tensor([-0.6406, -1.0312, -0.3379,  0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1784])
attention_mask shape: torch.Size([4, 1784])
reward: tensor([ 1.5547,  0.9922, -1.1562, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 831])
attention_mask shape: torch.Size([4, 831])
reward: tensor([ 0.2559, -1.1641,  1.2500, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.8203,  1.6094, -0.0400,  0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1733])
attention_mask shape: torch.Size([4, 1733])
reward: tensor([-0.5078, -0.6875,  0.3965, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1407])
attention_mask shape: torch.Size([4, 1407])
reward: tensor([ 0.2129,  1.3750, -1.0078, -1.5703], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1429])
attention_mask shape: torch.Size([4, 1429])
reward: tensor([-0.9062, -0.1602,  0.5859,  0.8945], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-0.5508, -1.5703, -0.5938, -0.5156], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1474])
attention_mask shape: torch.Size([4, 1474])
reward: tensor([0.4844, 0.0776, 0.0488, 0.8281], device='cuda:0', dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1779])
attention_mask shape: torch.Size([4, 1779])
reward: tensor([-0.9688, -0.5273, -0.7617, -0.0601], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1105])
attention_mask shape: torch.Size([4, 1105])
reward: tensor([-1.6562,  0.3379,  0.5234, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 538])
attention_mask shape: torch.Size([4, 538])
reward: tensor([ 0.0835, -1.3359, -0.4180, -1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1792])
attention_mask shape: torch.Size([4, 1792])
reward: tensor([-2.0000, -0.1221, -0.3281, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1844])
attention_mask shape: torch.Size([4, 1844])
reward: tensor([ 0.5938, -2.1094, -0.4629,  0.0222], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([-0.2197, -1.2891, -0.7305, -1.9766], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.0703,  0.2227, -1.5938,  1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1294])
attention_mask shape: torch.Size([4, 1294])
reward: tensor([-0.2070, -0.3066, -0.3164,  1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1347])
attention_mask shape: torch.Size([4, 1347])
reward: tensor([ 0.7656, -0.7148, -0.5547, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([ 0.1396, -1.4219, -0.4219, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6055, -1.5000,  0.3516, -1.7188], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1927])
attention_mask shape: torch.Size([4, 1927])
reward: tensor([-0.5547,  0.2617, -0.6875, -0.1846], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 348])
attention_mask shape: torch.Size([4, 348])
reward: tensor([ 0.6055, -0.7422, -1.0156, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2039])
attention_mask shape: torch.Size([4, 2039])
reward: tensor([-0.8164, -0.0645, -1.0469,  0.7539], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1476])
attention_mask shape: torch.Size([4, 1476])
reward: tensor([-2.0312, -0.3516, -0.9141, -0.2109], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1201])
attention_mask shape: torch.Size([4, 1201])
reward: tensor([0.0033, 0.6562, 0.2930, 0.5195], device='cuda:0', dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1273])
attention_mask shape: torch.Size([4, 1273])
reward: tensor([-0.8906, -0.9062, -1.1172, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1372])
attention_mask shape: torch.Size([4, 1372])
reward: tensor([-1.2031, -0.0957, -0.1885, -0.6250], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 991])
attention_mask shape: torch.Size([4, 991])
reward: tensor([-0.1001, -0.3457,  0.5938, -0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 866])
attention_mask shape: torch.Size([4, 866])
reward: tensor([-0.3965, -0.7109, -0.1045,  0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1708])
attention_mask shape: torch.Size([4, 1708])
reward: tensor([-1.3828, -0.0713,  0.7148, -1.6094], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([ 0.5430, -0.1221,  0.1777, -0.0601], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1641])
attention_mask shape: torch.Size([4, 1641])
reward: tensor([ 1.5859, -0.9336, -1.6094, -0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1829])
attention_mask shape: torch.Size([4, 1829])
reward: tensor([-2.0156,  0.2637,  0.3770, -0.7227], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1641])
attention_mask shape: torch.Size([4, 1641])
reward: tensor([-0.1777, -0.0444,  0.6914, -1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([ 0.9375, -1.6406, -0.2930, -0.0801], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1416])
attention_mask shape: torch.Size([4, 1416])
reward: tensor([-1.8281,  0.2480,  0.4316, -0.4492], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7344, -0.6328, -0.4258, -1.7266], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1917])
attention_mask shape: torch.Size([4, 1917])
reward: tensor([-1.8906,  0.3906, -1.6797, -0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3379, -0.5469, -1.7344,  0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1606])
attention_mask shape: torch.Size([4, 1606])
reward: tensor([ 0.3809,  1.0156,  0.4570, -0.6172], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.1094,  0.1553, -0.1982, -0.3516], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1594])
attention_mask shape: torch.Size([4, 1594])
reward: tensor([-0.5625, -1.4922, -1.2969, -1.7969], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.0234,  0.7852, -0.5273, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 644])
attention_mask shape: torch.Size([4, 644])
reward: tensor([ 0.7734, -0.0913, -0.0200,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1251])
attention_mask shape: torch.Size([4, 1251])
reward: tensor([-1.5391,  0.4609, -0.9688,  1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.1846, -1.5547,  0.2324, -0.4492], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.3516, -1.2344, -0.4980, -1.2891], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1856])
attention_mask shape: torch.Size([4, 1856])
reward: tensor([-1.6328, -1.3828, -0.4570, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 915])
attention_mask shape: torch.Size([4, 915])
reward: tensor([0.0610, 0.1201, 0.2852, 0.1758], device='cuda:0', dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.0781, -1.0859, -1.4453, -1.3984], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.6172, -0.9688, -1.1250, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1430])
attention_mask shape: torch.Size([4, 1430])
reward: tensor([ 0.3867, -0.6680,  1.4219, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1633])
attention_mask shape: torch.Size([4, 1633])
reward: tensor([-0.8477, -0.7070, -1.1094,  0.6328], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 791])
attention_mask shape: torch.Size([4, 791])
reward: tensor([-0.2930, -0.6016, -0.1396, -0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1502])
attention_mask shape: torch.Size([4, 1502])
reward: tensor([-0.9883, -0.8047, -0.8906, -1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1485])
attention_mask shape: torch.Size([4, 1485])
reward: tensor([ 0.3027, -0.5391, -0.7773, -0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1422])
attention_mask shape: torch.Size([4, 1422])
reward: tensor([-1.4766, -0.2793, -1.2734, -1.5547], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1497])
attention_mask shape: torch.Size([4, 1497])
reward: tensor([ 0.4082,  0.7109, -1.4688, -0.2471], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 705])
attention_mask shape: torch.Size([4, 705])
reward: tensor([ 1.4062, -0.2598,  0.4531, -0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([-2.0312, -0.3379, -1.3672, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([ 0.6094, -0.7148, -0.7305,  1.0000], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-0.8789,  0.3691, -1.6875, -0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-0.8438,  1.1250, -0.4746, -1.4453], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 949])
attention_mask shape: torch.Size([4, 949])
reward: tensor([-0.8984, -1.2188, -1.1016,  1.4766], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1345])
attention_mask shape: torch.Size([4, 1345])
reward: tensor([-0.1001, -1.7266, -0.0713, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 734])
attention_mask shape: torch.Size([4, 734])
reward: tensor([ 0.1289, -0.1396, -0.0933, -0.6250], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1647])
attention_mask shape: torch.Size([4, 1647])
reward: tensor([-0.3828, -1.5938, -0.4883,  0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1827])
attention_mask shape: torch.Size([4, 1827])
reward: tensor([-0.8086, -0.7344, -1.4297,  0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1461])
attention_mask shape: torch.Size([4, 1461])
reward: tensor([ 0.3105, -1.3125,  0.7539, -0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1389])
attention_mask shape: torch.Size([4, 1389])
reward: tensor([-0.1133,  1.5078,  0.9336, -1.7422], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1194])
attention_mask shape: torch.Size([4, 1194])
reward: tensor([-0.4082, -1.8203,  0.4785, -0.0510], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1185])
attention_mask shape: torch.Size([4, 1185])
reward: tensor([ 0.5117, -0.0089,  0.7383, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 863])
attention_mask shape: torch.Size([4, 863])
reward: tensor([0.0801, 0.1885, 0.6914, 0.9883], device='cuda:0', dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1898])
attention_mask shape: torch.Size([4, 1898])
reward: tensor([ 0.1875, -1.4453, -0.4141, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1340])
attention_mask shape: torch.Size([4, 1340])
reward: tensor([-1.7031, -0.9492, -0.1914, -0.8320], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.5156, -1.8047,  0.1035, -1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1496])
attention_mask shape: torch.Size([4, 1496])
reward: tensor([ 1.2656, -0.5039, -0.0422, -0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1132])
attention_mask shape: torch.Size([4, 1132])
reward: tensor([ 1.0859, -0.6211, -1.7031, -1.9219], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.4043, -1.0781,  0.0688, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1341])
attention_mask shape: torch.Size([4, 1341])
reward: tensor([1.2422, 0.0432, 0.1211, 0.1523], device='cuda:0', dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1857])
attention_mask shape: torch.Size([4, 1857])
reward: tensor([-0.9336,  0.2373, -1.3828,  1.4844], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.3516, -0.9961,  1.4609, -1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 699])
attention_mask shape: torch.Size([4, 699])
reward: tensor([-0.3730, -1.1406, -1.0234,  1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1680])
attention_mask shape: torch.Size([4, 1680])
reward: tensor([-0.1582, -0.8086, -1.1250,  1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1120])
attention_mask shape: torch.Size([4, 1120])
reward: tensor([-0.4746,  1.3672, -0.0801,  0.2910], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.1357, -0.5352,  0.5938, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1972])
attention_mask shape: torch.Size([4, 1972])
reward: tensor([ 0.1143, -0.2812,  0.0854, -1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1591])
attention_mask shape: torch.Size([4, 1591])
reward: tensor([-0.4570, -1.8984,  0.2598,  0.1562], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1472])
attention_mask shape: torch.Size([4, 1472])
reward: tensor([-0.5703,  0.3652,  0.1299, -1.1797], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1516])
attention_mask shape: torch.Size([4, 1516])
reward: tensor([-0.6914, -1.1016, -0.3027, -0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1441])
attention_mask shape: torch.Size([4, 1441])
reward: tensor([-0.6875, -0.1582, -0.6523, -0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 870])
attention_mask shape: torch.Size([4, 870])
reward: tensor([-0.3418,  0.1582, -1.4141, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1612])
attention_mask shape: torch.Size([4, 1612])
reward: tensor([-0.5781, -0.7109, -1.0234,  1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([ 1.5625, -1.9766, -0.9883, -0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.5078, -0.8984,  1.4609, -0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.5234, -1.4141,  0.5898, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1431])
attention_mask shape: torch.Size([4, 1431])
reward: tensor([ 0.2471,  0.3086,  0.1235, -0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.0977, -0.4004, -0.6094,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.5742, -2.1719, -0.5391, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1341])
attention_mask shape: torch.Size([4, 1341])
reward: tensor([-0.6992, -0.0111,  0.8828,  0.2480], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1062])
attention_mask shape: torch.Size([4, 1062])
reward: tensor([-0.5156, -0.5352,  1.5000,  0.8359], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.1016,  0.9648, -0.9766, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6367, -2.2031, -0.2773, -0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-1.8125, -2.0781, -2.2031, -0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.0864,  0.0601,  0.6836, -1.5859], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1304])
attention_mask shape: torch.Size([4, 1304])
reward: tensor([-1.3359, -0.2578, -0.0510,  0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1229])
attention_mask shape: torch.Size([4, 1229])
reward: tensor([-0.1113, -1.0156,  1.5938,  0.0623], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1001])
attention_mask shape: torch.Size([4, 1001])
reward: tensor([ 0.3379, -0.4883, -0.2969, -1.2344], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1852])
attention_mask shape: torch.Size([4, 1852])
reward: tensor([-2.1406,  1.5859,  0.5664, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.1797,  0.2090, -0.1826, -1.7578], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.0532, -1.0469, -0.5938,  0.2080], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([-0.9609, -1.9844, -0.7070,  0.2969], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-0.7695, -0.0045, -0.8594, -0.4355], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1115])
attention_mask shape: torch.Size([4, 1115])
reward: tensor([ 0.3906, -2.1719, -1.1250, -0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1575])
attention_mask shape: torch.Size([4, 1575])
reward: tensor([ 0.2451, -1.0859, -1.1172,  0.1689], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 559])
attention_mask shape: torch.Size([4, 559])
reward: tensor([ 0.0732, -0.5703, -0.6328, -1.8125], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1249])
attention_mask shape: torch.Size([4, 1249])
reward: tensor([-1.3672, -0.3457, -0.1201,  0.3770], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1879])
attention_mask shape: torch.Size([4, 1879])
reward: tensor([ 0.5078, -1.0312,  0.2393, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.1016, -0.2910, -1.9453,  0.1602], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.7812, -1.4609, -0.6641,  0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1283])
attention_mask shape: torch.Size([4, 1283])
reward: tensor([-1.5703, -0.1865, -1.9141, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1333])
attention_mask shape: torch.Size([4, 1333])
reward: tensor([-1.7266, -0.3281, -0.6992, -1.6094], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-1.7656,  1.9453,  0.4629, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1137])
attention_mask shape: torch.Size([4, 1137])
reward: tensor([-0.0933, -0.5547, -0.7383, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2013])
attention_mask shape: torch.Size([4, 2013])
reward: tensor([ 0.2852,  0.2734,  0.2441, -0.1089], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1786])
attention_mask shape: torch.Size([4, 1786])
reward: tensor([ 0.0056,  0.8047,  0.1514, -0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([-0.3594, -0.4141,  1.8281, -0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1134])
attention_mask shape: torch.Size([4, 1134])
reward: tensor([-1.2891,  2.0469,  0.9297, -1.2344], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1867])
attention_mask shape: torch.Size([4, 1867])
reward: tensor([ 1.0469, -0.4805, -0.4531, -0.5508], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1306])
attention_mask shape: torch.Size([4, 1306])
reward: tensor([-0.8984, -1.8281,  0.9492, -1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1175])
attention_mask shape: torch.Size([4, 1175])
reward: tensor([ 0.5547, -0.9688, -1.8281,  1.0625], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1553])
attention_mask shape: torch.Size([4, 1553])
reward: tensor([-0.3418, -0.8281, -1.4688, -0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1746])
attention_mask shape: torch.Size([4, 1746])
reward: tensor([-0.1484,  0.1748, -0.5078,  1.8984], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1235])
attention_mask shape: torch.Size([4, 1235])
reward: tensor([-0.7617, -2.1406, -1.3750, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1445])
attention_mask shape: torch.Size([4, 1445])
reward: tensor([-0.6836, -1.9766, -0.9336,  1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1741])
attention_mask shape: torch.Size([4, 1741])
reward: tensor([ 1.6016, -1.7500, -0.3652, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1722])
attention_mask shape: torch.Size([4, 1722])
reward: tensor([-1.1875, -1.1016, -0.2559,  1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 681])
attention_mask shape: torch.Size([4, 681])
reward: tensor([ 0.2109, -1.2812, -0.1689, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.4609,  0.3398, -0.5430, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1608])
attention_mask shape: torch.Size([4, 1608])
reward: tensor([-1.0078, -2.1250, -1.4453,  1.0625], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1379])
attention_mask shape: torch.Size([4, 1379])
reward: tensor([ 0.9922, -2.1406, -0.5898, -0.8398], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1621])
attention_mask shape: torch.Size([4, 1621])
reward: tensor([ 0.2021, -0.4941, -1.0469, -0.0356], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1477])
attention_mask shape: torch.Size([4, 1477])
reward: tensor([ 1.4688, -0.7773,  1.8984, -0.1865], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1490])
attention_mask shape: torch.Size([4, 1490])
reward: tensor([ 2.1719, -0.7461, -0.1289,  1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1457])
attention_mask shape: torch.Size([4, 1457])
reward: tensor([-0.3027, -0.2422,  0.0742, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2014])
attention_mask shape: torch.Size([4, 2014])
reward: tensor([-0.0289, -1.0547, -0.7852,  0.7305], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1288])
attention_mask shape: torch.Size([4, 1288])
reward: tensor([-2.1719, -1.0938, -1.6016,  0.4492], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6523, -1.9609, -0.1338, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1866])
attention_mask shape: torch.Size([4, 1866])
reward: tensor([ 1.1328, -0.4629, -1.4688, -1.1797], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1639])
attention_mask shape: torch.Size([4, 1639])
reward: tensor([-0.4531, -2.1719,  1.0547,  0.4609], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1224])
attention_mask shape: torch.Size([4, 1224])
reward: tensor([-0.4980,  2.1250,  0.7812, -0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6172, -1.8828,  0.5547, -1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1731])
attention_mask shape: torch.Size([4, 1731])
reward: tensor([ 0.5859, -0.8164,  0.4570, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.5625,  0.0977, -0.0089,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.5859, -0.9961,  1.9141, -0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 895])
attention_mask shape: torch.Size([4, 895])
reward: tensor([-1.6250,  0.3516, -0.9961,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1092])
attention_mask shape: torch.Size([4, 1092])
reward: tensor([ 0.2266, -1.2656,  1.0469,  0.2178], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1615])
attention_mask shape: torch.Size([4, 1615])
reward: tensor([-1.0938, -0.1602, -0.6445, -0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1987])
attention_mask shape: torch.Size([4, 1987])
reward: tensor([-1.7812, -1.3125, -1.3984, -0.2471], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1759])
attention_mask shape: torch.Size([4, 1759])
reward: tensor([ 0.3691, -0.7773, -0.6875,  0.1631], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1109])
attention_mask shape: torch.Size([4, 1109])
reward: tensor([-1.4219,  1.6250, -0.6992,  0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1587])
attention_mask shape: torch.Size([4, 1587])
reward: tensor([ 0.1631,  1.0391, -0.8008, -1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 320])
attention_mask shape: torch.Size([4, 320])
reward: tensor([-0.4043, -0.3340,  0.2041, -1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([ 0.1807, -1.0156,  1.4453, -1.9219], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([1.8438, 0.1826, 0.5117, 0.1982], device='cuda:0', dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-0.6211,  0.9336,  0.7422, -0.2246], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-22 03:01:37,175] [INFO] [launch.py:351:main] Process 608691 exits successfully.
[2024-10-22 03:02:18,218] [INFO] [launch.py:351:main] Process 608694 exits successfully.
[2024-10-22 03:02:53,263] [INFO] [launch.py:351:main] Process 608692 exits successfully.
[2024-10-22 03:03:22,319] [INFO] [launch.py:351:main] Process 608693 exits successfully.
+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-3th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-3th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_3 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-22 03:03:27,406] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 03:03:29,311] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-22 03:03:29,311] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-3th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_3 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-22 03:03:31,910] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 03:03:33,747] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-22 03:03:33,748] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-22 03:03:33,748] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-22 03:03:33,748] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-22 03:03:33,748] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-22 03:03:33,748] [INFO] [launch.py:256:main] process 609939 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-3th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_3', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 03:03:33,748] [INFO] [launch.py:256:main] process 609940 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-3th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_3', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 03:03:33,749] [INFO] [launch.py:256:main] process 609941 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-3th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_3', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 03:03:33,749] [INFO] [launch.py:256:main] process 609942 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-3th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_3', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 03:03:35,732] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 03:03:35,763] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 03:03:35,770] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 03:03:35,770] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-22 03:03:38,897] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 03:03:38,897] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-22 03:03:39,588] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 03:03:39,588] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 03:03:39,589] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.56it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.50it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.36it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.37it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.30it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.48it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.36it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.38it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.27it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.71it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.63it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.39it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.40it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.36it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.64it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.53it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.64it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.54it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.67it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.54it/s]
[2024-10-22 03:03:51,929] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:03:51,965] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:04:01,563] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-22 03:04:01,758] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-22 03:04:01,758] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-22 03:04:01,758] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:04:03,105] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-22 03:04:03,106] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-22 03:04:03,107] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:04:03,107] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:04:03,107] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:04:03,228] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-22 03:04:03,228] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-22 03:04:03,229] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.97 GB, percent = 2.5%
[2024-10-22 03:04:03,363] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-22 03:04:03,363] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-22 03:04:03,363] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.97 GB, percent = 2.5%
[2024-10-22 03:04:03,364] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0894f3ebc0>
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-22 03:04:03,365] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-22 03:04:03,366] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-22 03:04:03,367] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-22 03:04:03,367] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-22 03:04:03,367] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-22 03:04:03,367] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-22 03:04:03,367] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-22 03:04:03,367] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-22 03:04:03,367] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-22 03:04:03,367] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:04:08,444] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-22 03:04:08,446] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-22 03:04:08,578] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-22 03:04:08,579] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-22 03:04:08,579] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.0 GB, percent = 2.5%
[2024-10-22 03:04:08,696] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-22 03:04:08,697] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-22 03:04:08,697] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.0 GB, percent = 2.5%
[2024-10-22 03:04:08,698] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-22 03:04:08,698] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-22 03:04:08,698] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-22 03:04:08,698] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-22 03:04:08,698] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-22 03:04:08,698] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f088c73a890>
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-22 03:04:08,699] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-22 03:04:08,700] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-22 03:04:08,700] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 617/100000 [00:00<00:16, 6165.49it/s]Preprocessing data:   2%|█▍                                                                                          | 1624/100000 [00:00<00:11, 8461.04it/s]Preprocessing data:   3%|██▍                                                                                         | 2617/100000 [00:00<00:10, 9131.40it/s]Preprocessing data:   4%|███▎                                                                                        | 3622/100000 [00:00<00:10, 9492.95it/s]Preprocessing data:   5%|████▎                                                                                       | 4621/100000 [00:00<00:09, 9668.25it/s]Preprocessing data:   6%|█████▏                                                                                      | 5618/100000 [00:00<00:09, 9767.98it/s]Preprocessing data:   7%|██████                                                                                      | 6614/100000 [00:00<00:09, 9830.50it/s]Preprocessing data:   8%|██████▉                                                                                     | 7603/100000 [00:00<00:09, 9849.13it/s]Preprocessing data:   9%|███████▉                                                                                    | 8608/100000 [00:00<00:09, 9911.44it/s]Preprocessing data:  10%|████████▊                                                                                   | 9609/100000 [00:01<00:09, 9940.59it/s]Preprocessing data:  11%|█████████▌                                                                                | 10646/100000 [00:01<00:08, 10070.75it/s]Preprocessing data:  12%|██████████▌                                                                               | 11697/100000 [00:01<00:08, 10201.37it/s]Preprocessing data:  13%|███████████▍                                                                              | 12758/100000 [00:01<00:08, 10322.48it/s]Preprocessing data:  14%|████████████▍                                                                             | 13817/100000 [00:01<00:08, 10402.97it/s]Preprocessing data:  15%|█████████████▍                                                                            | 14874/100000 [00:01<00:08, 10450.35it/s]Preprocessing data:  16%|██████████████▎                                                                           | 15920/100000 [00:01<00:08, 10392.98it/s]Preprocessing data:  17%|███████████████▎                                                                          | 16961/100000 [00:01<00:07, 10397.79it/s]Preprocessing data:  18%|████████████████▏                                                                         | 18005/100000 [00:01<00:07, 10407.69it/s]Preprocessing data:  19%|█████████████████▏                                                                        | 19061/100000 [00:01<00:07, 10450.04it/s]Preprocessing data:  20%|██████████████████                                                                        | 20116/100000 [00:02<00:07, 10479.61it/s]Preprocessing data:  21%|███████████████████                                                                       | 21191/100000 [00:02<00:07, 10558.03it/s]Preprocessing data:  22%|████████████████████                                                                      | 22258/100000 [00:02<00:07, 10589.59it/s]Preprocessing data:  23%|████████████████████▉                                                                     | 23317/100000 [00:02<00:07, 10539.42it/s]Preprocessing data:  24%|█████████████████████▉                                                                    | 24372/100000 [00:02<00:07, 10464.94it/s]Preprocessing data:  25%|██████████████████████▉                                                                   | 25419/100000 [00:02<00:07, 10441.20it/s]Preprocessing data:  26%|███████████████████████▊                                                                  | 26464/100000 [00:02<00:07, 10413.56it/s]Preprocessing data:  28%|████████████████████████▊                                                                 | 27506/100000 [00:02<00:06, 10376.23it/s]Preprocessing data:  29%|█████████████████████████▋                                                                | 28544/100000 [00:02<00:06, 10366.21it/s]Preprocessing data:  30%|██████████████████████████▌                                                               | 29581/100000 [00:02<00:06, 10328.60it/s]Preprocessing data:  31%|███████████████████████████▌                                                              | 30615/100000 [00:03<00:06, 10330.30it/s]Preprocessing data:  32%|████████████████████████████▍                                                             | 31649/100000 [00:03<00:06, 10325.53it/s]Preprocessing data:  33%|█████████████████████████████▍                                                            | 32683/100000 [00:03<00:06, 10328.06it/s]Preprocessing data:  34%|██████████████████████████████▎                                                           | 33716/100000 [00:03<00:06, 10250.78it/s]Preprocessing data:  35%|███████████████████████████████▎                                                          | 34750/100000 [00:03<00:06, 10275.90it/s]Preprocessing data:  36%|████████████████████████████████▏                                                         | 35778/100000 [00:03<00:06, 10270.55it/s]Preprocessing data:  37%|█████████████████████████████████▏                                                        | 36809/100000 [00:03<00:06, 10279.49it/s]Preprocessing data:  38%|██████████████████████████████████                                                        | 37838/100000 [00:03<00:06, 10237.96it/s]Preprocessing data:  39%|██████████████████████████████████▉                                                       | 38863/100000 [00:03<00:05, 10239.01it/s]Preprocessing data:  40%|███████████████████████████████████▉                                                      | 39894/100000 [00:03<00:05, 10258.12it/s]Preprocessing data:  41%|████████████████████████████████████▊                                                     | 40929/100000 [00:04<00:05, 10282.89it/s]Preprocessing data:  42%|█████████████████████████████████████▊                                                    | 41958/100000 [00:04<00:05, 10230.85it/s]Preprocessing data:  43%|██████████████████████████████████████▋                                                   | 42992/100000 [00:04<00:05, 10261.71it/s]Preprocessing data:  44%|███████████████████████████████████████▋                                                  | 44046/100000 [00:04<00:05, 10344.04it/s]Preprocessing data:  45%|████████████████████████████████████████▌                                                 | 45106/100000 [00:04<00:05, 10418.44it/s]Preprocessing data:  46%|█████████████████████████████████████████▌                                                | 46165/100000 [00:04<00:05, 10468.12it/s]Preprocessing data:  47%|██████████████████████████████████████████▍                                               | 47222/100000 [00:04<00:05, 10497.66it/s]Preprocessing data:  48%|███████████████████████████████████████████▍                                              | 48280/100000 [00:04<00:04, 10521.30it/s]Preprocessing data:  49%|████████████████████████████████████████████▍                                             | 49338/100000 [00:04<00:04, 10537.66it/s]Preprocessing data:  50%|█████████████████████████████████████████████▎                                            | 50399/100000 [00:04<00:04, 10559.02it/s]Preprocessing data:  51%|██████████████████████████████████████████████▎                                           | 51455/100000 [00:05<00:04, 10540.73it/s]Preprocessing data:  53%|███████████████████████████████████████████████▎                                          | 52513/100000 [00:05<00:04, 10551.75it/s]Preprocessing data:  54%|████████████████████████████████████████████████▏                                         | 53573/100000 [00:05<00:04, 10563.36it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▏                                        | 54632/100000 [00:05<00:04, 10568.73it/s]Preprocessing data:  56%|██████████████████████████████████████████████████                                        | 55694/100000 [00:05<00:04, 10583.69it/s]Preprocessing data:  57%|███████████████████████████████████████████████████                                       | 56759/100000 [00:05<00:04, 10600.54it/s]Preprocessing data:  58%|████████████████████████████████████████████████████                                      | 57823/100000 [00:05<00:03, 10610.17it/s]Preprocessing data:  59%|████████████████████████████████████████████████████▉                                     | 58885/100000 [00:05<00:03, 10545.05it/s]Preprocessing data:  60%|█████████████████████████████████████████████████████▉                                    | 59940/100000 [00:05<00:03, 10366.36it/s]Preprocessing data:  61%|██████████████████████████████████████████████████████▉                                   | 60979/100000 [00:05<00:03, 10372.47it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▊                                  | 62033/100000 [00:06<00:03, 10421.03it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▊                                 | 63100/100000 [00:06<00:03, 10494.79it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▋                                | 64152/100000 [00:06<00:03, 10500.84it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▋                               | 65206/100000 [00:06<00:03, 10511.65it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▋                              | 66258/100000 [00:06<00:03, 10499.40it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████▌                             | 67309/100000 [00:06<00:03, 10490.35it/s]Preprocessing data:  68%|█████████████████████████████████████████████████████████████▌                            | 68359/100000 [00:06<00:03, 10460.87it/s]Preprocessing data:  69%|██████████████████████████████████████████████████████████████▍                           | 69406/100000 [00:06<00:02, 10404.25it/s]Preprocessing data:  70%|███████████████████████████████████████████████████████████████▍                          | 70454/100000 [00:06<00:02, 10423.55it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▎                         | 71500/100000 [00:06<00:02, 10433.53it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▎                        | 72544/100000 [00:07<00:02, 10376.31it/s]Preprocessing data:  74%|██████████████████████████████████████████████████████████████████▉                        | 73582/100000 [00:07<00:02, 9453.20it/s]Preprocessing data:  75%|███████████████████████████████████████████████████████████████████▊                       | 74543/100000 [00:07<00:02, 8914.45it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▋                      | 75450/100000 [00:07<00:02, 8581.05it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████▍                     | 76319/100000 [00:07<00:02, 8347.22it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▏                    | 77161/100000 [00:07<00:02, 8201.10it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▉                    | 77986/100000 [00:07<00:02, 8077.11it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▋                   | 78797/100000 [00:07<00:02, 7954.75it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▍                  | 79594/100000 [00:07<00:02, 7874.10it/s]Preprocessing data:  80%|█████████████████████████████████████████████████████████████████████████▏                 | 80383/100000 [00:08<00:02, 7639.18it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▉                 | 81229/100000 [00:08<00:02, 7870.52it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▋                | 82125/100000 [00:08<00:02, 8181.52it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▍               | 82957/100000 [00:08<00:02, 8221.25it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▏              | 83782/100000 [00:08<00:02, 7979.50it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████              | 84731/100000 [00:08<00:01, 8415.07it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████             | 85755/100000 [00:08<00:01, 8947.97it/s]Preprocessing data:  87%|██████████████████████████████████████████████████████████████████████████████▊            | 86654/100000 [00:08<00:01, 8479.78it/s]Preprocessing data:  88%|███████████████████████████████████████████████████████████████████████████████▋           | 87523/100000 [00:08<00:01, 8537.68it/s]Preprocessing data:  89%|████████████████████████████████████████████████████████████████████████████████▌          | 88502/100000 [00:08<00:01, 8898.35it/s]Preprocessing data:  89%|█████████████████████████████████████████████████████████████████████████████████▍         | 89471/100000 [00:09<00:01, 9126.91it/s]Preprocessing data:  90%|██████████████████████████████████████████████████████████████████████████████████▎        | 90388/100000 [00:09<00:01, 8917.49it/s]Preprocessing data:  91%|███████████████████████████████████████████████████████████████████████████████████        | 91284/100000 [00:09<00:01, 8262.71it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▊       | 92122/100000 [00:09<00:01, 7846.14it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▌      | 92917/100000 [00:09<00:00, 7581.58it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▎     | 93683/100000 [00:09<00:00, 7529.63it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████     | 94568/100000 [00:09<00:00, 7896.69it/s]Preprocessing data:  96%|██████████████████████████████████████████████████████████████████████████████████████▉    | 95506/100000 [00:09<00:00, 8317.12it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▋   | 96351/100000 [00:09<00:00, 8350.88it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▍  | 97191/100000 [00:10<00:00, 8250.65it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▏ | 98069/100000 [00:10<00:00, 8403.61it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████ | 99037/100000 [00:10<00:00, 8777.98it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9651.63it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1357])
attention_mask shape: torch.Size([4, 1357])
reward: tensor([-0.1309, -0.8125, -0.9883, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1401])
attention_mask shape: torch.Size([4, 1401])
reward: tensor([ 1.8984, -1.2969, -1.3438,  0.0200], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1054])
attention_mask shape: torch.Size([4, 1054])
reward: tensor([-1.7188, -1.1641, -1.9766, -0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 321])
attention_mask shape: torch.Size([4, 321])
reward: tensor([-1.7578, -0.3770,  0.2227,  0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1444])
attention_mask shape: torch.Size([4, 1444])
reward: tensor([ 0.6367,  0.4980, -1.0781, -0.2539], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1083])
attention_mask shape: torch.Size([4, 1083])
reward: tensor([ 1.8984, -0.8984, -0.8047,  0.1855], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1143])
attention_mask shape: torch.Size([4, 1143])
reward: tensor([-0.5156, -0.2715, -2.0000, -1.7266], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 906])
attention_mask shape: torch.Size([4, 906])
reward: tensor([ 0.1826,  0.0388, -0.0378, -1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 859])
attention_mask shape: torch.Size([4, 859])
reward: tensor([-0.7812, -0.6055,  0.0522, -0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1103])
attention_mask shape: torch.Size([4, 1103])
reward: tensor([-0.0845,  0.2021,  0.0366, -1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 710])
attention_mask shape: torch.Size([4, 710])
reward: tensor([-2.1719, -0.8438, -0.3203, -0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 985])
attention_mask shape: torch.Size([4, 985])
reward: tensor([-1.3359, -0.4707, -0.6875, -0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1041])
attention_mask shape: torch.Size([4, 1041])
reward: tensor([-1.6250, -0.4180, -0.2490,  0.1357], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1346])
attention_mask shape: torch.Size([4, 1346])
reward: tensor([-0.4629,  0.3516,  1.2031, -1.9531], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 214])
attention_mask shape: torch.Size([4, 214])
reward: tensor([-1.1562, -0.4141, -0.3828, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.4141, -0.1245, -0.3516, -1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1330])
attention_mask shape: torch.Size([4, 1330])
reward: tensor([-0.8516, -1.9844, -0.7422,  1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([ 0.3594, -0.0378, -0.8281,  0.2539], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 939])
attention_mask shape: torch.Size([4, 939])
reward: tensor([ 0.8945, -0.5898,  0.8203, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 716])
attention_mask shape: torch.Size([4, 716])
reward: tensor([ 0.9258, -0.8516, -1.2656, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1085])
attention_mask shape: torch.Size([4, 1085])
reward: tensor([-1.0859, -1.4922, -1.9922, -0.0045], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1120])
attention_mask shape: torch.Size([4, 1120])
reward: tensor([ 0.2422,  0.0189, -0.8789, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 811])
attention_mask shape: torch.Size([4, 811])
reward: tensor([-1.1484,  0.5195, -0.3457, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1390])
attention_mask shape: torch.Size([4, 1390])
reward: tensor([-2.2031,  0.0776, -1.4453,  0.2373], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1271])
attention_mask shape: torch.Size([4, 1271])
reward: tensor([ 0.5547, -0.7305, -0.0977, -0.0422], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1357])
attention_mask shape: torch.Size([4, 1357])
reward: tensor([ 0.6133, -0.7109, -0.1865, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1781])
attention_mask shape: torch.Size([4, 1781])
reward: tensor([-0.5742, -0.5508, -0.6094, -0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 282])
attention_mask shape: torch.Size([4, 282])
reward: tensor([-0.1709, -1.0469, -0.9336, -1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 772])
attention_mask shape: torch.Size([4, 772])
reward: tensor([ 0.3672, -1.5078,  0.1758, -0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1087])
attention_mask shape: torch.Size([4, 1087])
reward: tensor([-0.2715, -0.7539, -0.2471, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1607])
attention_mask shape: torch.Size([4, 1607])
reward: tensor([-1.0391,  0.4980, -1.9766, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1485])
attention_mask shape: torch.Size([4, 1485])
reward: tensor([-0.9961, -0.1465, -1.4766, -0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1559])
attention_mask shape: torch.Size([4, 1559])
reward: tensor([-1.7266, -0.3340,  1.9297, -0.7773], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1108])
attention_mask shape: torch.Size([4, 1108])
reward: tensor([ 0.2617, -0.7500, -1.6328, -0.4395], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 768])
attention_mask shape: torch.Size([4, 768])
reward: tensor([-0.4805, -0.8047, -0.6562, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1422])
attention_mask shape: torch.Size([4, 1422])
reward: tensor([ 1.2812, -0.3457,  0.7031, -1.5703], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1030])
attention_mask shape: torch.Size([4, 1030])
reward: tensor([ 0.3867, -0.5469, -0.5703,  0.0167], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1081])
attention_mask shape: torch.Size([4, 1081])
reward: tensor([ 0.3027, -1.1641, -0.1289, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 804])
attention_mask shape: torch.Size([4, 804])
reward: tensor([-0.3164, -1.8203,  0.2910, -1.8047], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1386])
attention_mask shape: torch.Size([4, 1386])
reward: tensor([ 0.2324, -0.5156, -0.1045,  1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1042])
attention_mask shape: torch.Size([4, 1042])
reward: tensor([-0.3242, -0.4883,  0.0200,  0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([ 1.4297, -0.8398, -0.1621, -0.1885], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 278])
attention_mask shape: torch.Size([4, 278])
reward: tensor([-0.1201, -0.3965, -1.4609, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 553])
attention_mask shape: torch.Size([4, 553])
reward: tensor([-0.6992, -0.6641, -0.7383,  0.0089], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1705])
attention_mask shape: torch.Size([4, 1705])
reward: tensor([-0.7227,  1.5703,  0.5234,  1.2891], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 682])
attention_mask shape: torch.Size([4, 682])
reward: tensor([-0.2266, -0.3965, -0.5938, -0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 633])
attention_mask shape: torch.Size([4, 633])
reward: tensor([ 0.0000, -1.8516,  0.7500, -0.1201], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3242, -1.4297, -0.1602,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1432])
attention_mask shape: torch.Size([4, 1432])
reward: tensor([-1.8672, -1.2188, -0.2373, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 698])
attention_mask shape: torch.Size([4, 698])
reward: tensor([-1.0078,  0.0723,  0.3730,  0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-1.2500, -0.0400, -0.8125, -0.4883], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1157])
attention_mask shape: torch.Size([4, 1157])
reward: tensor([-0.5703,  1.6016,  0.0022,  0.6445], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1349])
attention_mask shape: torch.Size([4, 1349])
reward: tensor([-0.2598, -0.6406, -0.3730, -1.5469], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([-2.1406, -0.2754,  0.2559,  0.4551], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1510])
attention_mask shape: torch.Size([4, 1510])
reward: tensor([-0.5430,  0.7109, -0.3027, -0.1826], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 747])
attention_mask shape: torch.Size([4, 747])
reward: tensor([-1.4375, -0.8281, -2.0156, -1.9297], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 967])
attention_mask shape: torch.Size([4, 967])
reward: tensor([-0.8711, -0.8906, -1.1172, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 523])
attention_mask shape: torch.Size([4, 523])
reward: tensor([-1.0938,  0.5938,  0.2002, -0.6445], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1318])
attention_mask shape: torch.Size([4, 1318])
reward: tensor([ 0.7305,  0.5703, -0.9883, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1156])
attention_mask shape: torch.Size([4, 1156])
reward: tensor([-0.9883, -1.5938, -1.8516, -0.0776], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 952])
attention_mask shape: torch.Size([4, 952])
reward: tensor([-0.8398, -1.0703, -1.8125, -1.6328], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1408])
attention_mask shape: torch.Size([4, 1408])
reward: tensor([-1.1875, -0.8359,  0.8750,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1326])
attention_mask shape: torch.Size([4, 1326])
reward: tensor([-0.2969, -0.4941, -1.4609,  0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 812])
attention_mask shape: torch.Size([4, 812])
reward: tensor([-0.9492,  0.2266,  0.7930, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 908])
attention_mask shape: torch.Size([4, 908])
reward: tensor([ 0.8008, -0.0510, -0.6797, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 937])
attention_mask shape: torch.Size([4, 937])
reward: tensor([ 0.3203, -0.9258, -1.8906,  0.4102], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 988])
attention_mask shape: torch.Size([4, 988])
reward: tensor([-0.5703,  0.1079, -0.9609, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1246])
attention_mask shape: torch.Size([4, 1246])
reward: tensor([-0.6992, -0.9492, -0.1689,  1.4297], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 315])
attention_mask shape: torch.Size([4, 315])
reward: tensor([-1.2734, -0.0977, -0.6172,  0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1324])
attention_mask shape: torch.Size([4, 1324])
reward: tensor([-0.9688, -0.9141,  0.2676,  0.1099], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1223])
attention_mask shape: torch.Size([4, 1223])
reward: tensor([ 0.5195, -0.7930, -1.5234, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 678])
attention_mask shape: torch.Size([4, 678])
reward: tensor([-0.2197, -0.9258, -1.9922, -1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1092])
attention_mask shape: torch.Size([4, 1092])
reward: tensor([-0.8203, -0.9336,  0.6719, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1214])
attention_mask shape: torch.Size([4, 1214])
reward: tensor([0.2695, 0.9727, 0.3594, 0.4688], device='cuda:0', dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 475])
attention_mask shape: torch.Size([4, 475])
reward: tensor([-1.6328, -0.3203, -1.2344,  0.0178], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 553])
attention_mask shape: torch.Size([4, 553])
reward: tensor([-0.8984, -0.9062, -1.2109, -1.6953], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1349])
attention_mask shape: torch.Size([4, 1349])
reward: tensor([ 1.7031, -0.3027, -1.3125, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1209])
attention_mask shape: torch.Size([4, 1209])
reward: tensor([ 0.4531, -1.0859, -0.3066, -1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1487])
attention_mask shape: torch.Size([4, 1487])
reward: tensor([-1.4062,  1.5312,  2.0469, -1.8828], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 735])
attention_mask shape: torch.Size([4, 735])
reward: tensor([-1.4844, -2.1562,  0.1797, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1066])
attention_mask shape: torch.Size([4, 1066])
reward: tensor([-0.3828, -0.6641,  1.7344, -1.9531], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 438])
attention_mask shape: torch.Size([4, 438])
reward: tensor([-0.5508, -0.4492, -0.3770, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1522])
attention_mask shape: torch.Size([4, 1522])
reward: tensor([-0.5586,  0.8125,  0.9453, -0.0688], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 454])
attention_mask shape: torch.Size([4, 454])
reward: tensor([-0.0422, -0.1289, -0.6914, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1494])
attention_mask shape: torch.Size([4, 1494])
reward: tensor([ 0.1416, -0.6328,  0.1729,  0.2812], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1573])
attention_mask shape: torch.Size([4, 1573])
reward: tensor([-0.6133, -0.6328, -0.7383,  1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1217])
attention_mask shape: torch.Size([4, 1217])
reward: tensor([-0.9258, -0.8477,  1.4219, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1523])
attention_mask shape: torch.Size([4, 1523])
reward: tensor([-1.0078, -0.7383,  0.4609, -0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1423])
attention_mask shape: torch.Size([4, 1423])
reward: tensor([ 0.1582, -0.2969,  0.8750, -0.2158], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 931])
attention_mask shape: torch.Size([4, 931])
reward: tensor([-0.5195, -0.6406,  0.9258, -1.7891], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 840])
attention_mask shape: torch.Size([4, 840])
reward: tensor([-1.6953,  1.3672, -0.0289,  1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1470])
attention_mask shape: torch.Size([4, 1470])
reward: tensor([ 1.1016,  0.5547, -0.7852, -1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 240])
attention_mask shape: torch.Size([4, 240])
reward: tensor([-0.7031,  0.0432, -2.1250, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1593])
attention_mask shape: torch.Size([4, 1593])
reward: tensor([ 0.3828,  0.5156, -0.6406, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1200])
attention_mask shape: torch.Size([4, 1200])
reward: tensor([-0.5742, -1.7500,  1.1016, -1.6172], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 796])
attention_mask shape: torch.Size([4, 796])
reward: tensor([ 0.4707, -0.4844, -1.5547, -0.0889], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1597])
attention_mask shape: torch.Size([4, 1597])
reward: tensor([-1.3359,  0.3535,  0.3008, -0.1001], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-1.3516,  0.1934,  0.6133,  0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 671])
attention_mask shape: torch.Size([4, 671])
reward: tensor([-0.0178, -0.4141,  0.4082, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 996])
attention_mask shape: torch.Size([4, 996])
reward: tensor([-0.2598, -0.5273,  0.2871, -1.8203], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1105])
attention_mask shape: torch.Size([4, 1105])
reward: tensor([-1.5625, -0.4883, -0.2930, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 429])
attention_mask shape: torch.Size([4, 429])
reward: tensor([ 0.1309, -0.9414, -0.7617, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 886])
attention_mask shape: torch.Size([4, 886])
reward: tensor([-0.3691, -0.6484,  0.1943, -1.8281], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1159])
attention_mask shape: torch.Size([4, 1159])
reward: tensor([-1.0938, -0.9414,  0.8672,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([ 0.0933, -0.8477,  0.7383, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1416])
attention_mask shape: torch.Size([4, 1416])
reward: tensor([-0.6523,  0.1670,  0.0654,  1.6875], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1228])
attention_mask shape: torch.Size([4, 1228])
reward: tensor([-1.0938,  0.1445, -1.4453,  0.0432], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 559])
attention_mask shape: torch.Size([4, 559])
reward: tensor([ 0.3379, -0.5352, -0.3105, -1.3750], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 133])
attention_mask shape: torch.Size([4, 133])
reward: tensor([-1.8281, -1.5078, -0.0311, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1347])
attention_mask shape: torch.Size([4, 1347])
reward: tensor([-0.8438, -0.6992, -0.5703,  0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1176])
attention_mask shape: torch.Size([4, 1176])
reward: tensor([-1.4453,  0.0388,  0.3691,  0.1924], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 209])
attention_mask shape: torch.Size([4, 209])
reward: tensor([-1.4375, -0.1846, -1.3984, -1.4922], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1440])
attention_mask shape: torch.Size([4, 1440])
reward: tensor([-0.7422,  0.7031, -1.8906,  1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 625])
attention_mask shape: torch.Size([4, 625])
reward: tensor([ 0.2158, -0.3594, -2.0000, -0.0957], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 933])
attention_mask shape: torch.Size([4, 933])
reward: tensor([-1.2422,  0.0410,  0.5117, -0.0311], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 578])
attention_mask shape: torch.Size([4, 578])
reward: tensor([-0.3242, -0.9766, -0.6875,  0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1483])
attention_mask shape: torch.Size([4, 1483])
reward: tensor([-1.0156,  0.2754,  1.7266, -0.5742], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1178])
attention_mask shape: torch.Size([4, 1178])
reward: tensor([-0.1064, -1.1094, -0.3730, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 616])
attention_mask shape: torch.Size([4, 616])
reward: tensor([ 0.1377, -2.1719,  0.2197, -0.2314], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1405])
attention_mask shape: torch.Size([4, 1405])
reward: tensor([-1.0938, -0.1514,  1.7031, -0.1309], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1002])
attention_mask shape: torch.Size([4, 1002])
reward: tensor([ 0.5117, -0.6875, -0.5391, -1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1533])
attention_mask shape: torch.Size([4, 1533])
reward: tensor([ 1.1250, -0.5156, -1.3281, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1046])
attention_mask shape: torch.Size([4, 1046])
reward: tensor([-1.4844,  0.2969, -0.5078, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1138])
attention_mask shape: torch.Size([4, 1138])
reward: tensor([-1.0078,  0.9883,  0.1602, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1103])
attention_mask shape: torch.Size([4, 1103])
reward: tensor([ 1.7656, -1.1484, -1.2812,  0.4629], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 625])
attention_mask shape: torch.Size([4, 625])
reward: tensor([-1.0312,  0.9258,  0.7383, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1562])
attention_mask shape: torch.Size([4, 1562])
reward: tensor([-0.5547, -0.2812, -1.1250, -0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1085])
attention_mask shape: torch.Size([4, 1085])
reward: tensor([-0.7852, -0.4219, -0.5781, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1242])
attention_mask shape: torch.Size([4, 1242])
reward: tensor([-1.0703, -0.4180, -1.0234,  0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1406])
attention_mask shape: torch.Size([4, 1406])
reward: tensor([ 0.3613, -0.9688,  0.9492, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.7812,  0.2373, -1.6797, -0.3516], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 978])
attention_mask shape: torch.Size([4, 978])
reward: tensor([ 0.4062, -1.9453, -0.6250, -1.5938], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1252])
attention_mask shape: torch.Size([4, 1252])
reward: tensor([-0.0200, -0.1709, -1.6953, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 418])
attention_mask shape: torch.Size([4, 418])
reward: tensor([-0.4180, -1.8125,  0.2305,  0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 401])
attention_mask shape: torch.Size([4, 401])
reward: tensor([-2.2188, -0.4883, -0.1270,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1287])
attention_mask shape: torch.Size([4, 1287])
reward: tensor([-0.3457, -2.2188, -0.0222, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1223])
attention_mask shape: torch.Size([4, 1223])
reward: tensor([-1.3047, -1.4688, -0.5742, -0.1113], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1394])
attention_mask shape: torch.Size([4, 1394])
reward: tensor([ 1.1250, -1.0469,  0.7109, -0.4492], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 679])
attention_mask shape: torch.Size([4, 679])
reward: tensor([-0.5273,  0.6641,  0.2158, -0.5898], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1318])
attention_mask shape: torch.Size([4, 1318])
reward: tensor([ 0.2158, -0.1953, -0.2559, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1237])
attention_mask shape: torch.Size([4, 1237])
reward: tensor([-0.5625, -1.2109, -0.5625, -0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 837])
attention_mask shape: torch.Size([4, 837])
reward: tensor([-0.5859,  0.8359,  0.7539, -1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1128])
attention_mask shape: torch.Size([4, 1128])
reward: tensor([ 0.7070, -0.9258, -0.7422,  0.3086], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 885])
attention_mask shape: torch.Size([4, 885])
reward: tensor([-0.6523, -1.4766, -1.3672,  0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 714])
attention_mask shape: torch.Size([4, 714])
reward: tensor([-1.7891, -1.6016, -1.5312,  0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1396])
attention_mask shape: torch.Size([4, 1396])
reward: tensor([ 0.3691, -1.8281, -1.3750, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 566])
attention_mask shape: torch.Size([4, 566])
reward: tensor([-1.6328, -0.4746, -0.3164, -0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 845])
attention_mask shape: torch.Size([4, 845])
reward: tensor([ 0.8984, -0.4141, -0.6523, -0.1426], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 658])
attention_mask shape: torch.Size([4, 658])
reward: tensor([-0.4492, -0.6680, -1.3984, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 916])
attention_mask shape: torch.Size([4, 916])
reward: tensor([ 0.0835,  0.6680,  0.4844, -0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 570])
attention_mask shape: torch.Size([4, 570])
reward: tensor([-0.8008, -1.5469, -0.9258,  0.0422], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 766])
attention_mask shape: torch.Size([4, 766])
reward: tensor([-0.4453, -0.3242,  0.5938, -0.0757], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1253])
attention_mask shape: torch.Size([4, 1253])
reward: tensor([-0.9492,  1.9531, -0.6367,  0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 735])
attention_mask shape: torch.Size([4, 735])
reward: tensor([-1.1484, -0.4180, -1.9844,  1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 761])
attention_mask shape: torch.Size([4, 761])
reward: tensor([-0.3770, -0.7031, -0.3164, -0.0178], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 633])
attention_mask shape: torch.Size([4, 633])
reward: tensor([ 0.0913, -1.0391, -0.6250,  0.2832], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1143])
attention_mask shape: torch.Size([4, 1143])
reward: tensor([ 0.1079, -0.4883, -1.6953,  1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1472])
attention_mask shape: torch.Size([4, 1472])
reward: tensor([-1.3984, -0.8164,  0.3398,  0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1867])
attention_mask shape: torch.Size([4, 1867])
reward: tensor([-0.2314,  0.9688, -0.7344, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 921])
attention_mask shape: torch.Size([4, 921])
reward: tensor([ 0.3418,  0.1299, -1.5703, -1.7500], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1194])
attention_mask shape: torch.Size([4, 1194])
reward: tensor([ 0.0156,  0.8672, -1.9922, -1.9375], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 630])
attention_mask shape: torch.Size([4, 630])
reward: tensor([-0.3555,  0.8945, -0.3340, -0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 820])
attention_mask shape: torch.Size([4, 820])
reward: tensor([ 0.0854, -0.0311,  0.7031, -0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1334])
attention_mask shape: torch.Size([4, 1334])
reward: tensor([-1.4609, -0.5273, -1.3594,  1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 849])
attention_mask shape: torch.Size([4, 849])
reward: tensor([-0.2402, -0.3105, -1.3828, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1169])
attention_mask shape: torch.Size([4, 1169])
reward: tensor([ 0.6211, -2.0000,  0.1885, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1180])
attention_mask shape: torch.Size([4, 1180])
reward: tensor([ 1.1172, -1.1797, -0.4629, -0.1641], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 432])
attention_mask shape: torch.Size([4, 432])
reward: tensor([-0.3027, -1.8281, -0.4453, -0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1183])
attention_mask shape: torch.Size([4, 1183])
reward: tensor([ 1.3125, -1.9453,  0.2354, -1.3750], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1181])
attention_mask shape: torch.Size([4, 1181])
reward: tensor([-0.2754, -1.0703, -0.1157,  0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1405])
attention_mask shape: torch.Size([4, 1405])
reward: tensor([-0.2969, -0.5156, -0.9688,  1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1126])
attention_mask shape: torch.Size([4, 1126])
reward: tensor([ 0.6055, -0.1621, -0.2793, -0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 601])
attention_mask shape: torch.Size([4, 601])
reward: tensor([-0.6445, -1.7578, -1.4141,  1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 941])
attention_mask shape: torch.Size([4, 941])
reward: tensor([-0.2490, -0.5391, -2.0781,  0.0854], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1127])
attention_mask shape: torch.Size([4, 1127])
reward: tensor([-1.9766,  0.3965,  0.6680, -0.2578], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1136])
attention_mask shape: torch.Size([4, 1136])
reward: tensor([ 0.3906, -0.9492, -1.1641, -0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1928])
attention_mask shape: torch.Size([4, 1928])
reward: tensor([ 0.3203, -0.0056, -0.3457, -0.2617], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1580])
attention_mask shape: torch.Size([4, 1580])
reward: tensor([ 0.4727, -2.1719,  1.2422, -0.0757], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1409])
attention_mask shape: torch.Size([4, 1409])
reward: tensor([-0.5391, -0.5273, -0.3867, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1296])
attention_mask shape: torch.Size([4, 1296])
reward: tensor([-0.0913, -1.1250, -0.2676,  0.2402], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 531])
attention_mask shape: torch.Size([4, 531])
reward: tensor([-0.7734, -0.5938, -0.6484,  0.2949], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 590])
attention_mask shape: torch.Size([4, 590])
reward: tensor([-1.6719, -1.4844, -1.0469, -0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1392])
attention_mask shape: torch.Size([4, 1392])
reward: tensor([-0.7383, -0.4395,  0.2354,  1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 521])
attention_mask shape: torch.Size([4, 521])
reward: tensor([ 1.1719, -1.6016, -0.3594, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1684])
attention_mask shape: torch.Size([4, 1684])
reward: tensor([-0.9258, -0.4570,  1.4141, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1323])
attention_mask shape: torch.Size([4, 1323])
reward: tensor([ 0.2285,  0.4883, -0.0820, -0.0757], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1099])
attention_mask shape: torch.Size([4, 1099])
reward: tensor([-0.5039, -0.0623,  0.0967, -0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1530])
attention_mask shape: torch.Size([4, 1530])
reward: tensor([ 0.2002,  0.6211, -1.4609,  0.0977], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1475])
attention_mask shape: torch.Size([4, 1475])
reward: tensor([ 0.0723,  0.4863,  0.7305, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 716])
attention_mask shape: torch.Size([4, 716])
reward: tensor([ 0.8359, -0.1514, -0.2354, -0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1182])
attention_mask shape: torch.Size([4, 1182])
reward: tensor([-0.2373, -0.1670,  0.5938, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1662])
attention_mask shape: torch.Size([4, 1662])
reward: tensor([-1.4375, -0.2852,  1.7500, -0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1420])
attention_mask shape: torch.Size([4, 1420])
reward: tensor([-0.4805, -0.4883,  0.1201, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-0.5234, -0.3516, -0.7188, -0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1184])
attention_mask shape: torch.Size([4, 1184])
reward: tensor([-0.5625, -0.1582, -1.1719, -0.2041], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 665])
attention_mask shape: torch.Size([4, 665])
reward: tensor([-1.6562, -1.8281,  1.1719,  0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 967])
attention_mask shape: torch.Size([4, 967])
reward: tensor([-0.7305, -0.6523,  0.3887, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 868])
attention_mask shape: torch.Size([4, 868])
reward: tensor([-0.3457, -0.6406, -0.2891, -0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1201])
attention_mask shape: torch.Size([4, 1201])
reward: tensor([-0.2559,  0.4453, -1.1016, -0.1309], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1325])
attention_mask shape: torch.Size([4, 1325])
reward: tensor([-1.2891,  0.0388, -0.9414,  0.0211], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1101])
attention_mask shape: torch.Size([4, 1101])
reward: tensor([-1.1641, -0.6914, -0.1089, -0.0645], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([-0.9141, -2.1406,  1.3359,  0.4688], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-0.5352, -1.2109, -2.2031,  0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 315])
attention_mask shape: torch.Size([4, 315])
reward: tensor([ 0.3379, -2.0781, -0.7734, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 678])
attention_mask shape: torch.Size([4, 678])
reward: tensor([-0.9414, -1.1172, -0.7070, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 465])
attention_mask shape: torch.Size([4, 465])
reward: tensor([-0.6094,  0.6523, -1.0156, -0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1002])
attention_mask shape: torch.Size([4, 1002])
reward: tensor([-0.5352, -0.5352, -0.2070, -0.2070], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1370])
attention_mask shape: torch.Size([4, 1370])
reward: tensor([-0.1514, -0.8125,  0.2891,  0.2363], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([ 0.9570,  0.4238, -1.1797,  0.1211], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1244])
attention_mask shape: torch.Size([4, 1244])
reward: tensor([-1.9531, -1.5547,  1.2500,  0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 575])
attention_mask shape: torch.Size([4, 575])
reward: tensor([-1.7500, -0.8789, -2.1719, -1.1797], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1138])
attention_mask shape: torch.Size([4, 1138])
reward: tensor([-1.0312, -0.2910, -1.1875, -1.7969], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-1.0938, -2.2031,  0.1689,  0.0122], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 803])
attention_mask shape: torch.Size([4, 803])
reward: tensor([-0.1934, -1.6328, -1.5547, -1.2891], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1369])
attention_mask shape: torch.Size([4, 1369])
reward: tensor([-0.1001,  1.0078, -0.8359,  0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1175])
attention_mask shape: torch.Size([4, 1175])
reward: tensor([ 1.0078, -0.9336, -1.0938, -1.8750], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 611])
attention_mask shape: torch.Size([4, 611])
reward: tensor([-0.6055, -0.7500,  1.5625, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1134])
attention_mask shape: torch.Size([4, 1134])
reward: tensor([-0.6875,  1.9453,  0.7539, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1714])
attention_mask shape: torch.Size([4, 1714])
reward: tensor([-0.6367, -0.0289, -0.3203, -0.2246], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 695])
attention_mask shape: torch.Size([4, 695])
reward: tensor([-0.5273, -0.9141, -0.1846, -1.2031], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 745])
attention_mask shape: torch.Size([4, 745])
reward: tensor([ 0.7188, -0.5039, -0.9883,  1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1042])
attention_mask shape: torch.Size([4, 1042])
reward: tensor([-0.6562,  0.2207, -1.2812, -0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1593])
attention_mask shape: torch.Size([4, 1593])
reward: tensor([-0.1826, -0.7188, -0.3965,  1.2344], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 559])
attention_mask shape: torch.Size([4, 559])
reward: tensor([-1.0547, -0.0820, -1.0391, -0.4453], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 608])
attention_mask shape: torch.Size([4, 608])
reward: tensor([-0.4395, -0.3828, -1.0547, -0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 861])
attention_mask shape: torch.Size([4, 861])
reward: tensor([ 0.6836, -0.9883, -0.2373, -0.1426], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1123])
attention_mask shape: torch.Size([4, 1123])
reward: tensor([ 0.1602, -1.8281, -0.8906,  0.1934], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 479])
attention_mask shape: torch.Size([4, 479])
reward: tensor([-1.2734, -0.9688, -0.3066,  0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1356])
attention_mask shape: torch.Size([4, 1356])
reward: tensor([-1.5312,  0.6406, -0.3555,  0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1158])
attention_mask shape: torch.Size([4, 1158])
reward: tensor([ 0.1709, -1.4062, -0.0078, -0.3691], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1202])
attention_mask shape: torch.Size([4, 1202])
reward: tensor([ 0.4004, -2.1719, -1.5234, -1.7188], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1219])
attention_mask shape: torch.Size([4, 1219])
reward: tensor([-0.8203, -0.9258, -0.5586,  0.0679], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1254])
attention_mask shape: torch.Size([4, 1254])
reward: tensor([ 0.2832, -1.7422,  1.8438, -1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1259])
attention_mask shape: torch.Size([4, 1259])
reward: tensor([ 2.1250, -0.9492,  0.2754,  0.0820], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 859])
attention_mask shape: torch.Size([4, 859])
reward: tensor([ 0.2832, -0.3906, -0.3105,  1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1347])
attention_mask shape: torch.Size([4, 1347])
reward: tensor([ 0.4863, -0.5625, -1.0234,  0.1445], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 629])
attention_mask shape: torch.Size([4, 629])
reward: tensor([-0.3555, -1.3047,  0.1484, -0.6094], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([-0.6484, -0.9141,  1.0000, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1136])
attention_mask shape: torch.Size([4, 1136])
reward: tensor([ 0.9453, -0.4844, -1.6562, -0.0111], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 856])
attention_mask shape: torch.Size([4, 856])
reward: tensor([ 0.7188, -2.1719,  0.7305, -0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 787])
attention_mask shape: torch.Size([4, 787])
reward: tensor([ 0.0334, -0.5898, -0.0244, -0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
[2024-10-22 03:36:54,792] [INFO] [launch.py:351:main] Process 609940 exits successfully.
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2500, -0.9609, -1.0391, -1.6250], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([ 0.3242, -0.3242, -2.1406, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1438])
attention_mask shape: torch.Size([4, 1438])
reward: tensor([ 0.1514, -0.1309, -1.7578,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1640])
attention_mask shape: torch.Size([4, 1640])
reward: tensor([-0.4941, -1.1016,  1.9531, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 756])
attention_mask shape: torch.Size([4, 756])
reward: tensor([-1.0312, -0.4082, -1.7031,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 647])
attention_mask shape: torch.Size([4, 647])
reward: tensor([-0.5195, -0.2490,  0.2305,  0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1473])
attention_mask shape: torch.Size([4, 1473])
reward: tensor([-0.3379, -1.2188, -0.5391, -1.1719], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1086])
attention_mask shape: torch.Size([4, 1086])
reward: tensor([-0.3164, -2.0938, -1.2969, -0.8125], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([-0.5117, -0.8516, -0.6445,  0.6328], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 738])
attention_mask shape: torch.Size([4, 738])
reward: tensor([ 0.4004, -1.1641, -0.2617,  0.2852], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1284])
attention_mask shape: torch.Size([4, 1284])
reward: tensor([-0.2285,  0.2812, -1.6797, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 579])
attention_mask shape: torch.Size([4, 579])
reward: tensor([-1.2734, -0.7188,  0.1885, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 765])
attention_mask shape: torch.Size([4, 765])
reward: tensor([0.3906, 0.2637, 0.4062, 0.0233], device='cuda:0', dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1731])
attention_mask shape: torch.Size([4, 1731])
reward: tensor([ 1.5938,  1.2188,  1.1328, -0.0378], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1059])
attention_mask shape: torch.Size([4, 1059])
reward: tensor([ 0.1211, -0.0732,  1.2812,  0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-22 03:39:18,941] [INFO] [launch.py:351:main] Process 609939 exits successfully.
[2024-10-22 03:39:21,945] [INFO] [launch.py:351:main] Process 609941 exits successfully.
[2024-10-22 03:40:26,009] [INFO] [launch.py:351:main] Process 609942 exits successfully.
+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-4th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-4th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_4 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-22 03:40:30,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 03:40:32,568] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-22 03:40:32,568] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-4th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_4 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-22 03:40:33,948] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 03:40:35,900] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-22 03:40:35,901] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-22 03:40:35,901] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-22 03:40:35,901] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-22 03:40:35,901] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-22 03:40:35,901] [INFO] [launch.py:256:main] process 611188 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-4th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_4', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 03:40:35,901] [INFO] [launch.py:256:main] process 611189 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-4th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_4', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 03:40:35,902] [INFO] [launch.py:256:main] process 611190 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-4th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_4', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 03:40:35,902] [INFO] [launch.py:256:main] process 611191 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-4th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_4', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 03:40:37,944] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 03:40:37,951] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 03:40:37,956] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 03:40:37,963] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-22 03:40:40,439] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 03:40:40,439] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-22 03:40:41,112] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 03:40:41,112] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 03:40:41,113] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.52it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.67it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.19it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.56it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.69it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.54it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.17it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.69it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.80it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.70it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.97it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.86it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.21it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.39it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.31it/s]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2024-10-22 03:40:43,599] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:40:43,616] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.66it/s][2024-10-22 03:40:43,872] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.66it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.75it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.07it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.93it/s]
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-22 03:40:44,960] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-22 03:40:44,960] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-22 03:40:44,960] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:40:45,681] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-22 03:40:45,682] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-22 03:40:45,683] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:40:45,683] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:40:45,684] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:40:45,810] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-22 03:40:45,810] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-22 03:40:45,811] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.92 GB, percent = 2.5%
[2024-10-22 03:40:45,941] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-22 03:40:45,942] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-22 03:40:45,942] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.92 GB, percent = 2.5%
[2024-10-22 03:40:45,943] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-22 03:40:45,943] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-22 03:40:45,943] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-22 03:40:45,943] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-22 03:40:45,943] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff605205cf0>
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-22 03:40:45,944] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-22 03:40:45,945] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-22 03:40:45,945] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-22 03:40:45,945] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-22 03:40:45,946] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-22 03:40:45,946] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 03:40:50,794] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-22 03:40:50,796] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
[2024-10-22 03:40:50,914] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-22 03:40:50,915] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-22 03:40:50,915] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.94 GB, percent = 2.5%
[2024-10-22 03:40:51,049] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-22 03:40:51,050] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-22 03:40:51,050] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.95 GB, percent = 2.5%
[2024-10-22 03:40:51,051] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-22 03:40:51,051] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-22 03:40:51,051] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-22 03:40:51,051] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-22 03:40:51,051] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff6045aa830>
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-22 03:40:51,052] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-22 03:40:51,053] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-22 03:40:51,053] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 655/100000 [00:00<00:15, 6546.06it/s]Preprocessing data:   2%|█▌                                                                                          | 1677/100000 [00:00<00:11, 8703.96it/s]Preprocessing data:   3%|██▍                                                                                         | 2715/100000 [00:00<00:10, 9465.50it/s]Preprocessing data:   4%|███▍                                                                                        | 3753/100000 [00:00<00:09, 9825.35it/s]Preprocessing data:   5%|████▎                                                                                      | 4791/100000 [00:00<00:09, 10021.52it/s]Preprocessing data:   6%|█████▎                                                                                     | 5826/100000 [00:00<00:09, 10130.69it/s]Preprocessing data:   7%|██████▏                                                                                    | 6865/100000 [00:00<00:09, 10213.90it/s]Preprocessing data:   8%|███████▏                                                                                   | 7907/100000 [00:00<00:08, 10276.61it/s]Preprocessing data:   9%|████████▏                                                                                  | 8935/100000 [00:00<00:08, 10185.23it/s]Preprocessing data:  10%|█████████                                                                                  | 9975/100000 [00:01<00:08, 10248.70it/s]Preprocessing data:  11%|█████████▉                                                                                | 11055/100000 [00:01<00:08, 10414.78it/s]Preprocessing data:  12%|██████████▉                                                                               | 12138/100000 [00:01<00:08, 10537.70it/s]Preprocessing data:  13%|███████████▉                                                                              | 13220/100000 [00:01<00:08, 10620.65it/s]Preprocessing data:  14%|████████████▊                                                                             | 14305/100000 [00:01<00:08, 10689.61it/s]Preprocessing data:  15%|█████████████▊                                                                            | 15390/100000 [00:01<00:07, 10735.06it/s]Preprocessing data:  16%|██████████████▊                                                                           | 16464/100000 [00:01<00:07, 10710.48it/s]Preprocessing data:  18%|███████████████▊                                                                          | 17541/100000 [00:01<00:07, 10727.93it/s]Preprocessing data:  19%|████████████████▊                                                                         | 18625/100000 [00:01<00:07, 10759.20it/s]Preprocessing data:  20%|█████████████████▋                                                                        | 19711/100000 [00:01<00:07, 10789.34it/s]Preprocessing data:  21%|██████████████████▋                                                                       | 20812/100000 [00:02<00:07, 10853.52it/s]Preprocessing data:  22%|███████████████████▋                                                                      | 21910/100000 [00:02<00:07, 10888.76it/s]Preprocessing data:  23%|████████████████████▋                                                                     | 22999/100000 [00:02<00:07, 10847.09it/s]Preprocessing data:  24%|█████████████████████▋                                                                    | 24084/100000 [00:02<00:07, 10811.30it/s]Preprocessing data:  25%|██████████████████████▋                                                                   | 25166/100000 [00:02<00:06, 10779.16it/s]Preprocessing data:  26%|███████████████████████▌                                                                  | 26244/100000 [00:02<00:06, 10742.45it/s]Preprocessing data:  27%|████████████████████████▌                                                                 | 27319/100000 [00:02<00:06, 10710.15it/s]Preprocessing data:  28%|█████████████████████████▌                                                                | 28391/100000 [00:02<00:06, 10696.15it/s]Preprocessing data:  29%|██████████████████████████▌                                                               | 29461/100000 [00:02<00:06, 10656.14it/s]Preprocessing data:  31%|███████████████████████████▍                                                              | 30527/100000 [00:02<00:06, 10652.99it/s]Preprocessing data:  32%|████████████████████████████▍                                                             | 31593/100000 [00:03<00:06, 10646.61it/s]Preprocessing data:  33%|█████████████████████████████▍                                                            | 32658/100000 [00:03<00:06, 10639.74it/s]Preprocessing data:  34%|██████████████████████████████▎                                                           | 33722/100000 [00:03<00:06, 10637.47it/s]Preprocessing data:  35%|███████████████████████████████▎                                                          | 34786/100000 [00:03<00:06, 10626.61it/s]Preprocessing data:  36%|████████████████████████████████▎                                                         | 35849/100000 [00:03<00:06, 10607.86it/s]Preprocessing data:  37%|█████████████████████████████████▏                                                        | 36910/100000 [00:03<00:05, 10559.65it/s]Preprocessing data:  38%|██████████████████████████████████▏                                                       | 37967/100000 [00:03<00:05, 10549.31it/s]Preprocessing data:  39%|███████████████████████████████████                                                       | 39022/100000 [00:03<00:05, 10481.10it/s]Preprocessing data:  40%|████████████████████████████████████                                                      | 40080/100000 [00:03<00:05, 10509.55it/s]Preprocessing data:  41%|█████████████████████████████████████                                                     | 41139/100000 [00:03<00:05, 10532.19it/s]Preprocessing data:  42%|█████████████████████████████████████▉                                                    | 42193/100000 [00:04<00:05, 10519.84it/s]Preprocessing data:  43%|██████████████████████████████████████▉                                                   | 43256/100000 [00:04<00:05, 10551.69it/s]Preprocessing data:  44%|███████████████████████████████████████▉                                                  | 44334/100000 [00:04<00:05, 10619.69it/s]Preprocessing data:  45%|████████████████████████████████████████▊                                                 | 45416/100000 [00:04<00:05, 10679.16it/s]Preprocessing data:  46%|█████████████████████████████████████████▊                                                | 46499/100000 [00:04<00:04, 10723.86it/s]Preprocessing data:  48%|██████████████████████████████████████████▊                                               | 47580/100000 [00:04<00:04, 10748.20it/s]Preprocessing data:  49%|███████████████████████████████████████████▊                                              | 48663/100000 [00:04<00:04, 10770.10it/s]Preprocessing data:  50%|████████████████████████████████████████████▊                                             | 49746/100000 [00:04<00:04, 10787.39it/s]Preprocessing data:  51%|█████████████████████████████████████████████▋                                            | 50825/100000 [00:04<00:04, 10734.51it/s]Preprocessing data:  52%|██████████████████████████████████████████████▋                                           | 51907/100000 [00:04<00:04, 10759.32it/s]Preprocessing data:  53%|███████████████████████████████████████████████▋                                          | 52990/100000 [00:05<00:04, 10777.46it/s]Preprocessing data:  54%|████████████████████████████████████████████████▋                                         | 54071/100000 [00:05<00:04, 10786.69it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▋                                        | 55153/100000 [00:05<00:04, 10794.49it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▌                                       | 56235/100000 [00:05<00:04, 10799.24it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▌                                      | 57317/100000 [00:05<00:03, 10804.32it/s]Preprocessing data:  58%|████████████████████████████████████████████████████▌                                     | 58398/100000 [00:05<00:03, 10804.26it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████▌                                    | 59479/100000 [00:05<00:03, 10800.37it/s]Preprocessing data:  61%|██████████████████████████████████████████████████████▌                                   | 60560/100000 [00:05<00:03, 10784.69it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▍                                  | 61639/100000 [00:05<00:03, 10758.37it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▍                                 | 62728/100000 [00:05<00:03, 10797.15it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▍                                | 63808/100000 [00:06<00:03, 10783.11it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▍                               | 64887/100000 [00:06<00:03, 10758.84it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▎                              | 65963/100000 [00:06<00:03, 10746.49it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████▎                             | 67038/100000 [00:06<00:03, 10735.33it/s]Preprocessing data:  68%|█████████████████████████████████████████████████████████████▎                            | 68112/100000 [00:06<00:02, 10713.15it/s]Preprocessing data:  69%|██████████████████████████████████████████████████████████████▎                           | 69184/100000 [00:06<00:02, 10702.49it/s]Preprocessing data:  70%|███████████████████████████████████████████████████████████████▏                          | 70255/100000 [00:06<00:02, 10704.47it/s]Preprocessing data:  71%|████████████████████████████████████████████████████████████████▏                         | 71326/100000 [00:06<00:02, 10700.39it/s]Preprocessing data:  72%|█████████████████████████████████████████████████████████████████▏                        | 72397/100000 [00:06<00:02, 10690.44it/s]Preprocessing data:  73%|██████████████████████████████████████████████████████████████████▊                        | 73467/100000 [00:06<00:02, 9791.79it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▊                       | 74461/100000 [00:07<00:02, 9193.68it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▌                      | 75396/100000 [00:07<00:02, 8824.80it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████▍                     | 76290/100000 [00:07<00:02, 8566.44it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▏                    | 77155/100000 [00:07<00:02, 8400.18it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▉                    | 78000/100000 [00:07<00:02, 8282.08it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▋                   | 78831/100000 [00:07<00:02, 8172.03it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▍                  | 79650/100000 [00:07<00:02, 8107.74it/s]Preprocessing data:  80%|█████████████████████████████████████████████████████████████████████████▏                 | 80462/100000 [00:07<00:02, 7840.45it/s]Preprocessing data:  81%|██████████████████████████████████████████████████████████████████████████                 | 81373/100000 [00:07<00:02, 8198.32it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▉                | 82291/100000 [00:08<00:02, 8480.44it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▋               | 83144/100000 [00:08<00:01, 8493.09it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▍              | 83996/100000 [00:08<00:01, 8182.85it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▍             | 85053/100000 [00:08<00:01, 8869.73it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▎            | 86028/100000 [00:08<00:01, 9125.93it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████            | 86946/100000 [00:08<00:01, 8616.91it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████           | 87948/100000 [00:08<00:01, 9012.96it/s]Preprocessing data:  89%|████████████████████████████████████████████████████████████████████████████████▉          | 88945/100000 [00:08<00:01, 9286.94it/s]Preprocessing data:  90%|█████████████████████████████████████████████████████████████████████████████████▊         | 89942/100000 [00:08<00:01, 9485.81it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▋        | 90896/100000 [00:09<00:01, 8830.34it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▌       | 91792/100000 [00:09<00:00, 8308.16it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▎      | 92636/100000 [00:09<00:00, 7962.32it/s]Preprocessing data:  93%|█████████████████████████████████████████████████████████████████████████████████████      | 93442/100000 [00:09<00:00, 7825.64it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▊     | 94294/100000 [00:09<00:00, 8015.78it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▋    | 95311/100000 [00:09<00:00, 8622.84it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▌   | 96267/100000 [00:09<00:00, 8890.59it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▍  | 97163/100000 [00:09<00:00, 8835.75it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▏ | 98052/100000 [00:09<00:00, 8836.18it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████▏| 99065/100000 [00:09<00:00, 9214.89it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9946.69it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-0.1289, -0.6094, -0.4082, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1507])
attention_mask shape: torch.Size([4, 1507])
reward: tensor([ 1.8828, -0.8789, -0.0133,  0.0557], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1068])
attention_mask shape: torch.Size([4, 1068])
reward: tensor([-1.8906, -0.8359, -1.0781, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 441])
attention_mask shape: torch.Size([4, 441])
reward: tensor([-1.1016, -0.3730, -0.6250,  0.1289], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1445])
attention_mask shape: torch.Size([4, 1445])
reward: tensor([ 1.1094,  0.9609, -0.2812,  0.0156], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 863])
attention_mask shape: torch.Size([4, 863])
reward: tensor([ 1.0156, -1.2891, -1.2109,  0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1379])
attention_mask shape: torch.Size([4, 1379])
reward: tensor([-0.4629, -0.0222,  0.6680, -1.4453], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 855])
attention_mask shape: torch.Size([4, 855])
reward: tensor([ 0.5547, -0.3242, -0.5898,  0.9570], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 855])
attention_mask shape: torch.Size([4, 855])
reward: tensor([-1.2500, -0.9062, -0.4492, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1053])
attention_mask shape: torch.Size([4, 1053])
reward: tensor([ 0.2334,  1.4375, -0.1357, -0.8359], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 714])
attention_mask shape: torch.Size([4, 714])
reward: tensor([ 0.1611,  0.3750, -1.3594,  0.0820], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 863])
attention_mask shape: torch.Size([4, 863])
reward: tensor([ 0.9688, -0.6836, -0.3379,  0.2559], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1030])
attention_mask shape: torch.Size([4, 1030])
reward: tensor([-1.5938, -0.9688, -0.6094, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1287])
attention_mask shape: torch.Size([4, 1287])
reward: tensor([-0.4395, -0.1709,  1.2656, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 629])
attention_mask shape: torch.Size([4, 629])
reward: tensor([-0.1885, -0.6094,  1.2500, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1280])
attention_mask shape: torch.Size([4, 1280])
reward: tensor([-0.0311, -0.3867, -0.2314, -1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1460])
attention_mask shape: torch.Size([4, 1460])
reward: tensor([-1.5703, -1.9141, -0.5391,  0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1626])
attention_mask shape: torch.Size([4, 1626])
reward: tensor([-0.4668,  0.1982, -0.2109,  0.3926], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 956])
attention_mask shape: torch.Size([4, 956])
reward: tensor([-0.4043, -0.4980, -0.5117,  1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 798])
attention_mask shape: torch.Size([4, 798])
reward: tensor([ 1.6875, -0.7773, -1.7656,  0.0378], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 814])
attention_mask shape: torch.Size([4, 814])
reward: tensor([-0.1396, -1.1719, -0.2617, -0.1709], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1158])
attention_mask shape: torch.Size([4, 1158])
reward: tensor([ 0.0145,  0.8789, -1.2891,  0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 733])
attention_mask shape: torch.Size([4, 733])
reward: tensor([ 0.0557,  0.1670, -0.2637, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1053])
attention_mask shape: torch.Size([4, 1053])
reward: tensor([ 0.7344, -1.1875,  0.5664,  1.8672], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1313])
attention_mask shape: torch.Size([4, 1313])
reward: tensor([ 0.9805, -0.0532, -1.3438, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1520])
attention_mask shape: torch.Size([4, 1520])
reward: tensor([-0.1021, -0.6172,  0.7227, -1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1589])
attention_mask shape: torch.Size([4, 1589])
reward: tensor([-0.6914,  0.3418,  0.7227,  0.0300], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 253])
attention_mask shape: torch.Size([4, 253])
reward: tensor([-0.5117, -0.6133, -1.5938, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1147])
attention_mask shape: torch.Size([4, 1147])
reward: tensor([ 0.7070, -0.9609, -0.1338, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1323])
attention_mask shape: torch.Size([4, 1323])
reward: tensor([-0.1201,  0.1348, -0.1953, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1442])
attention_mask shape: torch.Size([4, 1442])
reward: tensor([-0.6250,  0.5391, -0.1484, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 674])
attention_mask shape: torch.Size([4, 674])
reward: tensor([-1.7891,  0.0757,  0.0923, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1277])
attention_mask shape: torch.Size([4, 1277])
reward: tensor([-0.7695, -0.3418,  0.9766, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1143])
attention_mask shape: torch.Size([4, 1143])
reward: tensor([ 0.1533,  0.8008, -1.3516, -0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 816])
attention_mask shape: torch.Size([4, 816])
reward: tensor([-0.3203, -0.9414, -0.6719, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1361])
attention_mask shape: torch.Size([4, 1361])
reward: tensor([ 1.0391, -0.2471,  0.8477, -0.1113], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 964])
attention_mask shape: torch.Size([4, 964])
reward: tensor([ 1.9453, -1.5703, -0.5820, -1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1335])
attention_mask shape: torch.Size([4, 1335])
reward: tensor([ 0.4531, -1.8125, -0.5703, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 638])
attention_mask shape: torch.Size([4, 638])
reward: tensor([-0.5898, -0.0422,  0.3262, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1364])
attention_mask shape: torch.Size([4, 1364])
reward: tensor([ 0.2832, -0.3340, -0.3066,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1195])
attention_mask shape: torch.Size([4, 1195])
reward: tensor([0.1514, 0.1621, 0.6094, 0.2793], device='cuda:0', dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1102])
attention_mask shape: torch.Size([4, 1102])
reward: tensor([ 0.6641,  0.7031, -0.3164, -0.0289], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 310])
attention_mask shape: torch.Size([4, 310])
reward: tensor([-0.1021, -1.8828,  0.6562, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 580])
attention_mask shape: torch.Size([4, 580])
reward: tensor([-1.5625, -0.3105, -0.6211, -0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1579])
attention_mask shape: torch.Size([4, 1579])
reward: tensor([-0.9961,  0.9648, -0.0156,  1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 819])
attention_mask shape: torch.Size([4, 819])
reward: tensor([ 0.2246, -0.8984, -0.2266,  0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 435])
attention_mask shape: torch.Size([4, 435])
reward: tensor([-0.4258, -0.0977,  0.0189, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1832])
attention_mask shape: torch.Size([4, 1832])
reward: tensor([-0.3281, -0.3457, -0.2334,  0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1271])
attention_mask shape: torch.Size([4, 1271])
reward: tensor([-0.5898,  0.0133,  0.0811, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 689])
attention_mask shape: torch.Size([4, 689])
reward: tensor([-1.1016, -0.8008, -0.0266,  0.1641], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1288])
attention_mask shape: torch.Size([4, 1288])
reward: tensor([-1.5078, -0.5703, -0.4941, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([-0.4043,  1.5859,  0.1099,  1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1368])
attention_mask shape: torch.Size([4, 1368])
reward: tensor([ 0.3418, -0.3770,  0.6797, -1.8672], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1180])
attention_mask shape: torch.Size([4, 1180])
reward: tensor([-2.1094, -1.1875,  0.2559,  0.5000], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1506])
attention_mask shape: torch.Size([4, 1506])
reward: tensor([-0.1826,  0.2090,  0.8516, -0.1270], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 620])
attention_mask shape: torch.Size([4, 620])
reward: tensor([-1.0234, -1.0547, -1.5312, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1211])
attention_mask shape: torch.Size([4, 1211])
reward: tensor([-1.3672, -0.2793,  0.4180, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 577])
attention_mask shape: torch.Size([4, 577])
reward: tensor([-1.3359, -0.2891, -0.2314, -0.3418], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 677])
attention_mask shape: torch.Size([4, 677])
reward: tensor([-0.2969, -0.6250, -1.3516, -1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1312])
attention_mask shape: torch.Size([4, 1312])
reward: tensor([-1.6953, -1.2188, -0.9883, -0.2676], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 966])
attention_mask shape: torch.Size([4, 966])
reward: tensor([-0.4883, -1.3438, -1.0391, -1.7891], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1266])
attention_mask shape: torch.Size([4, 1266])
reward: tensor([-1.2266, -0.1245,  0.8906, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1139])
attention_mask shape: torch.Size([4, 1139])
reward: tensor([-0.2969, -0.6797, -1.1719,  0.3184], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 471])
attention_mask shape: torch.Size([4, 471])
reward: tensor([-0.4844,  0.1387,  0.1123, -1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1018])
attention_mask shape: torch.Size([4, 1018])
reward: tensor([ 0.8438, -0.0111,  0.2471, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 656])
attention_mask shape: torch.Size([4, 656])
reward: tensor([-0.9492,  1.2109,  0.2754, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 967])
attention_mask shape: torch.Size([4, 967])
reward: tensor([-0.5391, -1.3359, -0.7539, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1328])
attention_mask shape: torch.Size([4, 1328])
reward: tensor([ 0.1641, -1.0156, -0.2676,  1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 411])
attention_mask shape: torch.Size([4, 411])
reward: tensor([-0.0688, -0.6484, -0.3965, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1158])
attention_mask shape: torch.Size([4, 1158])
reward: tensor([-1.0547, -1.1094, -0.4941,  0.3438], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1467])
attention_mask shape: torch.Size([4, 1467])
reward: tensor([-1.0391, -0.2197, -0.3867,  0.0742], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1038])
attention_mask shape: torch.Size([4, 1038])
reward: tensor([ 0.6133, -1.1719, -0.5859, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 955])
attention_mask shape: torch.Size([4, 955])
reward: tensor([-0.8633,  0.4805,  0.0011,  1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1150])
attention_mask shape: torch.Size([4, 1150])
reward: tensor([-0.1089, -0.3340, -0.4707,  1.3594], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 353])
attention_mask shape: torch.Size([4, 353])
reward: tensor([-1.4219, -0.2793, -0.2910, -0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 820])
attention_mask shape: torch.Size([4, 820])
reward: tensor([ 1.2656,  0.1099, -1.1406, -0.6719], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1411])
attention_mask shape: torch.Size([4, 1411])
reward: tensor([ 1.8828, -0.0400, -1.5000, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1205])
attention_mask shape: torch.Size([4, 1205])
reward: tensor([ 0.6211, -0.3281, -0.4219, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 880])
attention_mask shape: torch.Size([4, 880])
reward: tensor([-0.7930,  0.3594, -0.0864,  0.6719], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 611])
attention_mask shape: torch.Size([4, 611])
reward: tensor([-0.3965, -0.9141,  0.4531, -1.5703], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1125])
attention_mask shape: torch.Size([4, 1125])
reward: tensor([-0.3828, -0.0688,  1.0156, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1030])
attention_mask shape: torch.Size([4, 1030])
reward: tensor([ 1.9922,  0.3730,  0.2637, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1144])
attention_mask shape: torch.Size([4, 1144])
reward: tensor([-0.7031,  0.2539, -0.4453, -0.0713], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 650])
attention_mask shape: torch.Size([4, 650])
reward: tensor([-0.4043, -0.1309, -0.3281,  0.0698], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1392])
attention_mask shape: torch.Size([4, 1392])
reward: tensor([-0.2773, -0.3730, -0.2754,  0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1163])
attention_mask shape: torch.Size([4, 1163])
reward: tensor([-0.3770, -0.7383,  0.1699,  0.0522], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 892])
attention_mask shape: torch.Size([4, 892])
reward: tensor([-0.3906,  0.0776, -0.0801, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1418])
attention_mask shape: torch.Size([4, 1418])
reward: tensor([-0.9883,  0.4395, -0.1982, -0.2041], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1333])
attention_mask shape: torch.Size([4, 1333])
reward: tensor([-0.1582, -1.3359,  0.7930, -0.0067], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 559])
attention_mask shape: torch.Size([4, 559])
reward: tensor([ 0.3711, -0.5742, -0.2090, -1.6094], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 799])
attention_mask shape: torch.Size([4, 799])
reward: tensor([-0.9141,  0.6133,  0.0410,  1.2891], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1530])
attention_mask shape: torch.Size([4, 1530])
reward: tensor([ 1.3672,  0.5078, -0.8398, -0.1709], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 489])
attention_mask shape: torch.Size([4, 489])
reward: tensor([ 0.2930, -0.8359,  0.3555,  0.3828], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1469])
attention_mask shape: torch.Size([4, 1469])
reward: tensor([ 0.2441,  1.4375, -0.6406, -0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1703])
attention_mask shape: torch.Size([4, 1703])
reward: tensor([ 0.9375, -0.2002,  0.0078, -0.6250], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 819])
attention_mask shape: torch.Size([4, 819])
reward: tensor([-1.7266,  0.4453, -1.2969, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1239])
attention_mask shape: torch.Size([4, 1239])
reward: tensor([-0.6094,  0.2373,  0.0757,  0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 773])
attention_mask shape: torch.Size([4, 773])
reward: tensor([-1.7188, -0.6328,  0.6211, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 803])
attention_mask shape: torch.Size([4, 803])
reward: tensor([-0.0133, -0.5195,  0.0713, -0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1087])
attention_mask shape: torch.Size([4, 1087])
reward: tensor([-0.6758, -0.0334,  0.2637, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1017])
attention_mask shape: torch.Size([4, 1017])
reward: tensor([-1.6328, -0.3066,  0.1621, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 453])
attention_mask shape: torch.Size([4, 453])
reward: tensor([-0.2070, -0.2812, -1.2031, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-0.2197, -0.3281, -0.1982,  1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1308])
attention_mask shape: torch.Size([4, 1308])
reward: tensor([ 0.7188, -0.5078, -0.3379,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 775])
attention_mask shape: torch.Size([4, 775])
reward: tensor([ 0.6836, -0.6992,  0.3965, -0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1459])
attention_mask shape: torch.Size([4, 1459])
reward: tensor([-0.6445,  0.1035, -0.0557,  1.5859], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1494])
attention_mask shape: torch.Size([4, 1494])
reward: tensor([-1.3438, -1.7891,  0.1133, -0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 520])
attention_mask shape: torch.Size([4, 520])
reward: tensor([-0.1709,  0.3066, -0.0222, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 653])
attention_mask shape: torch.Size([4, 653])
reward: tensor([-0.9062, -0.7031,  0.1396, -0.0289], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6914, -0.7930,  0.0801,  1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1373])
attention_mask shape: torch.Size([4, 1373])
reward: tensor([-0.8008,  0.0864,  0.7383, -0.0356], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 245])
attention_mask shape: torch.Size([4, 245])
reward: tensor([ 0.6133, -1.1172, -1.0312, -1.5469], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1502])
attention_mask shape: torch.Size([4, 1502])
reward: tensor([-0.2070,  0.9141, -1.1172,  0.3184], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 704])
attention_mask shape: torch.Size([4, 704])
reward: tensor([-1.0781, -0.3691, -2.0781, -0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1025])
attention_mask shape: torch.Size([4, 1025])
reward: tensor([-0.7734, -0.2041, -0.1729,  0.1123], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 461])
attention_mask shape: torch.Size([4, 461])
reward: tensor([-1.0156,  0.0366, -0.2109,  0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1231])
attention_mask shape: torch.Size([4, 1231])
reward: tensor([-1.2031,  0.1387,  1.2266, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1019])
attention_mask shape: torch.Size([4, 1019])
reward: tensor([-1.0938, -0.5742,  0.8633, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 453])
attention_mask shape: torch.Size([4, 453])
reward: tensor([ 0.0444, -1.3047, -0.2754, -0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1402])
attention_mask shape: torch.Size([4, 1402])
reward: tensor([-1.1250, -0.1484, -0.0133, -1.7344], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 965])
attention_mask shape: torch.Size([4, 965])
reward: tensor([ 0.6836, -1.0156, -0.6992,  0.0078], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-0.6250, -0.2734, -0.6719, -0.2734], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1104])
attention_mask shape: torch.Size([4, 1104])
reward: tensor([-1.4375,  0.5391, -0.2852, -1.3281], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 995])
attention_mask shape: torch.Size([4, 995])
reward: tensor([ 1.4219,  0.5273,  0.0289, -1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 822])
attention_mask shape: torch.Size([4, 822])
reward: tensor([ 0.7656, -0.4082, -1.4062, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 629])
attention_mask shape: torch.Size([4, 629])
reward: tensor([-0.6367,  0.1836, -1.2422,  0.2334], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1543])
attention_mask shape: torch.Size([4, 1543])
reward: tensor([-0.2969, -0.2871, -0.4629,  0.2344], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1178])
attention_mask shape: torch.Size([4, 1178])
reward: tensor([ 0.1426,  0.0854,  0.7109, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1264])
attention_mask shape: torch.Size([4, 1264])
reward: tensor([-0.7500,  0.0679, -0.4258,  1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1179])
attention_mask shape: torch.Size([4, 1179])
reward: tensor([ 0.3047, -1.0938,  0.0835, -1.6641], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1631])
attention_mask shape: torch.Size([4, 1631])
reward: tensor([-0.7344,  0.1953,  0.3203, -0.2754], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 898])
attention_mask shape: torch.Size([4, 898])
reward: tensor([ 1.1172, -1.2500, -0.0977, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1253])
attention_mask shape: torch.Size([4, 1253])
reward: tensor([-0.0601, -0.7969, -1.2812, -1.9531], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 446])
attention_mask shape: torch.Size([4, 446])
reward: tensor([ 0.0366, -0.9688,  0.4648, -0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 510])
attention_mask shape: torch.Size([4, 510])
reward: tensor([ 0.2129,  0.5742, -0.1797,  0.4785], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1400])
attention_mask shape: torch.Size([4, 1400])
reward: tensor([-0.3555, -2.2031, -0.1270,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1131])
attention_mask shape: torch.Size([4, 1131])
reward: tensor([-1.0234, -0.7812, -0.3066, -0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1504])
attention_mask shape: torch.Size([4, 1504])
reward: tensor([ 1.2422, -0.2715, -0.2793, -1.6875], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 640])
attention_mask shape: torch.Size([4, 640])
reward: tensor([-0.7344,  0.6914, -0.1309,  0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1234])
attention_mask shape: torch.Size([4, 1234])
reward: tensor([ 0.1875, -1.3359,  0.0801, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 914])
attention_mask shape: torch.Size([4, 914])
reward: tensor([-0.4629, -1.0078, -0.3066, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 760])
attention_mask shape: torch.Size([4, 760])
reward: tensor([ 0.0133,  0.5508, -0.1089, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1265])
attention_mask shape: torch.Size([4, 1265])
reward: tensor([ 0.3672,  0.5938, -0.6992,  0.2227], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 798])
attention_mask shape: torch.Size([4, 798])
reward: tensor([ 0.5117, -1.5391, -0.6719, -0.0156], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 789])
attention_mask shape: torch.Size([4, 789])
reward: tensor([-0.9961, -0.1045,  0.3438, -0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1236])
attention_mask shape: torch.Size([4, 1236])
reward: tensor([ 0.3320, -0.4844, -0.3457, -0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 976])
attention_mask shape: torch.Size([4, 976])
reward: tensor([ 0.0723,  0.8008,  0.6406, -0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 831])
attention_mask shape: torch.Size([4, 831])
reward: tensor([ 1.1562,  0.2812, -2.1719, -0.1729], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 656])
attention_mask shape: torch.Size([4, 656])
reward: tensor([ 1.3828, -0.3242, -0.1797, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([-0.9414,  0.5195,  0.0579,  0.8125], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 832])
attention_mask shape: torch.Size([4, 832])
reward: tensor([-0.5273, -1.0156, -0.8516,  1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 849])
attention_mask shape: torch.Size([4, 849])
reward: tensor([-0.4453, -0.2930,  0.5391, -0.2314], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1111])
attention_mask shape: torch.Size([4, 1111])
reward: tensor([-0.5898,  0.9883, -1.4219,  1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 705])
attention_mask shape: torch.Size([4, 705])
reward: tensor([-0.2422, -0.6680, -1.6016, -0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 908])
attention_mask shape: torch.Size([4, 908])
reward: tensor([-0.2676, -1.2109,  0.6211, -1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 577])
attention_mask shape: torch.Size([4, 577])
reward: tensor([ 0.1318,  0.2119, -0.6250,  0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1022])
attention_mask shape: torch.Size([4, 1022])
reward: tensor([-0.0933, -0.8320, -1.6641,  0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1208])
attention_mask shape: torch.Size([4, 1208])
reward: tensor([-0.9766, -0.8164, -0.8359,  0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1493])
attention_mask shape: torch.Size([4, 1493])
reward: tensor([ 0.7969, -0.4258, -0.2871, -0.2402], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1083])
attention_mask shape: torch.Size([4, 1083])
reward: tensor([ 0.2617,  0.6328, -0.2930, -1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 641])
attention_mask shape: torch.Size([4, 641])
reward: tensor([-1.7266, -1.0547, -0.2930, -0.1797], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 797])
attention_mask shape: torch.Size([4, 797])
reward: tensor([-0.0422, -0.8711, -0.1113, -0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 738])
attention_mask shape: torch.Size([4, 738])
reward: tensor([0.2402, 1.1562, 0.8672, 0.0544], device='cuda:0', dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1064])
attention_mask shape: torch.Size([4, 1064])
reward: tensor([-0.4258,  0.1924, -0.3340,  0.0879], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1176])
attention_mask shape: torch.Size([4, 1176])
reward: tensor([-0.0557, -0.6055, -0.9688, -0.7773], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1179])
attention_mask shape: torch.Size([4, 1179])
reward: tensor([ 0.5352, -0.4531,  0.1221, -1.2031], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1128])
attention_mask shape: torch.Size([4, 1128])
reward: tensor([ 1.0859, -2.2188, -0.4004, -0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 513])
attention_mask shape: torch.Size([4, 513])
reward: tensor([-0.1426, -0.4258, -0.4395, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1151])
attention_mask shape: torch.Size([4, 1151])
reward: tensor([ 2.0312, -1.7266,  1.3125, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1045])
attention_mask shape: torch.Size([4, 1045])
reward: tensor([ 1.1562,  0.9453, -0.2490,  0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1168])
attention_mask shape: torch.Size([4, 1168])
reward: tensor([-0.5820,  0.5352, -0.7461,  1.2656], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1418])
attention_mask shape: torch.Size([4, 1418])
reward: tensor([ 1.3125, -0.2715,  1.2656, -0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 293])
attention_mask shape: torch.Size([4, 293])
reward: tensor([-0.6641, -2.2188, -0.1777, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1260])
attention_mask shape: torch.Size([4, 1260])
reward: tensor([-0.4570,  1.2656, -2.1094,  1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 997])
attention_mask shape: torch.Size([4, 997])
reward: tensor([-0.5039,  0.0776,  0.0557,  0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1228])
attention_mask shape: torch.Size([4, 1228])
reward: tensor([-0.1001, -1.0703,  0.4531, -0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1366])
attention_mask shape: torch.Size([4, 1366])
reward: tensor([ 0.5430,  0.2812, -0.4844, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1336])
attention_mask shape: torch.Size([4, 1336])
reward: tensor([-1.0156, -2.1250,  0.7109,  0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1012])
attention_mask shape: torch.Size([4, 1012])
reward: tensor([-0.7031, -0.4180,  0.1934, -1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1506])
attention_mask shape: torch.Size([4, 1506])
reward: tensor([-0.6094, -1.1562, -0.3242, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1267])
attention_mask shape: torch.Size([4, 1267])
reward: tensor([-1.8750,  0.5078, -0.5938, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 705])
attention_mask shape: torch.Size([4, 705])
reward: tensor([-0.9336, -0.9336, -1.0469, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1352])
attention_mask shape: torch.Size([4, 1352])
reward: tensor([-0.6719, -0.2969, -0.7461,  0.4629], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 552])
attention_mask shape: torch.Size([4, 552])
reward: tensor([ 1.2812, -1.6172,  0.0579, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1461])
attention_mask shape: torch.Size([4, 1461])
reward: tensor([-0.8906, -0.4004,  0.2559, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1217])
attention_mask shape: torch.Size([4, 1217])
reward: tensor([-1.0234, -0.2871, -0.2637, -0.0289], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1137])
attention_mask shape: torch.Size([4, 1137])
reward: tensor([-0.2891,  0.2891,  0.0967, -0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1753])
attention_mask shape: torch.Size([4, 1753])
reward: tensor([ 0.3398,  0.6680, -1.7031,  0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1344])
attention_mask shape: torch.Size([4, 1344])
reward: tensor([-0.6250,  0.0466,  0.7109, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 554])
attention_mask shape: torch.Size([4, 554])
reward: tensor([ 0.0488, -0.6406,  0.1934, -0.3828], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 484])
attention_mask shape: torch.Size([4, 484])
reward: tensor([-0.4355, -0.1670,  1.1250,  0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1632])
attention_mask shape: torch.Size([4, 1632])
reward: tensor([-2.2188,  0.4980,  1.5234, -0.6719], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1090])
attention_mask shape: torch.Size([4, 1090])
reward: tensor([-0.4883, -1.1719, -0.7852, -0.2578], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 876])
attention_mask shape: torch.Size([4, 876])
reward: tensor([-1.3750, -0.7031,  0.1670, -0.0845], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1169])
attention_mask shape: torch.Size([4, 1169])
reward: tensor([-0.1885, -0.1826, -0.9141, -1.3281], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 593])
attention_mask shape: torch.Size([4, 593])
reward: tensor([-0.3242, -0.0601, -0.1953,  0.2148], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 943])
attention_mask shape: torch.Size([4, 943])
reward: tensor([-0.3281, -0.6562,  0.0723,  0.1904], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 543])
attention_mask shape: torch.Size([4, 543])
reward: tensor([-1.0859, -0.5078, -0.1709, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1435])
attention_mask shape: torch.Size([4, 1435])
reward: tensor([-0.2354,  1.4688, -0.0078, -0.1113], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1203])
attention_mask shape: torch.Size([4, 1203])
reward: tensor([-0.0334,  0.2676, -0.8633, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1064])
attention_mask shape: torch.Size([4, 1064])
reward: tensor([-0.9766, -0.4629, -0.3281, -1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1003])
attention_mask shape: torch.Size([4, 1003])
reward: tensor([ 0.1484, -0.8711,  0.7188,  0.2148], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 551])
attention_mask shape: torch.Size([4, 551])
reward: tensor([-0.2930, -0.2334, -0.1377,  0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 537])
attention_mask shape: torch.Size([4, 537])
reward: tensor([ 1.1719, -2.1719, -0.7461, -0.0801], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 956])
attention_mask shape: torch.Size([4, 956])
reward: tensor([-0.6836,  0.0033, -0.8633, -0.3691], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 476])
attention_mask shape: torch.Size([4, 476])
reward: tensor([-0.6172,  0.5547, -1.6719,  0.0400], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 677])
attention_mask shape: torch.Size([4, 677])
reward: tensor([ 0.3594,  0.4180, -0.9688, -0.0156], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1441])
attention_mask shape: torch.Size([4, 1441])
reward: tensor([ 0.0479, -0.4531, -0.1484, -0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1304])
attention_mask shape: torch.Size([4, 1304])
reward: tensor([ 1.1016,  1.0156, -1.2266,  0.6328], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1237])
attention_mask shape: torch.Size([4, 1237])
reward: tensor([-1.2344, -1.8281,  1.0703,  0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 556])
attention_mask shape: torch.Size([4, 556])
reward: tensor([-1.0156, -0.4258, -1.2266, -1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1131])
attention_mask shape: torch.Size([4, 1131])
reward: tensor([-0.9062, -0.3906,  0.2432, -1.9609], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1528])
attention_mask shape: torch.Size([4, 1528])
reward: tensor([-0.9961, -0.7656,  0.8477, -0.0889], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 366])
attention_mask shape: torch.Size([4, 366])
reward: tensor([-1.0703, -1.2969, -1.7812, -0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([0.0933, 0.7500, 0.8516, 0.1235], device='cuda:0', dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1247])
attention_mask shape: torch.Size([4, 1247])
reward: tensor([ 0.9805, -0.4805, -1.1094, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 766])
attention_mask shape: torch.Size([4, 766])
reward: tensor([ 0.1748, -0.0820, -0.4805, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 644])
attention_mask shape: torch.Size([4, 644])
reward: tensor([-0.5195,  1.4766,  0.9648, -1.2891], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1372])
attention_mask shape: torch.Size([4, 1372])
reward: tensor([-0.5039,  0.3008, -0.0033,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 632])
attention_mask shape: torch.Size([4, 632])
reward: tensor([-0.2354, -0.7070, -0.1865, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 582])
attention_mask shape: torch.Size([4, 582])
reward: tensor([-0.8203, -1.7422, -0.8906,  1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1170])
attention_mask shape: torch.Size([4, 1170])
reward: tensor([-0.6562, -0.5156, -1.5938, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1693])
attention_mask shape: torch.Size([4, 1693])
reward: tensor([-0.4980,  0.8438, -0.3828,  2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 889])
attention_mask shape: torch.Size([4, 889])
reward: tensor([ 0.4902, -0.3691, -1.2422, -1.1797], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 575])
attention_mask shape: torch.Size([4, 575])
reward: tensor([-0.5781, -0.4980, -0.7109, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1083])
attention_mask shape: torch.Size([4, 1083])
reward: tensor([ 0.3730, -0.9766, -0.4355,  0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1195])
attention_mask shape: torch.Size([4, 1195])
reward: tensor([ 1.2031, -0.5938, -0.8320,  0.1045], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 500])
attention_mask shape: torch.Size([4, 500])
reward: tensor([-0.4707, -0.9414,  0.0444,  0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1135])
attention_mask shape: torch.Size([4, 1135])
reward: tensor([-1.3047,  0.6914, -0.4492,  0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 807])
attention_mask shape: torch.Size([4, 807])
reward: tensor([-0.5586, -2.1562, -1.0078,  0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1015])
attention_mask shape: torch.Size([4, 1015])
reward: tensor([ 0.0957, -1.8047, -1.0859,  0.1699], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1069])
attention_mask shape: torch.Size([4, 1069])
reward: tensor([-0.3340, -1.2188, -0.4707,  0.1621], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1267])
attention_mask shape: torch.Size([4, 1267])
reward: tensor([ 0.9219, -1.3984,  2.2812, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1409])
attention_mask shape: torch.Size([4, 1409])
reward: tensor([ 2.0938, -0.7461, -0.0178,  0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 749])
attention_mask shape: torch.Size([4, 749])
reward: tensor([-0.1338, -0.0820,  0.7500,  0.2441], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1334])
attention_mask shape: torch.Size([4, 1334])
reward: tensor([ 1.1719, -0.3965, -1.6641,  0.3574], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 389])
attention_mask shape: torch.Size([4, 389])
reward: tensor([-0.7188, -1.2812, -0.3027, -0.2070], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1343])
attention_mask shape: torch.Size([4, 1343])
reward: tensor([-0.6250, -0.4141,  1.2656, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1077])
attention_mask shape: torch.Size([4, 1077])
reward: tensor([ 0.7461, -0.3203, -1.9766, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 790])
attention_mask shape: torch.Size([4, 790])
reward: tensor([ 0.8750, -1.8906,  0.8711, -0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 840])
attention_mask shape: torch.Size([4, 840])
reward: tensor([-0.2637, -1.6875,  0.4746, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1316])
attention_mask shape: torch.Size([4, 1316])
reward: tensor([-0.4316, -0.8008, -0.0623, -0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1003])
attention_mask shape: torch.Size([4, 1003])
reward: tensor([-0.0400, -0.6914,  1.5078, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([ 0.6133, -0.3027, -1.3438,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1617])
attention_mask shape: torch.Size([4, 1617])
reward: tensor([-0.5742, -1.1641,  1.7500, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 827])
attention_mask shape: torch.Size([4, 827])
reward: tensor([-0.9062, -1.1250, -1.2891,  0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 928])
attention_mask shape: torch.Size([4, 928])
reward: tensor([ 0.6328, -0.3340,  0.6836,  0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1300])
attention_mask shape: torch.Size([4, 1300])
reward: tensor([-0.2178, -0.6680, -0.5156, -0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1987])
attention_mask shape: torch.Size([4, 1987])
reward: tensor([-1.7266, -0.9336, -1.3516,  0.0835], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1168])
attention_mask shape: torch.Size([4, 1168])
reward: tensor([ 0.1377, -0.3340, -0.8281,  0.0623], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 411])
attention_mask shape: torch.Size([4, 411])
reward: tensor([-0.5039, -0.2871, -0.3516,  0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1372])
attention_mask shape: torch.Size([4, 1372])
reward: tensor([-0.1157,  0.7109, -0.8281, -1.3047], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 700])
attention_mask shape: torch.Size([4, 700])
reward: tensor([-2.1250, -0.9961,  1.5391, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 612])
attention_mask shape: torch.Size([4, 612])
reward: tensor([ 0.1001, -0.3066,  1.1016, -0.2871], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1615])
attention_mask shape: torch.Size([4, 1615])
reward: tensor([ 1.5312,  0.0522,  0.9258, -0.1934], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
[2024-10-22 04:12:34,566] [INFO] [launch.py:351:main] Process 611189 exits successfully.
sequences shape: torch.Size([4, 942])
attention_mask shape: torch.Size([4, 942])
reward: tensor([ 1.1719,  0.3438,  0.5430, -0.4453], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-22 04:12:45,577] [INFO] [launch.py:351:main] Process 611188 exits successfully.
[2024-10-22 04:13:02,594] [INFO] [launch.py:351:main] Process 611190 exits successfully.
[2024-10-22 04:16:35,812] [INFO] [launch.py:351:main] Process 611191 exits successfully.
+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-5th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-5th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_5 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-22 04:16:40,989] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 04:16:42,837] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-22 04:16:42,838] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-5th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_5 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-22 04:16:44,214] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 04:16:46,162] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-22 04:16:46,162] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-22 04:16:46,162] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-22 04:16:46,162] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-22 04:16:46,162] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-22 04:16:46,163] [INFO] [launch.py:256:main] process 612437 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-5th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_5', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 04:16:46,163] [INFO] [launch.py:256:main] process 612438 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-5th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_5', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 04:16:46,164] [INFO] [launch.py:256:main] process 612439 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-5th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_5', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 04:16:46,164] [INFO] [launch.py:256:main] process 612440 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-5th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_5', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-22 04:16:47,867] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 04:16:47,874] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 04:16:47,875] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-22 04:16:47,879] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-22 04:16:50,435] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 04:16:50,435] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-22 04:16:50,724] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 04:16:50,725] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-22 04:16:50,747] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.19it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.30it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  6.56it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.27it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  6.34it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  6.63it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.16it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.28it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  6.34it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  6.63it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.14it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.51it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.42it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.87it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.77it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  6.35it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.23it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.20it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.55it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.47it/s]
[2024-10-22 04:16:53,440] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 04:16:53,491] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-22 04:16:53,657] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-22 04:16:53,658] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-22 04:16:53,658] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 04:16:53,680] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 04:16:54,162] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-22 04:16:54,163] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-22 04:16:54,163] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 04:16:54,163] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 04:16:54,164] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 04:16:54,314] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-22 04:16:54,316] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-22 04:16:54,316] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.48 GB, percent = 2.4%
[2024-10-22 04:16:54,472] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-22 04:16:54,472] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-22 04:16:54,473] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.49 GB, percent = 2.4%
[2024-10-22 04:16:54,473] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6d1a522770>
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-22 04:16:54,474] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-22 04:16:54,475] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-22 04:16:54,476] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-22 04:16:54,476] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-22 04:16:54,476] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-22 04:16:54,476] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-22 04:16:54,476] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-22 04:16:58,015] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-22 04:16:58,017] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-22 04:16:58,177] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-22 04:16:58,177] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-22 04:16:58,178] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.49 GB, percent = 2.4%
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
[2024-10-22 04:16:58,313] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-22 04:16:58,314] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-22 04:16:58,314] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.5 GB, percent = 2.4%
[2024-10-22 04:16:58,315] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-22 04:16:58,315] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-22 04:16:58,315] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-22 04:16:58,315] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-22 04:16:58,315] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6d09e128c0>
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-22 04:16:58,316] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-22 04:16:58,317] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-22 04:16:58,317] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 641/100000 [00:00<00:15, 6406.19it/s]Preprocessing data:   2%|█▌                                                                                          | 1660/100000 [00:00<00:11, 8626.81it/s]Preprocessing data:   3%|██▍                                                                                         | 2681/100000 [00:00<00:10, 9349.18it/s]Preprocessing data:   4%|███▍                                                                                        | 3699/100000 [00:00<00:09, 9674.66it/s]Preprocessing data:   5%|████▎                                                                                       | 4720/100000 [00:00<00:09, 9865.14it/s]Preprocessing data:   6%|█████▎                                                                                      | 5731/100000 [00:00<00:09, 9948.15it/s]Preprocessing data:   7%|██████▏                                                                                    | 6747/100000 [00:00<00:09, 10014.95it/s]Preprocessing data:   8%|███████                                                                                    | 7757/100000 [00:00<00:09, 10040.12it/s]Preprocessing data:   9%|███████▉                                                                                   | 8762/100000 [00:00<00:09, 10036.54it/s]Preprocessing data:  10%|████████▉                                                                                  | 9777/100000 [00:01<00:08, 10069.20it/s]Preprocessing data:  11%|█████████▊                                                                                | 10836/100000 [00:01<00:08, 10227.56it/s]Preprocessing data:  12%|██████████▋                                                                               | 11907/100000 [00:01<00:08, 10371.52it/s]Preprocessing data:  13%|███████████▋                                                                              | 12978/100000 [00:01<00:08, 10471.75it/s]Preprocessing data:  14%|████████████▋                                                                             | 14044/100000 [00:01<00:08, 10525.73it/s]Preprocessing data:  15%|█████████████▌                                                                            | 15110/100000 [00:01<00:08, 10565.61it/s]Preprocessing data:  16%|██████████████▌                                                                           | 16168/100000 [00:01<00:07, 10567.46it/s]Preprocessing data:  17%|███████████████▌                                                                          | 17233/100000 [00:01<00:07, 10591.07it/s]Preprocessing data:  18%|████████████████▍                                                                         | 18293/100000 [00:01<00:07, 10583.22it/s]Preprocessing data:  19%|█████████████████▍                                                                        | 19361/100000 [00:01<00:07, 10612.09it/s]Preprocessing data:  20%|██████████████████▍                                                                       | 20437/100000 [00:02<00:07, 10654.25it/s]Preprocessing data:  22%|███████████████████▎                                                                      | 21521/100000 [00:02<00:07, 10706.93it/s]Preprocessing data:  23%|████████████████████▎                                                                     | 22592/100000 [00:02<00:07, 10619.53it/s]Preprocessing data:  24%|█████████████████████▎                                                                    | 23655/100000 [00:02<00:07, 10602.97it/s]Preprocessing data:  25%|██████████████████████▏                                                                   | 24716/100000 [00:02<00:07, 10588.80it/s]Preprocessing data:  26%|███████████████████████▏                                                                  | 25775/100000 [00:02<00:07, 10527.85it/s]Preprocessing data:  27%|████████████████████████▏                                                                 | 26828/100000 [00:02<00:06, 10507.64it/s]Preprocessing data:  28%|█████████████████████████                                                                 | 27879/100000 [00:02<00:06, 10487.70it/s]Preprocessing data:  29%|██████████████████████████                                                                | 28928/100000 [00:02<00:06, 10436.52it/s]Preprocessing data:  30%|██████████████████████████▉                                                               | 29972/100000 [00:02<00:06, 10431.85it/s]Preprocessing data:  31%|███████████████████████████▉                                                              | 31016/100000 [00:03<00:06, 10315.95it/s]Preprocessing data:  32%|████████████████████████████▊                                                             | 32054/100000 [00:03<00:06, 10334.53it/s]Preprocessing data:  33%|█████████████████████████████▊                                                            | 33088/100000 [00:03<00:06, 10323.38it/s]Preprocessing data:  34%|██████████████████████████████▋                                                           | 34121/100000 [00:03<00:06, 10231.69it/s]Preprocessing data:  35%|███████████████████████████████▋                                                          | 35145/100000 [00:03<00:06, 10215.02it/s]Preprocessing data:  36%|████████████████████████████████▌                                                         | 36169/100000 [00:03<00:06, 10220.90it/s]Preprocessing data:  37%|█████████████████████████████████▍                                                        | 37210/100000 [00:03<00:06, 10274.99it/s]Preprocessing data:  38%|██████████████████████████████████▍                                                       | 38238/100000 [00:03<00:06, 10248.20it/s]Preprocessing data:  39%|███████████████████████████████████▎                                                      | 39279/100000 [00:03<00:05, 10295.42it/s]Preprocessing data:  40%|████████████████████████████████████▎                                                     | 40323/100000 [00:03<00:05, 10337.32it/s]Preprocessing data:  41%|█████████████████████████████████████▏                                                    | 41357/100000 [00:04<00:05, 10321.09it/s]Preprocessing data:  42%|██████████████████████████████████████▏                                                   | 42391/100000 [00:04<00:05, 10325.21it/s]Preprocessing data:  43%|███████████████████████████████████████                                                   | 43436/100000 [00:04<00:05, 10359.77it/s]Preprocessing data:  44%|████████████████████████████████████████                                                  | 44487/100000 [00:04<00:05, 10403.55it/s]Preprocessing data:  46%|████████████████████████████████████████▉                                                 | 45554/100000 [00:04<00:05, 10482.95it/s]Preprocessing data:  47%|█████████████████████████████████████████▉                                                | 46622/100000 [00:04<00:05, 10540.25it/s]Preprocessing data:  48%|██████████████████████████████████████████▉                                               | 47677/100000 [00:04<00:05, 10461.86it/s]Preprocessing data:  49%|███████████████████████████████████████████▊                                              | 48741/100000 [00:04<00:04, 10513.81it/s]Preprocessing data:  50%|████████████████████████████████████████████▊                                             | 49805/100000 [00:04<00:04, 10551.16it/s]Preprocessing data:  51%|█████████████████████████████████████████████▊                                            | 50861/100000 [00:04<00:04, 10542.91it/s]Preprocessing data:  52%|██████████████████████████████████████████████▋                                           | 51927/100000 [00:05<00:04, 10576.03it/s]Preprocessing data:  53%|███████████████████████████████████████████████▋                                          | 52996/100000 [00:05<00:04, 10608.40it/s]Preprocessing data:  54%|████████████████████████████████████████████████▋                                         | 54064/100000 [00:05<00:04, 10627.64it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▌                                        | 55130/100000 [00:05<00:04, 10581.86it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▌                                       | 56196/100000 [00:05<00:04, 10602.73it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▌                                      | 57257/100000 [00:05<00:04, 10569.95it/s]Preprocessing data:  58%|████████████████████████████████████████████████████▍                                     | 58324/100000 [00:05<00:03, 10597.67it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████▍                                    | 59393/100000 [00:05<00:03, 10622.92it/s]Preprocessing data:  60%|██████████████████████████████████████████████████████▍                                   | 60456/100000 [00:05<00:03, 10594.71it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▎                                  | 61523/100000 [00:05<00:03, 10616.32it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▎                                 | 62599/100000 [00:06<00:03, 10657.04it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▎                                | 63665/100000 [00:06<00:03, 10630.30it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▎                               | 64729/100000 [00:06<00:03, 10619.38it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▏                              | 65795/100000 [00:06<00:03, 10628.60it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████▏                             | 66858/100000 [00:06<00:03, 10625.05it/s]Preprocessing data:  68%|█████████████████████████████████████████████████████████████▏                            | 67921/100000 [00:06<00:03, 10499.87it/s]Preprocessing data:  69%|██████████████████████████████████████████████████████████████                            | 68978/100000 [00:06<00:02, 10519.31it/s]Preprocessing data:  70%|███████████████████████████████████████████████████████████████                           | 70042/100000 [00:06<00:02, 10552.99it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▉                          | 71101/100000 [00:06<00:02, 10563.88it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▉                         | 72162/100000 [00:06<00:02, 10575.27it/s]Preprocessing data:  73%|██████████████████████████████████████████████████████████████████▋                        | 73220/100000 [00:07<00:02, 9862.78it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▌                       | 74216/100000 [00:07<00:02, 9215.04it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▍                      | 75151/100000 [00:07<00:02, 8802.38it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████▏                     | 76042/100000 [00:07<00:02, 8536.65it/s]Preprocessing data:  77%|█████████████████████████████████████████████████████████████████████▉                     | 76903/100000 [00:07<00:02, 8293.38it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▋                    | 77737/100000 [00:07<00:02, 8141.32it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▍                   | 78554/100000 [00:07<00:02, 8073.76it/s]Preprocessing data:  79%|████████████████████████████████████████████████████████████████████████▏                  | 79363/100000 [00:07<00:02, 8019.95it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▉                  | 80166/100000 [00:07<00:02, 7841.06it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▋                 | 80951/100000 [00:08<00:02, 7806.62it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▌                | 81935/100000 [00:08<00:02, 8390.76it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▎               | 82777/100000 [00:08<00:02, 8301.67it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████               | 83609/100000 [00:08<00:01, 8273.29it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▉              | 84492/100000 [00:08<00:01, 8435.40it/s]Preprocessing data:  86%|█████████████████████████████████████████████████████████████████████████████▊             | 85534/100000 [00:08<00:01, 9018.52it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▋            | 86438/100000 [00:08<00:01, 8796.93it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▍           | 87320/100000 [00:08<00:01, 8652.94it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████▎          | 88322/100000 [00:08<00:01, 9049.82it/s]Preprocessing data:  89%|█████████████████████████████████████████████████████████████████████████████████▎         | 89287/100000 [00:08<00:01, 9223.12it/s]Preprocessing data:  90%|██████████████████████████████████████████████████████████████████████████████████         | 90217/100000 [00:09<00:01, 9243.67it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▉        | 91143/100000 [00:09<00:01, 8522.19it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▋       | 92007/100000 [00:09<00:00, 8069.67it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▍      | 92826/100000 [00:09<00:00, 7748.18it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▏     | 93610/100000 [00:09<00:00, 7679.68it/s]Preprocessing data:  95%|█████████████████████████████████████████████████████████████████████████████████████▉     | 94504/100000 [00:09<00:00, 8028.07it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▉    | 95485/100000 [00:09<00:00, 8534.18it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▋   | 96402/100000 [00:09<00:00, 8714.51it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▌  | 97280/100000 [00:09<00:00, 8717.18it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▎ | 98156/100000 [00:10<00:00, 8726.61it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████▏| 99160/100000 [00:10<00:00, 9113.91it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9776.96it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1109])
attention_mask shape: torch.Size([4, 1109])
reward: tensor([-0.6484, -1.2109, -0.6211, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1365])
attention_mask shape: torch.Size([4, 1365])
reward: tensor([ 1.9219, -0.5703, -0.4043, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1046])
attention_mask shape: torch.Size([4, 1046])
reward: tensor([-1.4219, -1.4844, -1.9531, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 310])
attention_mask shape: torch.Size([4, 310])
reward: tensor([ 0.0410, -1.7656,  0.2227, -0.6094], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1423])
attention_mask shape: torch.Size([4, 1423])
reward: tensor([ 1.1094,  0.8516, -0.8125, -0.4355], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 609])
attention_mask shape: torch.Size([4, 609])
reward: tensor([-0.1426, -1.4453, -0.7305, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-0.1514, -0.5859, -0.2715, -1.7031], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 926])
attention_mask shape: torch.Size([4, 926])
reward: tensor([-1.3125, -0.4941, -0.8438, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1014])
attention_mask shape: torch.Size([4, 1014])
reward: tensor([-1.6406,  1.0938, -0.0022, -0.8008], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1050])
attention_mask shape: torch.Size([4, 1050])
reward: tensor([ 0.3105,  1.4375, -0.1357, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1021])
attention_mask shape: torch.Size([4, 1021])
reward: tensor([-2.1250,  0.3281, -0.6172, -0.1953], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 810])
attention_mask shape: torch.Size([4, 810])
reward: tensor([ 0.2715, -0.8125, -0.0713, -0.5156], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 529])
attention_mask shape: torch.Size([4, 529])
reward: tensor([-1.6641, -0.8086, -0.7773, -0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1375])
attention_mask shape: torch.Size([4, 1375])
reward: tensor([-0.9414,  0.4043, -0.8477,  0.2080], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 320])
attention_mask shape: torch.Size([4, 320])
reward: tensor([-0.3516, -0.9609, -0.3105, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1408])
attention_mask shape: torch.Size([4, 1408])
reward: tensor([-0.1338, -0.2041, -0.2314, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1333])
attention_mask shape: torch.Size([4, 1333])
reward: tensor([-1.1250, -1.4297, -0.0466,  0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1308])
attention_mask shape: torch.Size([4, 1308])
reward: tensor([-1.1562, -0.7344, -0.6211,  0.2158], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 602])
attention_mask shape: torch.Size([4, 602])
reward: tensor([-0.0466, -0.4180, -0.9766, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 553])
attention_mask shape: torch.Size([4, 553])
reward: tensor([ 1.1797,  0.1367, -1.4609,  0.0366], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 834])
attention_mask shape: torch.Size([4, 834])
reward: tensor([-0.9336,  0.1865, -0.7148, -1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 955])
attention_mask shape: torch.Size([4, 955])
reward: tensor([ 0.1670,  0.7383,  0.3359, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 942])
attention_mask shape: torch.Size([4, 942])
reward: tensor([-1.6719,  0.5703, -0.0244, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1398])
attention_mask shape: torch.Size([4, 1398])
reward: tensor([0.6172, 0.0145, 1.7812, 0.4844], device='cuda:0', dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1453])
attention_mask shape: torch.Size([4, 1453])
reward: tensor([ 0.8438, -0.3516, -2.0938, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1476])
attention_mask shape: torch.Size([4, 1476])
reward: tensor([ 0.5156, -0.8633,  0.2773, -0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1033])
attention_mask shape: torch.Size([4, 1033])
reward: tensor([-0.6914, -0.5508,  0.3086, -0.0444], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 253])
attention_mask shape: torch.Size([4, 253])
reward: tensor([-0.8359, -1.1406, -1.0391, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1009])
attention_mask shape: torch.Size([4, 1009])
reward: tensor([ 0.6055, -1.8906, -0.4746,  0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1502])
attention_mask shape: torch.Size([4, 1502])
reward: tensor([ 0.0791,  1.5859, -0.2334, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1101])
attention_mask shape: torch.Size([4, 1101])
reward: tensor([-0.9492,  0.5469, -0.7695, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 605])
attention_mask shape: torch.Size([4, 605])
reward: tensor([-1.9766, -0.1709, -0.3203, -0.6445], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1328])
attention_mask shape: torch.Size([4, 1328])
reward: tensor([-2.0156, -0.3340,  0.8008, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1102])
attention_mask shape: torch.Size([4, 1102])
reward: tensor([-0.0111, -0.5234, -1.2891, -0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 767])
attention_mask shape: torch.Size([4, 767])
reward: tensor([-0.5547, -1.0078, -0.8711, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1269])
attention_mask shape: torch.Size([4, 1269])
reward: tensor([-0.7773, -0.3164,  0.7539, -1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1152])
attention_mask shape: torch.Size([4, 1152])
reward: tensor([-0.9258, -0.9336, -0.3242, -1.6016], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1144])
attention_mask shape: torch.Size([4, 1144])
reward: tensor([ 0.4141, -1.7344,  0.5938, -0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 539])
attention_mask shape: torch.Size([4, 539])
reward: tensor([-1.7656, -0.0801,  0.1099, -1.7969], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1720])
attention_mask shape: torch.Size([4, 1720])
reward: tensor([-0.1709, -0.2891,  0.0957,  1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1078])
attention_mask shape: torch.Size([4, 1078])
reward: tensor([ 0.0889, -0.3594, -0.8008,  0.9727], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([ 0.9805,  0.9922, -0.1621,  0.7539], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 247])
attention_mask shape: torch.Size([4, 247])
reward: tensor([-1.4141, -0.8164, -0.1934, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 792])
attention_mask shape: torch.Size([4, 792])
reward: tensor([-1.6797, -0.2178, -1.2344, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1316])
attention_mask shape: torch.Size([4, 1316])
reward: tensor([-1.0156,  0.4102,  0.6406,  1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 776])
attention_mask shape: torch.Size([4, 776])
reward: tensor([-0.0864, -1.0938, -0.5938, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 621])
attention_mask shape: torch.Size([4, 621])
reward: tensor([-1.0391, -0.2754,  0.2773, -1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1945])
attention_mask shape: torch.Size([4, 1945])
reward: tensor([ 0.1445,  0.0056, -0.2334, -0.2490], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1086])
attention_mask shape: torch.Size([4, 1086])
reward: tensor([-0.3242, -0.4668, -0.6719, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 655])
attention_mask shape: torch.Size([4, 655])
reward: tensor([-0.9609,  0.0889, -1.1797, -0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1268])
attention_mask shape: torch.Size([4, 1268])
reward: tensor([-0.7617, -0.4629, -0.6406, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1199])
attention_mask shape: torch.Size([4, 1199])
reward: tensor([-0.4707,  1.6172, -0.3242,  1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1342])
attention_mask shape: torch.Size([4, 1342])
reward: tensor([ 0.4512, -1.6797, -0.3594, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1538])
attention_mask shape: torch.Size([4, 1538])
reward: tensor([-1.6250, -2.0000,  0.2559,  1.8828], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1417])
attention_mask shape: torch.Size([4, 1417])
reward: tensor([-0.3457,  0.5625,  0.3867, -0.4395], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 573])
attention_mask shape: torch.Size([4, 573])
reward: tensor([-0.9961, -1.3281, -1.4922, -1.6328], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1000])
attention_mask shape: torch.Size([4, 1000])
reward: tensor([-0.8984, -1.3359,  0.3809, -0.3418], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 559])
attention_mask shape: torch.Size([4, 559])
reward: tensor([-0.9883,  0.4980, -0.8438, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1269])
attention_mask shape: torch.Size([4, 1269])
reward: tensor([-0.4258, -0.3906, -2.0312, -1.4297], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1380])
attention_mask shape: torch.Size([4, 1380])
reward: tensor([-1.5469,  1.2266, -1.4062, -0.0913], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 964])
attention_mask shape: torch.Size([4, 964])
reward: tensor([-1.2734, -1.6719, -1.4922, -1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1231])
attention_mask shape: torch.Size([4, 1231])
reward: tensor([-0.1445, -1.0312,  1.3672, -1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1434])
attention_mask shape: torch.Size([4, 1434])
reward: tensor([-0.3516, -0.7695, -1.4453,  0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 479])
attention_mask shape: torch.Size([4, 479])
reward: tensor([-1.2266, -0.0356, -1.5703, -1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 907])
attention_mask shape: torch.Size([4, 907])
reward: tensor([ 0.2266, -0.1514, -0.2422, -1.8125], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 676])
attention_mask shape: torch.Size([4, 676])
reward: tensor([-0.1338,  0.7031, -1.6875, -0.0111], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 627])
attention_mask shape: torch.Size([4, 627])
reward: tensor([-0.6992, -0.6211, -0.9609, -0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-1.4219, -0.8320, -1.5078,  1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 494])
attention_mask shape: torch.Size([4, 494])
reward: tensor([ 0.6055, -0.1201,  0.0056,  0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1228])
attention_mask shape: torch.Size([4, 1228])
reward: tensor([-0.7188, -1.4844,  0.7656,  0.1611], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1326])
attention_mask shape: torch.Size([4, 1326])
reward: tensor([-1.2266, -0.4492, -0.4043,  0.4355], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 979])
attention_mask shape: torch.Size([4, 979])
reward: tensor([ 0.7930, -1.0391, -1.0391, -1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1024])
attention_mask shape: torch.Size([4, 1024])
reward: tensor([-1.1719, -0.3203, -0.6172, -0.5898], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([0.3496, 0.3398, 0.1187, 0.7109], device='cuda:0', dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 403])
attention_mask shape: torch.Size([4, 403])
reward: tensor([-1.5234, -0.5703, -1.1406, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 492])
attention_mask shape: torch.Size([4, 492])
reward: tensor([-0.7383, -1.0312, -2.1719, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1265])
attention_mask shape: torch.Size([4, 1265])
reward: tensor([ 1.8594, -0.2910, -1.6094, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1185])
attention_mask shape: torch.Size([4, 1185])
reward: tensor([ 0.5430, -0.7188, -0.8359, -1.2031], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 859])
attention_mask shape: torch.Size([4, 859])
reward: tensor([-0.0488,  0.0167, -0.7695,  0.4824], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 574])
attention_mask shape: torch.Size([4, 574])
reward: tensor([-1.3594, -2.1250, -0.9141, -0.8125], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 759])
attention_mask shape: torch.Size([4, 759])
reward: tensor([-0.3828,  0.0133,  0.9688, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 467])
attention_mask shape: torch.Size([4, 467])
reward: tensor([-0.7617, -0.5273, -0.5508, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1278])
attention_mask shape: torch.Size([4, 1278])
reward: tensor([-0.6367,  0.8203,  0.7812, -0.0933], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 689])
attention_mask shape: torch.Size([4, 689])
reward: tensor([-0.5781, -1.8516, -0.0669,  0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1245])
attention_mask shape: torch.Size([4, 1245])
reward: tensor([-0.5352, -0.5234,  0.4883,  0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1230])
attention_mask shape: torch.Size([4, 1230])
reward: tensor([-0.5273, -0.7500, -1.2266, -0.2598], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 721])
attention_mask shape: torch.Size([4, 721])
reward: tensor([-0.9609,  0.1133, -0.8789, -0.3828], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1342])
attention_mask shape: torch.Size([4, 1342])
reward: tensor([-0.6992,  0.1758,  0.4609, -0.2598], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1527])
attention_mask shape: torch.Size([4, 1527])
reward: tensor([ 0.3340, -1.7188,  0.8438,  0.3418], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 613])
attention_mask shape: torch.Size([4, 613])
reward: tensor([ 0.9609, -0.7539,  1.0156, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 796])
attention_mask shape: torch.Size([4, 796])
reward: tensor([-1.0469,  1.3516, -0.8164,  0.9648], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1637])
attention_mask shape: torch.Size([4, 1637])
reward: tensor([ 1.7656,  0.1357, -1.2109, -0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 475])
attention_mask shape: torch.Size([4, 475])
reward: tensor([ 0.3613, -0.9609, -2.2031, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1501])
attention_mask shape: torch.Size([4, 1501])
reward: tensor([ 0.4180,  1.4453, -0.6172, -0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1577])
attention_mask shape: torch.Size([4, 1577])
reward: tensor([ 0.3008, -0.2002,  1.1250, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 745])
attention_mask shape: torch.Size([4, 745])
reward: tensor([-0.6133,  1.0469, -1.8125, -0.1777], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1297])
attention_mask shape: torch.Size([4, 1297])
reward: tensor([-0.6094,  0.1953,  0.3672, -0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-0.6367,  0.6758, -0.5938, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 715])
attention_mask shape: torch.Size([4, 715])
reward: tensor([-0.4141, -0.4043, -0.3867, -1.5469], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 999])
attention_mask shape: torch.Size([4, 999])
reward: tensor([-1.1139e-03, -3.3447e-02,  2.1094e-01, -1.2969e+00], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1103])
attention_mask shape: torch.Size([4, 1103])
reward: tensor([-1.2969,  0.1641, -1.0156, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 591])
attention_mask shape: torch.Size([4, 591])
reward: tensor([ 0.2559, -0.5039, -0.8906, -0.0713], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1210])
attention_mask shape: torch.Size([4, 1210])
reward: tensor([ 0.2148, -0.7031, -0.2695,  0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([ 0.1826, -0.8398, -0.1982,  0.4062], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 956])
attention_mask shape: torch.Size([4, 956])
reward: tensor([-1.0156, -0.8984, -0.0033, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1315])
attention_mask shape: torch.Size([4, 1315])
reward: tensor([-0.7070, -1.0859, -1.5469,  1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1227])
attention_mask shape: torch.Size([4, 1227])
reward: tensor([-1.2500,  0.7383, -1.1484,  0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 757])
attention_mask shape: torch.Size([4, 757])
reward: tensor([-1.5859, -1.9922, -0.6484, -1.7891], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 714])
attention_mask shape: torch.Size([4, 714])
reward: tensor([-0.4746, -0.9961, -2.0000, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1766])
attention_mask shape: torch.Size([4, 1766])
reward: tensor([-0.3027, -0.6797, -0.0111,  0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1199])
attention_mask shape: torch.Size([4, 1199])
reward: tensor([-1.1719, -0.1533,  0.2949, -0.6367], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 232])
attention_mask shape: torch.Size([4, 232])
reward: tensor([ 0.3867, -1.1094, -1.0312, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1110])
attention_mask shape: torch.Size([4, 1110])
reward: tensor([-1.9531, -0.2070, -0.0933,  0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 657])
attention_mask shape: torch.Size([4, 657])
reward: tensor([-0.6914, -1.0703, -1.6016,  0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 930])
attention_mask shape: torch.Size([4, 930])
reward: tensor([-2.0469,  0.3320, -0.1729, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 821])
attention_mask shape: torch.Size([4, 821])
reward: tensor([-0.7617, -0.7070, -0.0045,  0.0579], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1444])
attention_mask shape: torch.Size([4, 1444])
reward: tensor([-1.1562,  0.2656,  1.4922, -0.3418], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 861])
attention_mask shape: torch.Size([4, 861])
reward: tensor([ 0.1611, -0.9414,  0.6758,  0.4160], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 741])
attention_mask shape: torch.Size([4, 741])
reward: tensor([ 0.1377, -1.2656,  0.0356, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1229])
attention_mask shape: torch.Size([4, 1229])
reward: tensor([-0.0623, -0.2090,  1.0391, -1.7422], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1342])
attention_mask shape: torch.Size([4, 1342])
reward: tensor([ 1.0234, -0.6992, -0.7969, -1.5547], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1126])
attention_mask shape: torch.Size([4, 1126])
reward: tensor([-0.6484, -1.2422, -1.2422, -0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1505])
attention_mask shape: torch.Size([4, 1505])
reward: tensor([-0.7500,  0.7930,  0.4355, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1170])
attention_mask shape: torch.Size([4, 1170])
reward: tensor([-1.0703,  1.2031, -1.0391, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1019])
attention_mask shape: torch.Size([4, 1019])
reward: tensor([ 1.2031, -1.3438, -1.2734, -0.0557], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 552])
attention_mask shape: torch.Size([4, 552])
reward: tensor([-0.3027,  0.5078, -1.8047,  0.1001], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1207])
attention_mask shape: torch.Size([4, 1207])
reward: tensor([-0.7656, -0.2812, -0.6875, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1032])
attention_mask shape: torch.Size([4, 1032])
reward: tensor([-1.0234, -0.9141, -0.2021, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1758])
attention_mask shape: torch.Size([4, 1758])
reward: tensor([-0.8398,  0.0122, -1.1875,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1215])
attention_mask shape: torch.Size([4, 1215])
reward: tensor([-0.6055, -0.5273, -0.2373, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1723])
attention_mask shape: torch.Size([4, 1723])
reward: tensor([-1.3984,  0.1035, -0.2559, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 937])
attention_mask shape: torch.Size([4, 937])
reward: tensor([ 0.6055, -2.1094, -1.1797, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1088])
attention_mask shape: torch.Size([4, 1088])
reward: tensor([-0.1113, -0.8789, -1.9219, -1.9141], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 385])
attention_mask shape: torch.Size([4, 385])
reward: tensor([ 0.1670, -1.8516, -1.2734, -0.2422], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 592])
attention_mask shape: torch.Size([4, 592])
reward: tensor([ 0.9961,  0.5938, -0.2969,  0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1304])
attention_mask shape: torch.Size([4, 1304])
reward: tensor([-0.4043, -1.7344, -0.2109,  0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1055])
attention_mask shape: torch.Size([4, 1055])
reward: tensor([-0.9141, -1.2734, -0.6992, -0.6250], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1323])
attention_mask shape: torch.Size([4, 1323])
reward: tensor([ 0.5859, -1.1719, -0.7734, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 638])
attention_mask shape: torch.Size([4, 638])
reward: tensor([-1.0781,  0.6914, -0.1709, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1075])
attention_mask shape: torch.Size([4, 1075])
reward: tensor([ 0.3379, -1.3750, -0.9492, -1.1641], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 635])
attention_mask shape: torch.Size([4, 635])
reward: tensor([-0.6250, -0.7852,  0.4238, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 937])
attention_mask shape: torch.Size([4, 937])
reward: tensor([-0.9961,  0.1680,  1.1875, -1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 971])
attention_mask shape: torch.Size([4, 971])
reward: tensor([-0.6328, -0.2930, -0.5156,  0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 526])
attention_mask shape: torch.Size([4, 526])
reward: tensor([-0.6562, -1.5000, -1.5234, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 628])
attention_mask shape: torch.Size([4, 628])
reward: tensor([-0.7070, -1.9922, -0.5430, -0.1514], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1244])
attention_mask shape: torch.Size([4, 1244])
reward: tensor([ 0.3926, -0.6367, -1.0391,  0.0200], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1151])
attention_mask shape: torch.Size([4, 1151])
reward: tensor([-0.8594,  0.3066,  0.4980, -0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 669])
attention_mask shape: torch.Size([4, 669])
reward: tensor([-0.4004,  0.0356, -0.9141, -0.1426], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 570])
attention_mask shape: torch.Size([4, 570])
reward: tensor([ 1.2188, -0.7930, -1.5469, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 954])
attention_mask shape: torch.Size([4, 954])
reward: tensor([-1.2656, -1.2266,  0.4004, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 585])
attention_mask shape: torch.Size([4, 585])
reward: tensor([-1.0078, -1.2031, -0.7070, -0.1445], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 798])
attention_mask shape: torch.Size([4, 798])
reward: tensor([-1.1250, -0.9883, -0.4180, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 812])
attention_mask shape: torch.Size([4, 812])
reward: tensor([-1.4219,  0.8672, -0.9414, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 667])
attention_mask shape: torch.Size([4, 667])
reward: tensor([-0.3164, -2.0469, -0.9609,  1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 790])
attention_mask shape: torch.Size([4, 790])
reward: tensor([-0.9141, -1.6719,  0.4219, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 579])
attention_mask shape: torch.Size([4, 579])
reward: tensor([ 0.2178, -0.7617, -0.5391, -0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1185])
attention_mask shape: torch.Size([4, 1185])
reward: tensor([-0.5352, -1.9844, -1.8828,  1.5859], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1291])
attention_mask shape: torch.Size([4, 1291])
reward: tensor([-0.7227, -0.5039,  0.1514,  1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1209])
attention_mask shape: torch.Size([4, 1209])
reward: tensor([-1.3594, -0.4980, -1.1016, -0.1270], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 925])
attention_mask shape: torch.Size([4, 925])
reward: tensor([ 0.3105, -0.5039, -1.1094, -1.6953], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 614])
attention_mask shape: torch.Size([4, 614])
reward: tensor([-1.3281,  0.6836,  0.1729, -1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1137])
attention_mask shape: torch.Size([4, 1137])
reward: tensor([ 0.3867, -1.2266, -0.1533, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 748])
attention_mask shape: torch.Size([4, 748])
reward: tensor([-0.1396,  0.7930, -0.0864, -0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 917])
attention_mask shape: torch.Size([4, 917])
reward: tensor([ 0.1748, -0.5234, -0.8359, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 719])
attention_mask shape: torch.Size([4, 719])
reward: tensor([-1.6953, -0.6758, -2.1406, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1399])
attention_mask shape: torch.Size([4, 1399])
reward: tensor([ 0.6445, -0.0820,  0.7148, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1136])
attention_mask shape: torch.Size([4, 1136])
reward: tensor([ 0.4141, -1.1797, -1.5312, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 817])
attention_mask shape: torch.Size([4, 817])
reward: tensor([ 0.5938, -0.6680, -0.8203, -1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1136])
attention_mask shape: torch.Size([4, 1136])
reward: tensor([ 1.6406, -1.7188,  0.5430, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 749])
attention_mask shape: torch.Size([4, 749])
reward: tensor([ 0.2852, -0.9336, -0.5781, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1311])
attention_mask shape: torch.Size([4, 1311])
reward: tensor([-0.5547, -0.3652, -0.8633, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1455])
attention_mask shape: torch.Size([4, 1455])
reward: tensor([ 1.4062,  0.1660,  1.3438, -0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 592])
attention_mask shape: torch.Size([4, 592])
reward: tensor([-1.3281, -1.2422, -0.0933,  0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1176])
attention_mask shape: torch.Size([4, 1176])
reward: tensor([-0.7617, -0.3516, -1.6172,  0.1465], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 786])
attention_mask shape: torch.Size([4, 786])
reward: tensor([-2.1094, -0.8906, -0.8047,  0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-0.2227, -0.9062, -0.1377,  0.3828], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1444])
attention_mask shape: torch.Size([4, 1444])
reward: tensor([-0.1777,  0.1089, -0.7695, -0.5234], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1449])
attention_mask shape: torch.Size([4, 1449])
reward: tensor([-0.0378, -2.1406,  0.8711,  0.0845], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1592])
attention_mask shape: torch.Size([4, 1592])
reward: tensor([-0.4141,  1.2656, -0.6719, -1.4844], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1346])
attention_mask shape: torch.Size([4, 1346])
reward: tensor([-0.3828, -1.1406, -0.2197, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 615])
attention_mask shape: torch.Size([4, 615])
reward: tensor([-0.3066, -1.6250, -0.7383, -1.6562], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 742])
attention_mask shape: torch.Size([4, 742])
reward: tensor([-0.6758, -1.3672, -0.7617,  0.1045], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1229])
attention_mask shape: torch.Size([4, 1229])
reward: tensor([-0.5820, -0.4082, -1.5859,  0.1299], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 240])
attention_mask shape: torch.Size([4, 240])
reward: tensor([-1.2344, -0.9609, -1.9219, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1532])
attention_mask shape: torch.Size([4, 1532])
reward: tensor([-1.5000, -0.4141,  0.5586, -1.6172], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1328])
attention_mask shape: torch.Size([4, 1328])
reward: tensor([-0.2930, -0.7773, -0.4316, -0.1797], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1069])
attention_mask shape: torch.Size([4, 1069])
reward: tensor([-0.7969, -0.1133,  0.0967, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1558])
attention_mask shape: torch.Size([4, 1558])
reward: tensor([ 0.2041, -0.3516, -1.8047,  1.4453], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1550])
attention_mask shape: torch.Size([4, 1550])
reward: tensor([-0.1045,  0.6055, -0.3203, -0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 625])
attention_mask shape: torch.Size([4, 625])
reward: tensor([-0.6445,  0.2637,  0.7109, -0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 793])
attention_mask shape: torch.Size([4, 793])
reward: tensor([-0.8984, -0.1670,  1.4297,  0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1516])
attention_mask shape: torch.Size([4, 1516])
reward: tensor([-2.2188,  0.3281,  0.8789, -1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1259])
attention_mask shape: torch.Size([4, 1259])
reward: tensor([-0.4746, -0.5352, -0.3203, -0.2617], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-0.9766, -0.7734, -0.6328, -0.1514], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1215])
attention_mask shape: torch.Size([4, 1215])
reward: tensor([-0.4844,  0.1680, -0.4707, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 594])
attention_mask shape: torch.Size([4, 594])
reward: tensor([-1.6641, -0.3379, -0.3066, -0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1120])
attention_mask shape: torch.Size([4, 1120])
reward: tensor([-1.1875, -0.8594,  1.6641, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 643])
attention_mask shape: torch.Size([4, 643])
reward: tensor([-0.6367, -0.6484, -2.0938,  0.0289], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1277])
attention_mask shape: torch.Size([4, 1277])
reward: tensor([-0.0266,  1.1797, -0.7148, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1166])
attention_mask shape: torch.Size([4, 1166])
reward: tensor([-0.0334,  0.3867, -0.4941, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1057])
attention_mask shape: torch.Size([4, 1057])
reward: tensor([-1.0078, -0.4453, -0.1089, -0.4629], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 374])
attention_mask shape: torch.Size([4, 374])
reward: tensor([-0.0178, -1.7422, -1.7344, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 317])
attention_mask shape: torch.Size([4, 317])
reward: tensor([-0.3965, -0.2969, -1.1641, -0.1270], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 476])
attention_mask shape: torch.Size([4, 476])
reward: tensor([ 0.2637, -2.1719, -0.7227, -0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 628])
attention_mask shape: torch.Size([4, 628])
reward: tensor([-0.3066, -0.8633, -1.3438,  0.0767], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 465])
attention_mask shape: torch.Size([4, 465])
reward: tensor([0.1260, 0.6172, 0.1484, 0.0488], device='cuda:0', dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1015])
attention_mask shape: torch.Size([4, 1015])
reward: tensor([-1.1094, -0.9766,  0.0776, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1117])
attention_mask shape: torch.Size([4, 1117])
reward: tensor([ 0.0654, -0.6055, -1.6641,  0.2363], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1157])
attention_mask shape: torch.Size([4, 1157])
reward: tensor([ 0.9141, -0.3105, -1.3359,  0.0133], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1308])
attention_mask shape: torch.Size([4, 1308])
reward: tensor([-2.0156, -1.5469,  1.4062,  0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 333])
attention_mask shape: torch.Size([4, 333])
reward: tensor([-1.7422, -0.3828, -1.8672, -0.4883], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1128])
attention_mask shape: torch.Size([4, 1128])
reward: tensor([-0.6719, -0.2637, -1.5078, -1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1820])
attention_mask shape: torch.Size([4, 1820])
reward: tensor([-1.3438,  1.7656,  0.2559, -0.0601], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 195])
attention_mask shape: torch.Size([4, 195])
reward: tensor([-0.7344, -1.1641, -0.7422, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1379])
attention_mask shape: torch.Size([4, 1379])
reward: tensor([-0.6133,  0.7539,  0.0732, -0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1314])
attention_mask shape: torch.Size([4, 1314])
reward: tensor([ 1.0078, -0.1621, -0.1157, -0.1001], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 765])
attention_mask shape: torch.Size([4, 765])
reward: tensor([-0.9688, -1.4688,  1.2734, -0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1011])
attention_mask shape: torch.Size([4, 1011])
reward: tensor([-2.1719,  0.7773,  0.7734, -1.7344], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1290])
attention_mask shape: torch.Size([4, 1290])
reward: tensor([-0.6367,  0.0200, -0.2754,  0.0011], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 674])
attention_mask shape: torch.Size([4, 674])
reward: tensor([ 0.2275, -0.9414, -0.7344, -1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 410])
attention_mask shape: torch.Size([4, 410])
reward: tensor([ 0.7188, -0.5586, -2.0156, -0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1423])
attention_mask shape: torch.Size([4, 1423])
reward: tensor([-0.1270, -0.2285, -0.8906, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1602])
attention_mask shape: torch.Size([4, 1602])
reward: tensor([-0.2617,  0.1621, -0.3730,  1.5234], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 646])
attention_mask shape: torch.Size([4, 646])
reward: tensor([-0.8047, -0.5508, -1.3047, -1.7500], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 533])
attention_mask shape: torch.Size([4, 533])
reward: tensor([-1.1797, -0.3340, -0.8438, -0.1357], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1134])
attention_mask shape: torch.Size([4, 1134])
reward: tensor([ 1.5547, -0.6172, -0.1309, -0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1020])
attention_mask shape: torch.Size([4, 1020])
reward: tensor([ 1.2656, -0.7188, -1.0547, -0.8125], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 487])
attention_mask shape: torch.Size([4, 487])
reward: tensor([-1.0312, -1.4062, -0.7656,  0.3438], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1155])
attention_mask shape: torch.Size([4, 1155])
reward: tensor([-1.0469,  0.2637, -0.7461,  0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 689])
attention_mask shape: torch.Size([4, 689])
reward: tensor([-0.5625, -1.6875, -0.7383, -0.0200], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1088])
attention_mask shape: torch.Size([4, 1088])
reward: tensor([ 0.6797, -2.1562, -0.8477, -1.6953], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1093])
attention_mask shape: torch.Size([4, 1093])
reward: tensor([ 0.1396, -1.5391, -0.1133, -0.0045], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1170])
attention_mask shape: torch.Size([4, 1170])
reward: tensor([-1.7188, -1.0781,  2.2812, -1.8125], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1117])
attention_mask shape: torch.Size([4, 1117])
reward: tensor([ 2.2031, -1.0156, -1.4297,  0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1118])
attention_mask shape: torch.Size([4, 1118])
reward: tensor([-0.1157, -0.8906,  1.8516,  0.1045], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1529])
attention_mask shape: torch.Size([4, 1529])
reward: tensor([ 1.2500, -0.4258, -0.8906,  0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 570])
attention_mask shape: torch.Size([4, 570])
reward: tensor([-0.4395, -1.5234, -0.0178, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1318])
attention_mask shape: torch.Size([4, 1318])
reward: tensor([-0.7070, -0.2471, -0.4531, -1.8594], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1053])
attention_mask shape: torch.Size([4, 1053])
reward: tensor([ 0.7383, -0.7656, -1.8203,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 816])
attention_mask shape: torch.Size([4, 816])
reward: tensor([-0.9062, -1.8125, -0.2285, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 969])
attention_mask shape: torch.Size([4, 969])
reward: tensor([-0.2793,  1.6094, -1.2500, -0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1325])
attention_mask shape: torch.Size([4, 1325])
reward: tensor([-2.0156, -0.8906,  0.1133, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1016])
attention_mask shape: torch.Size([4, 1016])
reward: tensor([-0.5469, -0.1396, -0.4629, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1325])
attention_mask shape: torch.Size([4, 1325])
reward: tensor([ 0.6836,  0.3281, -1.1641,  0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1580])
attention_mask shape: torch.Size([4, 1580])
reward: tensor([-0.6875, -0.8594,  1.6562, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 856])
attention_mask shape: torch.Size([4, 856])
reward: tensor([-1.2109, -0.0801, -1.3750,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 892])
attention_mask shape: torch.Size([4, 892])
reward: tensor([-1.6172,  0.2041,  0.8008, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1128])
attention_mask shape: torch.Size([4, 1128])
reward: tensor([-0.2178, -1.5000, -0.4258, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1076])
attention_mask shape: torch.Size([4, 1076])
reward: tensor([-1.5312, -1.2344, -1.2188, -0.6172], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1179])
attention_mask shape: torch.Size([4, 1179])
reward: tensor([-0.2441, -0.6016, -0.6523,  0.0479], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 201])
attention_mask shape: torch.Size([4, 201])
reward: tensor([-1.2266, -1.5391, -0.7227, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1202])
attention_mask shape: torch.Size([4, 1202])
reward: tensor([-1.6797,  0.9688, -0.2021, -1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 360])
attention_mask shape: torch.Size([4, 360])
reward: tensor([-0.5742, -0.5938, -0.4805, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 648])
attention_mask shape: torch.Size([4, 648])
reward: tensor([-0.9609,  0.6992,  1.1172, -0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1602])
attention_mask shape: torch.Size([4, 1602])
reward: tensor([0.7734, 0.1914, 1.4844, 0.1348], device='cuda:0', dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
[2024-10-22 04:46:24,984] [INFO] [launch.py:351:main] Process 612438 exits successfully.
sequences shape: torch.Size([4, 1045])
attention_mask shape: torch.Size([4, 1045])
reward: tensor([-0.8203, -0.9766, -0.2891, -0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-22 04:46:27,987] [INFO] [launch.py:351:main] Process 612437 exits successfully.
[2024-10-22 04:48:50,133] [INFO] [launch.py:351:main] Process 612440 exits successfully.
[2024-10-22 04:49:01,145] [INFO] [launch.py:351:main] Process 612439 exits successfully.
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# [K(base) root@autodl-container-ec234bbd2e-925c6d34:~# bash run_eval_reward_openrlhf.sh
[?2004l+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-6th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-6th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_6 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-23 10:52:29,335] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-23 10:52:31,094] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-23 10:52:31,095] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-6th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_6 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-23 10:52:32,601] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-23 10:52:34,533] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-23 10:52:34,533] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-23 10:52:34,533] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-23 10:52:34,533] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-23 10:52:34,533] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-23 10:52:34,534] [INFO] [launch.py:256:main] process 655609 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-6th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_6', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-23 10:52:34,534] [INFO] [launch.py:256:main] process 655610 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-6th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_6', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-23 10:52:34,534] [INFO] [launch.py:256:main] process 655611 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-6th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_6', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-23 10:52:34,535] [INFO] [launch.py:256:main] process 655612 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-6th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_6', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-23 10:52:36,528] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-23 10:52:36,542] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-23 10:52:36,600] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-23 10:52:36,600] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-23 10:52:39,182] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-23 10:52:39,182] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-23 10:52:39,631] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-10-23 10:52:39,746] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-23 10:52:39,750] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  8.19it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  8.44it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  8.55it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.89it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.71it/s]
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.16it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.14it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.57it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.16it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.14it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.59it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.18it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.16it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.60it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.40it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.31it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.39it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.30it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.76it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.70it/s]
[2024-10-23 10:53:01,452] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-23 10:53:01,968] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-23 10:53:01,968] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-23 10:53:01,968] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 10:53:01,969] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 10:53:02,043] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 10:53:03,982] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-23 10:53:03,983] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-23 10:53:03,984] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 10:53:03,985] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 10:53:03,987] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 10:53:04,112] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-23 10:53:04,112] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-23 10:53:04,113] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.1 GB, percent = 3.0%
[2024-10-23 10:53:04,248] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-23 10:53:04,249] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-23 10:53:04,249] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.1 GB, percent = 3.0%
[2024-10-23 10:53:04,250] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-23 10:53:04,250] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-23 10:53:04,250] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-23 10:53:04,250] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-23 10:53:04,250] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-23 10:53:04,250] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-23 10:53:04,250] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-23 10:53:04,250] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-23 10:53:04,250] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-23 10:53:04,250] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-23 10:53:04,250] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f56397e6590>
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-23 10:53:04,251] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-23 10:53:04,252] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-23 10:53:04,252] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-23 10:53:04,252] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-23 10:53:04,252] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-23 10:53:04,252] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 10:53:10,210] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-23 10:53:10,211] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-23 10:53:10,337] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-23 10:53:10,338] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-23 10:53:10,338] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.11 GB, percent = 3.0%
[2024-10-23 10:53:10,464] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-23 10:53:10,464] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-23 10:53:10,465] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.11 GB, percent = 3.0%
[2024-10-23 10:53:10,466] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-23 10:53:10,466] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f563812a860>
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-23 10:53:10,467] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-23 10:53:10,468] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-23 10:53:10,468] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 609/100000 [00:00<00:16, 6085.88it/s]Preprocessing data:   2%|█▍                                                                                          | 1575/100000 [00:00<00:12, 8184.65it/s]Preprocessing data:   3%|██▎                                                                                         | 2547/100000 [00:00<00:10, 8883.20it/s]Preprocessing data:   4%|███▏                                                                                        | 3519/100000 [00:00<00:10, 9209.03it/s]Preprocessing data:   4%|████                                                                                        | 4477/100000 [00:00<00:10, 9339.64it/s]Preprocessing data:   5%|█████                                                                                       | 5454/100000 [00:00<00:09, 9485.14it/s]Preprocessing data:   6%|█████▉                                                                                      | 6432/100000 [00:00<00:09, 9581.12it/s]Preprocessing data:   7%|██████▊                                                                                     | 7410/100000 [00:00<00:09, 9644.03it/s]Preprocessing data:   8%|███████▋                                                                                    | 8389/100000 [00:00<00:09, 9686.51it/s]Preprocessing data:   9%|████████▌                                                                                   | 9365/100000 [00:01<00:09, 9708.32it/s]Preprocessing data:  10%|█████████▍                                                                                 | 10364/100000 [00:01<00:09, 9792.54it/s]Preprocessing data:  11%|██████████▎                                                                                | 11397/100000 [00:01<00:08, 9955.64it/s]Preprocessing data:  12%|███████████▏                                                                              | 12420/100000 [00:01<00:08, 10036.49it/s]Preprocessing data:  13%|████████████                                                                              | 13454/100000 [00:01<00:08, 10126.77it/s]Preprocessing data:  14%|█████████████                                                                             | 14491/100000 [00:01<00:08, 10197.09it/s]Preprocessing data:  16%|█████████████▉                                                                            | 15525/100000 [00:01<00:08, 10239.19it/s]Preprocessing data:  17%|██████████████▉                                                                           | 16559/100000 [00:01<00:08, 10267.95it/s]Preprocessing data:  18%|███████████████▊                                                                          | 17593/100000 [00:01<00:08, 10286.87it/s]Preprocessing data:  19%|████████████████▊                                                                         | 18627/100000 [00:01<00:07, 10302.64it/s]Preprocessing data:  20%|█████████████████▋                                                                        | 19663/100000 [00:02<00:07, 10317.17it/s]Preprocessing data:  21%|██████████████████▋                                                                       | 20718/100000 [00:02<00:07, 10385.21it/s]Preprocessing data:  22%|███████████████████▌                                                                      | 21766/100000 [00:02<00:07, 10412.06it/s]Preprocessing data:  23%|████████████████████▌                                                                     | 22808/100000 [00:02<00:07, 10369.47it/s]Preprocessing data:  24%|█████████████████████▍                                                                    | 23845/100000 [00:02<00:07, 10267.64it/s]Preprocessing data:  25%|██████████████████████▍                                                                   | 24872/100000 [00:02<00:07, 10238.85it/s]Preprocessing data:  26%|███████████████████████▎                                                                  | 25897/100000 [00:02<00:07, 10197.62it/s]Preprocessing data:  27%|████████████████████████▏                                                                 | 26917/100000 [00:02<00:07, 10177.56it/s]Preprocessing data:  28%|█████████████████████████▏                                                                | 27935/100000 [00:02<00:07, 10159.83it/s]Preprocessing data:  29%|██████████████████████████                                                                | 28952/100000 [00:02<00:07, 10137.14it/s]Preprocessing data:  30%|██████████████████████████▉                                                               | 29966/100000 [00:03<00:06, 10109.64it/s]Preprocessing data:  31%|███████████████████████████▉                                                              | 30977/100000 [00:03<00:06, 10103.07it/s]Preprocessing data:  32%|████████████████████████████▊                                                             | 31988/100000 [00:03<00:06, 10104.17it/s]Preprocessing data:  33%|█████████████████████████████▋                                                            | 32999/100000 [00:03<00:06, 10101.38it/s]Preprocessing data:  34%|██████████████████████████████▌                                                           | 34010/100000 [00:03<00:06, 10024.84it/s]Preprocessing data:  35%|███████████████████████████████▌                                                          | 35018/100000 [00:03<00:06, 10040.33it/s]Preprocessing data:  36%|████████████████████████████████▍                                                         | 36023/100000 [00:03<00:06, 10030.92it/s]Preprocessing data:  37%|█████████████████████████████████▎                                                        | 37028/100000 [00:03<00:06, 10034.06it/s]Preprocessing data:  38%|██████████████████████████████████▏                                                       | 38035/100000 [00:03<00:06, 10044.25it/s]Preprocessing data:  39%|███████████████████████████████████▏                                                      | 39042/100000 [00:03<00:06, 10049.59it/s]Preprocessing data:  40%|████████████████████████████████████                                                      | 40054/100000 [00:04<00:05, 10069.25it/s]Preprocessing data:  41%|████████████████████████████████████▉                                                     | 41061/100000 [00:04<00:05, 10067.28it/s]Preprocessing data:  42%|█████████████████████████████████████▊                                                    | 42068/100000 [00:04<00:05, 10045.92it/s]Preprocessing data:  43%|██████████████████████████████████████▊                                                   | 43080/100000 [00:04<00:05, 10066.39it/s]Preprocessing data:  44%|███████████████████████████████████████▋                                                  | 44091/100000 [00:04<00:05, 10076.31it/s]Preprocessing data:  45%|████████████████████████████████████████▌                                                 | 45117/100000 [00:04<00:05, 10128.90it/s]Preprocessing data:  46%|█████████████████████████████████████████▌                                                | 46152/100000 [00:04<00:05, 10193.40it/s]Preprocessing data:  47%|██████████████████████████████████████████▍                                               | 47185/100000 [00:04<00:05, 10232.50it/s]Preprocessing data:  48%|███████████████████████████████████████████▍                                              | 48221/100000 [00:04<00:05, 10270.35it/s]Preprocessing data:  49%|████████████████████████████████████████████▎                                             | 49257/100000 [00:04<00:04, 10294.67it/s]Preprocessing data:  50%|█████████████████████████████████████████████▎                                            | 50297/100000 [00:05<00:04, 10324.89it/s]Preprocessing data:  51%|██████████████████████████████████████████████▏                                           | 51330/100000 [00:05<00:04, 10269.76it/s]Preprocessing data:  52%|███████████████████████████████████████████████                                           | 52358/100000 [00:05<00:04, 10123.33it/s]Preprocessing data:  53%|████████████████████████████████████████████████                                          | 53385/100000 [00:05<00:04, 10165.40it/s]Preprocessing data:  54%|████████████████████████████████████████████████▉                                         | 54402/100000 [00:05<00:04, 10116.37it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▉                                        | 55424/100000 [00:05<00:04, 10146.43it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▊                                       | 56454/100000 [00:05<00:04, 10190.83it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▋                                      | 57489/100000 [00:05<00:04, 10237.88it/s]Preprocessing data:  59%|████████████████████████████████████████████████████▋                                     | 58526/100000 [00:05<00:04, 10275.48it/s]Preprocessing data:  60%|█████████████████████████████████████████████████████▌                                    | 59562/100000 [00:05<00:03, 10297.69it/s]Preprocessing data:  61%|██████████████████████████████████████████████████████▌                                   | 60594/100000 [00:06<00:03, 10303.64it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▍                                  | 61625/100000 [00:06<00:03, 10301.02it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▍                                 | 62656/100000 [00:06<00:03, 10137.26it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▎                                | 63678/100000 [00:06<00:03, 10159.30it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▏                               | 64695/100000 [00:06<00:03, 10102.80it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▏                              | 65706/100000 [00:06<00:03, 10078.22it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████                              | 66721/100000 [00:06<00:03, 10098.66it/s]Preprocessing data:  68%|████████████████████████████████████████████████████████████▉                             | 67741/100000 [00:06<00:03, 10127.76it/s]Preprocessing data:  69%|█████████████████████████████████████████████████████████████▉                            | 68771/100000 [00:06<00:03, 10176.04it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▊                           | 69793/100000 [00:06<00:02, 10188.76it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▋                          | 70820/100000 [00:07<00:02, 10212.83it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▋                         | 71842/100000 [00:07<00:02, 10209.14it/s]Preprocessing data:  73%|██████████████████████████████████████████████████████████████████▎                        | 72863/100000 [00:07<00:02, 9790.35it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▏                       | 73846/100000 [00:07<00:02, 8903.94it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████                       | 74753/100000 [00:07<00:03, 8351.35it/s]Preprocessing data:  76%|████████████████████████████████████████████████████████████████████▊                      | 75604/100000 [00:07<00:03, 8102.61it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████▌                     | 76425/100000 [00:07<00:02, 7943.57it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▎                    | 77226/100000 [00:07<00:02, 7862.79it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▉                    | 78017/100000 [00:07<00:02, 7805.06it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▋                   | 78800/100000 [00:08<00:02, 7754.16it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▍                  | 79577/100000 [00:08<00:02, 7723.13it/s]Preprocessing data:  80%|█████████████████████████████████████████████████████████████████████████                  | 80351/100000 [00:08<00:02, 7555.73it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▊                 | 81174/100000 [00:08<00:02, 7749.34it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▋                | 82069/100000 [00:08<00:02, 8096.14it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▍               | 82882/100000 [00:08<00:02, 8104.19it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▏              | 83694/100000 [00:08<00:02, 7930.12it/s]Preprocessing data:  85%|████████████████████████████████████████████████████████████████████████████▉              | 84545/100000 [00:08<00:01, 8099.20it/s]Preprocessing data:  86%|█████████████████████████████████████████████████████████████████████████████▊             | 85565/100000 [00:08<00:01, 8716.52it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▋            | 86439/100000 [00:08<00:01, 8506.69it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▍           | 87293/100000 [00:09<00:01, 8371.56it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████▎          | 88262/100000 [00:09<00:01, 8753.91it/s]Preprocessing data:  89%|█████████████████████████████████████████████████████████████████████████████████▏         | 89241/100000 [00:09<00:01, 9056.94it/s]Preprocessing data:  90%|██████████████████████████████████████████████████████████████████████████████████         | 90177/100000 [00:09<00:01, 9143.62it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▉        | 91094/100000 [00:09<00:01, 8329.16it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▋       | 91942/100000 [00:09<00:01, 7877.90it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▍      | 92744/100000 [00:09<00:00, 7599.84it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████      | 93514/100000 [00:09<00:00, 7450.95it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▊     | 94353/100000 [00:09<00:00, 7708.88it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▊    | 95333/100000 [00:10<00:00, 8296.86it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▌   | 96255/100000 [00:10<00:00, 8559.16it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▍  | 97118/100000 [00:10<00:00, 8503.58it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▏ | 98003/100000 [00:10<00:00, 8602.61it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████ | 98973/100000 [00:10<00:00, 8923.83it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████▉| 99935/100000 [00:10<00:00, 9128.91it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9465.62it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.2197, -0.7773, -1.1719, -0.5234], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1630])
attention_mask shape: torch.Size([4, 1630])
reward: tensor([ 0.8008, -0.3379,  0.4336, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1104])
attention_mask shape: torch.Size([4, 1104])
reward: tensor([-1.6719, -0.6680, -1.3359, -0.5508], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 337])
attention_mask shape: torch.Size([4, 337])
reward: tensor([-1.2344, -1.7422, -0.6250,  0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1733])
attention_mask shape: torch.Size([4, 1733])
reward: tensor([ 0.4707, -0.3242, -1.1250, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1374])
attention_mask shape: torch.Size([4, 1374])
reward: tensor([-0.6797, -0.8438, -1.5938,  0.4648], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1954])
attention_mask shape: torch.Size([4, 1954])
reward: tensor([ 0.8398, -1.9922,  0.7695, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([ 0.7773, -1.8672, -0.6992,  1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1135])
attention_mask shape: torch.Size([4, 1135])
reward: tensor([-0.6914, -0.7852, -0.4492, -0.2246], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1097])
attention_mask shape: torch.Size([4, 1097])
reward: tensor([ 0.0056,  0.5391, -0.0111, -0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1619])
attention_mask shape: torch.Size([4, 1619])
reward: tensor([-1.7812,  0.1445, -1.9922, -0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1277])
attention_mask shape: torch.Size([4, 1277])
reward: tensor([ 0.6133, -0.4531, -0.2812, -0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1099])
attention_mask shape: torch.Size([4, 1099])
reward: tensor([-1.3984, -0.5156, -0.0645, -0.0488], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1742])
attention_mask shape: torch.Size([4, 1742])
reward: tensor([-0.5508, -0.1221,  1.8047,  0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 531])
attention_mask shape: torch.Size([4, 531])
reward: tensor([-0.9961, -0.5195,  0.8789, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1572])
attention_mask shape: torch.Size([4, 1572])
reward: tensor([ 0.4043,  1.2422, -0.3691, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1098])
attention_mask shape: torch.Size([4, 1098])
reward: tensor([-0.7188, -1.3984, -1.0312, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1813])
attention_mask shape: torch.Size([4, 1813])
reward: tensor([-0.8477, -0.9258, -0.5547, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1379])
attention_mask shape: torch.Size([4, 1379])
reward: tensor([-0.1484, -1.3438, -1.6797, -0.0266], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 856])
attention_mask shape: torch.Size([4, 856])
reward: tensor([ 2.0781,  1.1250, -0.9883, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 973])
attention_mask shape: torch.Size([4, 973])
reward: tensor([-0.9609, -0.9961, -0.3828, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1226])
attention_mask shape: torch.Size([4, 1226])
reward: tensor([0.2578, 0.6094, 0.0189, 0.2070], device='cuda:0', dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1066])
attention_mask shape: torch.Size([4, 1066])
reward: tensor([-1.1719,  1.3516, -0.1953, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1737])
attention_mask shape: torch.Size([4, 1737])
reward: tensor([-2.1719, -0.0601, -2.0000,  0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1594])
attention_mask shape: torch.Size([4, 1594])
reward: tensor([ 0.7539, -1.1484, -1.9531, -0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1350])
attention_mask shape: torch.Size([4, 1350])
reward: tensor([ 0.4180,  0.0879, -0.0378, -1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1522])
attention_mask shape: torch.Size([4, 1522])
reward: tensor([-0.6797,  0.3418,  0.2197,  0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 455])
attention_mask shape: torch.Size([4, 455])
reward: tensor([ 0.8672, -0.3066, -0.4531, -0.0422], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1018])
attention_mask shape: torch.Size([4, 1018])
reward: tensor([ 1.1406, -0.6719,  0.5820,  1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1651])
attention_mask shape: torch.Size([4, 1651])
reward: tensor([-0.5898,  0.5859, -1.1797, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1630])
attention_mask shape: torch.Size([4, 1630])
reward: tensor([-1.4141,  0.6055, -1.3438, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1415])
attention_mask shape: torch.Size([4, 1415])
reward: tensor([-1.6406, -0.4492, -0.3105, -0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1442])
attention_mask shape: torch.Size([4, 1442])
reward: tensor([-1.1406, -0.5234,  0.6445, -1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1338])
attention_mask shape: torch.Size([4, 1338])
reward: tensor([ 0.5664,  0.9961, -0.9883,  0.1064], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 825])
attention_mask shape: torch.Size([4, 825])
reward: tensor([ 0.2930, -0.4805, -0.8203, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.4043,  1.0078,  0.8281, -0.7617], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1003])
attention_mask shape: torch.Size([4, 1003])
reward: tensor([-0.1133, -1.6406, -0.2637, -1.6016], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1387])
attention_mask shape: torch.Size([4, 1387])
reward: tensor([-0.3242, -1.0781,  0.6133, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([-1.0547, -0.1729,  2.2031, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1865])
attention_mask shape: torch.Size([4, 1865])
reward: tensor([ 0.0688, -1.4609, -1.5312,  0.8672], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1214])
attention_mask shape: torch.Size([4, 1214])
reward: tensor([ 0.3203, -0.9766,  1.8281,  0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([ 1.0781,  0.8477, -1.0469, -1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 655])
attention_mask shape: torch.Size([4, 655])
reward: tensor([-0.6211, -1.1250, -1.7266, -0.1465], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 577])
attention_mask shape: torch.Size([4, 577])
reward: tensor([-1.4766, -0.2422, -1.2031, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1725])
attention_mask shape: torch.Size([4, 1725])
reward: tensor([-0.1465,  1.5391,  0.7656,  1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 842])
attention_mask shape: torch.Size([4, 842])
reward: tensor([-0.5898, -1.1094,  0.0510, -0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 684])
attention_mask shape: torch.Size([4, 684])
reward: tensor([ 1.1172, -0.2969,  0.3203, -0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.5898, -0.3867, -0.1533,  0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.0422, -0.2314, -1.5078, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([-0.8789, -1.5312,  0.4883,  0.3770], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1333])
attention_mask shape: torch.Size([4, 1333])
reward: tensor([-0.8359, -0.6055,  0.1211, -0.2676], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1294])
attention_mask shape: torch.Size([4, 1294])
reward: tensor([-0.7109,  1.6562, -0.9414,  1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1467])
attention_mask shape: torch.Size([4, 1467])
reward: tensor([ 0.0500, -1.2188,  0.0211, -0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1579])
attention_mask shape: torch.Size([4, 1579])
reward: tensor([-1.0312,  0.4570,  0.2559,  1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1667])
attention_mask shape: torch.Size([4, 1667])
reward: tensor([-0.7461, -0.4180,  1.6094, -0.6172], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-0.9609, -1.2734, -1.3438, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1723])
attention_mask shape: torch.Size([4, 1723])
reward: tensor([-0.9258, -0.5625,  0.1562, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 857])
attention_mask shape: torch.Size([4, 857])
reward: tensor([-1.5312, -0.3281,  0.4805, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([ 1.0156, -0.2246, -0.8164, -0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1404])
attention_mask shape: torch.Size([4, 1404])
reward: tensor([-1.8438,  1.1562, -1.4062, -0.1465], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 973])
attention_mask shape: torch.Size([4, 973])
reward: tensor([ 0.6133, -1.6953, -1.7031, -1.8984], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1715])
attention_mask shape: torch.Size([4, 1715])
reward: tensor([-1.1172,  0.3535, -0.8477, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1444])
attention_mask shape: torch.Size([4, 1444])
reward: tensor([-0.0133, -0.6836, -1.2109, -0.1221], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 894])
attention_mask shape: torch.Size([4, 894])
reward: tensor([-2.1406,  0.2812, -0.6016, -1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 958])
attention_mask shape: torch.Size([4, 958])
reward: tensor([ 0.9102, -1.5234, -0.2334, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 886])
attention_mask shape: torch.Size([4, 886])
reward: tensor([-1.1719,  1.3750, -1.1250,  0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1572])
attention_mask shape: torch.Size([4, 1572])
reward: tensor([-0.5078, -1.2109, -1.9375, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1314])
attention_mask shape: torch.Size([4, 1314])
reward: tensor([-0.6875, -0.6836, -0.5469,  1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1250])
attention_mask shape: torch.Size([4, 1250])
reward: tensor([ 1.1328, -0.0178, -0.6328,  0.3320], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7852, -1.4922, -2.2188,  0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1427])
attention_mask shape: torch.Size([4, 1427])
reward: tensor([-1.7188, -0.3105, -0.3867,  0.0311], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1057])
attention_mask shape: torch.Size([4, 1057])
reward: tensor([ 1.5547, -1.2031, -0.4570, -1.8750], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1330])
attention_mask shape: torch.Size([4, 1330])
reward: tensor([-0.1089,  0.0133, -0.5586, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1390])
attention_mask shape: torch.Size([4, 1390])
reward: tensor([-0.2090,  0.5742,  0.5312,  0.0679], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 553])
attention_mask shape: torch.Size([4, 553])
reward: tensor([-1.7891, -0.6992, -0.3457, -0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 840])
attention_mask shape: torch.Size([4, 840])
reward: tensor([ 0.6914, -0.9961, -0.2471, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1431])
attention_mask shape: torch.Size([4, 1431])
reward: tensor([ 1.3359, -0.0732, -0.7109, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2042])
attention_mask shape: torch.Size([4, 2042])
reward: tensor([ 0.6406, -0.6328, -0.4805, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1747])
attention_mask shape: torch.Size([4, 1747])
reward: tensor([-0.1396, -1.4766, -0.3379,  0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1430])
attention_mask shape: torch.Size([4, 1430])
reward: tensor([-1.1797, -1.7578, -1.9141, -1.2031], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1154])
attention_mask shape: torch.Size([4, 1154])
reward: tensor([-1.0781, -0.1953,  0.6836, -1.6641], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 754])
attention_mask shape: torch.Size([4, 754])
reward: tensor([ 1.1719,  0.0854, -1.2266, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1435])
attention_mask shape: torch.Size([4, 1435])
reward: tensor([ 0.2559,  0.7109,  0.5234, -0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 590])
attention_mask shape: torch.Size([4, 590])
reward: tensor([-0.3770, -0.7188, -0.2314,  0.0457], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1536])
attention_mask shape: torch.Size([4, 1536])
reward: tensor([-0.5508,  0.1211, -0.0178,  0.0623], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1333])
attention_mask shape: torch.Size([4, 1333])
reward: tensor([-1.0469, -0.7227, -0.2598, -0.0033], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1179])
attention_mask shape: torch.Size([4, 1179])
reward: tensor([-0.7188,  0.0776,  0.3965, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.8711, -1.4922, -0.3379, -0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.1953, -1.3438,  0.3652, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1203])
attention_mask shape: torch.Size([4, 1203])
reward: tensor([-1.5234, -0.1670,  1.1328, -1.2656], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 742])
attention_mask shape: torch.Size([4, 742])
reward: tensor([-0.6641,  1.0781, -0.2930,  0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1584])
attention_mask shape: torch.Size([4, 1584])
reward: tensor([ 1.6250, -0.6250, -1.0234, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1245])
attention_mask shape: torch.Size([4, 1245])
reward: tensor([-0.0033, -0.3340, -0.7461,  0.1953], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.3438, -1.1016, -0.0266,  0.3848], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1699])
attention_mask shape: torch.Size([4, 1699])
reward: tensor([-0.9961, -0.2002,  0.8477, -1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 803])
attention_mask shape: torch.Size([4, 803])
reward: tensor([-0.4004,  0.9258, -1.8125, -0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1322])
attention_mask shape: torch.Size([4, 1322])
reward: tensor([-0.6406, -0.0078,  0.9688, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-2.0312,  1.5703, -1.1484, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 920])
attention_mask shape: torch.Size([4, 920])
reward: tensor([-0.1865, -1.0156, -0.1177, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1779])
attention_mask shape: torch.Size([4, 1779])
reward: tensor([-0.8633, -0.0334,  0.2314, -1.7422], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1057])
attention_mask shape: torch.Size([4, 1057])
reward: tensor([-1.2812, -0.2266, -0.1396, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1145])
attention_mask shape: torch.Size([4, 1145])
reward: tensor([-1.2344, -0.4219,  0.7734, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1559])
attention_mask shape: torch.Size([4, 1559])
reward: tensor([ 0.6016, -0.5703, -0.2793, -1.5938], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1430])
attention_mask shape: torch.Size([4, 1430])
reward: tensor([ 0.8125, -1.4688,  0.1514,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([ 1.1719, -1.1406, -1.4219, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1503])
attention_mask shape: torch.Size([4, 1503])
reward: tensor([-0.5469, -0.1445, -0.2246,  0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1779])
attention_mask shape: torch.Size([4, 1779])
reward: tensor([-1.8672e+00,  1.1139e-03, -2.6758e-01, -1.3047e+00], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1347])
attention_mask shape: torch.Size([4, 1347])
reward: tensor([-1.5703, -0.7773, -0.6875, -1.4141], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([-1.3594, -1.3984, -1.2812, -0.0056], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7734, -0.8633,  1.9609,  1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1176])
attention_mask shape: torch.Size([4, 1176])
reward: tensor([ 0.2129,  0.1021, -0.0156,  1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 257])
attention_mask shape: torch.Size([4, 257])
reward: tensor([ 0.9961, -0.4707, -0.9062, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.2969, -0.5234, -1.4141, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 820])
attention_mask shape: torch.Size([4, 820])
reward: tensor([ 0.2637,  0.4707, -1.8828, -0.4883], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1077])
attention_mask shape: torch.Size([4, 1077])
reward: tensor([ 0.1797,  0.5820, -0.1641, -0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1273])
attention_mask shape: torch.Size([4, 1273])
reward: tensor([-0.8633, -0.2930, -0.6797,  0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1552])
attention_mask shape: torch.Size([4, 1552])
reward: tensor([-1.3828,  0.1885,  1.6875, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1084])
attention_mask shape: torch.Size([4, 1084])
reward: tensor([ 0.0923,  0.1099,  0.4609, -0.1797], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 976])
attention_mask shape: torch.Size([4, 976])
reward: tensor([ 0.0500, -1.8750, -1.6562, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1335])
attention_mask shape: torch.Size([4, 1335])
reward: tensor([-1.5469, -0.2559,  0.2949, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1097])
attention_mask shape: torch.Size([4, 1097])
reward: tensor([ 0.2500, -0.7227,  0.3066, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1591])
attention_mask shape: torch.Size([4, 1591])
reward: tensor([ 1.5859, -0.2520, -1.2812, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1358])
attention_mask shape: torch.Size([4, 1358])
reward: tensor([-0.1113,  0.8750,  1.7969, -0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1323])
attention_mask shape: torch.Size([4, 1323])
reward: tensor([-0.4941, -0.1709, -1.2188, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([ 1.1719, -1.2812, -0.8516, -0.0532], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 660])
attention_mask shape: torch.Size([4, 660])
reward: tensor([-1.1562,  0.4746,  0.1514,  0.2617], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1236])
attention_mask shape: torch.Size([4, 1236])
reward: tensor([-0.0623, -0.4043, -0.6133, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1357])
attention_mask shape: torch.Size([4, 1357])
reward: tensor([-0.6445,  0.4355,  0.2334, -0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1881])
attention_mask shape: torch.Size([4, 1881])
reward: tensor([-0.9141, -0.3652, -2.1562,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.3828, -1.6016,  0.5195, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.7656, -0.0601, -0.6406, -0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1212])
attention_mask shape: torch.Size([4, 1212])
reward: tensor([ 0.4395, -1.8594, -0.2598, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1226])
attention_mask shape: torch.Size([4, 1226])
reward: tensor([-0.0732, -0.1309, -0.7227, -1.6172], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1136])
attention_mask shape: torch.Size([4, 1136])
reward: tensor([-0.8984, -0.5898, -0.2109,  0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 985])
attention_mask shape: torch.Size([4, 985])
reward: tensor([-1.0078,  0.8438,  0.0444,  1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1584])
attention_mask shape: torch.Size([4, 1584])
reward: tensor([-0.6250,  0.1133,  0.6562,  1.7969], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([-1.2109, -1.3281, -0.6328, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1434])
attention_mask shape: torch.Size([4, 1434])
reward: tensor([ 0.9141, -0.6133,  0.2656,  0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 726])
attention_mask shape: torch.Size([4, 726])
reward: tensor([-0.4746,  0.2012,  0.6133, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1487])
attention_mask shape: torch.Size([4, 1487])
reward: tensor([-0.1465, -0.8633, -1.2891, -0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.6680, -0.7539, -0.7227, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1014])
attention_mask shape: torch.Size([4, 1014])
reward: tensor([-1.3125, -1.1094,  1.4609,  1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1633])
attention_mask shape: torch.Size([4, 1633])
reward: tensor([-0.9961,  0.0200, -1.5469,  0.2559], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 870])
attention_mask shape: torch.Size([4, 870])
reward: tensor([ 0.5117, -0.9688, -0.8594, -0.1201], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 671])
attention_mask shape: torch.Size([4, 671])
reward: tensor([-1.7422, -2.1406, -1.5938,  0.1582], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1658])
attention_mask shape: torch.Size([4, 1658])
reward: tensor([ 0.2754, -1.4062, -0.5703,  0.1455], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1119])
attention_mask shape: torch.Size([4, 1119])
reward: tensor([-1.0312,  1.0703,  1.0312, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 808])
attention_mask shape: torch.Size([4, 808])
reward: tensor([ 0.7383, -0.1445, -0.9336, -0.0623], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([ 1.2422, -0.1357, -1.5703,  0.0820], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([ 0.9023,  0.1611, -0.4883, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([ 0.5703, -1.3125,  1.2969, -0.6250], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-1.3125,  0.2598, -0.0820, -0.2910], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-1.9141,  1.2031, -0.4805, -1.4453], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 830])
attention_mask shape: torch.Size([4, 830])
reward: tensor([-0.2422, -1.1094, -1.0859,  0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1345])
attention_mask shape: torch.Size([4, 1345])
reward: tensor([ 0.5234, -1.6719, -0.4043,  0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 566])
attention_mask shape: torch.Size([4, 566])
reward: tensor([-0.6055, -0.0011, -0.6250,  0.0356], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 913])
attention_mask shape: torch.Size([4, 913])
reward: tensor([-1.4297, -0.4746, -1.6250, -0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1508])
attention_mask shape: torch.Size([4, 1508])
reward: tensor([-0.7617, -0.0356,  0.2539,  1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.7539, -1.7891, -1.0156, -0.0400], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1177])
attention_mask shape: torch.Size([4, 1177])
reward: tensor([-0.4141,  1.0000, -0.3828, -1.6953], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1194])
attention_mask shape: torch.Size([4, 1194])
reward: tensor([ 0.1157,  0.1758, -1.7656, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1036])
attention_mask shape: torch.Size([4, 1036])
reward: tensor([-0.0244, -0.8789, -0.5703,  0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1364])
attention_mask shape: torch.Size([4, 1364])
reward: tensor([ 0.3418, -0.0033, -1.0781, -0.0356], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1898])
attention_mask shape: torch.Size([4, 1898])
reward: tensor([-0.1465, -0.8711, -1.0078, -0.0178], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1229])
attention_mask shape: torch.Size([4, 1229])
reward: tensor([ 0.8789,  0.4531, -1.3125, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.3242, -0.7812,  0.7695, -1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.2031, -2.0312, -0.5625, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 914])
attention_mask shape: torch.Size([4, 914])
reward: tensor([-1.0234, -0.3203, -1.4297,  0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1353])
attention_mask shape: torch.Size([4, 1353])
reward: tensor([ 1.0625, -1.8594,  0.6133, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1083])
attention_mask shape: torch.Size([4, 1083])
reward: tensor([ 0.7461,  0.3496, -0.8633,  0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1299])
attention_mask shape: torch.Size([4, 1299])
reward: tensor([-0.3281,  0.5469, -0.1982, -0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1452])
attention_mask shape: torch.Size([4, 1452])
reward: tensor([ 0.2734, -0.0466, -0.2021, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 733])
attention_mask shape: torch.Size([4, 733])
reward: tensor([-0.8789, -0.6328, -0.8125,  2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1510])
attention_mask shape: torch.Size([4, 1510])
reward: tensor([-0.4805, -0.0933, -1.0312,  0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1358])
attention_mask shape: torch.Size([4, 1358])
reward: tensor([-0.9688,  0.0835,  0.2500,  0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1918])
attention_mask shape: torch.Size([4, 1918])
reward: tensor([-0.3457, -0.9492,  0.0579, -0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.4395, -0.9414, -0.2812, -1.3594], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1524])
attention_mask shape: torch.Size([4, 1524])
reward: tensor([-0.6133, -2.1094,  0.1465, -0.1338], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1607])
attention_mask shape: torch.Size([4, 1607])
reward: tensor([-0.5781,  1.5703, -0.8984, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.9492, -1.2734, -0.7773, -0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1384])
attention_mask shape: torch.Size([4, 1384])
reward: tensor([-1.4062, -0.6758, -0.6992, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1001])
attention_mask shape: torch.Size([4, 1001])
reward: tensor([ 0.2969, -0.9688, -0.9688, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1478])
attention_mask shape: torch.Size([4, 1478])
reward: tensor([-1.0781, -0.7344, -2.0469,  1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 589])
attention_mask shape: torch.Size([4, 589])
reward: tensor([ 1.1484,  1.5078, -1.5703, -1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.8320, -0.4492,  1.2656, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1554])
attention_mask shape: torch.Size([4, 1554])
reward: tensor([-0.3770, -0.3066, -0.3457,  0.4609], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1355])
attention_mask shape: torch.Size([4, 1355])
reward: tensor([-0.1777,  0.2305, -1.3125, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2007])
attention_mask shape: torch.Size([4, 2007])
reward: tensor([ 0.6133,  0.8516, -0.7031,  1.4844], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1664])
attention_mask shape: torch.Size([4, 1664])
reward: tensor([-0.0022,  1.4688,  0.1768, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1102])
attention_mask shape: torch.Size([4, 1102])
reward: tensor([1.8125, 0.5391, 0.2393, 0.1260], device='cuda:0', dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 920])
attention_mask shape: torch.Size([4, 920])
reward: tensor([-0.3457, -0.1245,  0.6211,  0.3828], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7031,  0.5469,  1.6328, -0.2471], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1573])
attention_mask shape: torch.Size([4, 1573])
reward: tensor([-0.3027, -1.3047, -0.2402, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-1.1094, -1.2188, -0.3164, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1489])
attention_mask shape: torch.Size([4, 1489])
reward: tensor([-2.0469, -0.0311,  0.9727, -0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 588])
attention_mask shape: torch.Size([4, 588])
reward: tensor([-1.4688, -0.6094, -0.7539,  0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1104])
attention_mask shape: torch.Size([4, 1104])
reward: tensor([-0.2617, -0.6055,  1.2031, -0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 663])
attention_mask shape: torch.Size([4, 663])
reward: tensor([-0.0757, -0.5938, -1.9609, -1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1666])
attention_mask shape: torch.Size([4, 1666])
reward: tensor([-0.2490,  2.0156, -1.7188, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.9688,  0.0942, -0.5547, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1707])
attention_mask shape: torch.Size([4, 1707])
reward: tensor([-1.3125, -0.6172, -0.4707, -0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([-0.9062, -2.0469,  0.9766, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 252])
attention_mask shape: torch.Size([4, 252])
reward: tensor([-0.3105, -0.2334, -0.9609, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 766])
attention_mask shape: torch.Size([4, 766])
reward: tensor([-0.7109, -2.1719, -1.1484,  0.0835], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1575])
attention_mask shape: torch.Size([4, 1575])
reward: tensor([-0.9609,  0.1787, -1.2109,  0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 512])
attention_mask shape: torch.Size([4, 512])
reward: tensor([-0.4219,  0.2715, -0.8359, -0.0156], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 753])
attention_mask shape: torch.Size([4, 753])
reward: tensor([-0.5781, -0.8789, -0.8516, -1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1879])
attention_mask shape: torch.Size([4, 1879])
reward: tensor([ 0.0011, -0.8633,  0.3340, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.0938, -1.9844, -1.3828, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.0781, -1.5547,  0.3906, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1283])
attention_mask shape: torch.Size([4, 1283])
reward: tensor([-1.7344, -0.8008, -1.3984, -0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1611])
attention_mask shape: torch.Size([4, 1611])
reward: tensor([-0.8711, -0.4492, -1.2266, -1.9297], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-2.0781,  0.7656, -0.8008,  0.0879], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 699])
attention_mask shape: torch.Size([4, 699])
reward: tensor([-0.5508, -0.8477, -0.8984,  0.1631], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.4258,  0.7500, -0.0156, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.9805,  0.6328, -1.7578, -1.3594], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([ 0.2109, -2.0000,  1.4609, -1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1047])
attention_mask shape: torch.Size([4, 1047])
reward: tensor([-1.4062,  1.6172,  0.5273,  0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1867])
attention_mask shape: torch.Size([4, 1867])
reward: tensor([ 0.1167,  0.0791, -1.4297,  1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 785])
attention_mask shape: torch.Size([4, 785])
reward: tensor([ 0.4336, -0.2930, -1.2109, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1175])
attention_mask shape: torch.Size([4, 1175])
reward: tensor([ 0.7539, -1.6719, -1.6016,  0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([-0.0311, -1.0312, -1.8203, -0.2129], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1668])
attention_mask shape: torch.Size([4, 1668])
reward: tensor([ 0.1143, -1.0234, -0.9258,  1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1068])
attention_mask shape: torch.Size([4, 1068])
reward: tensor([-0.9609,  0.1768, -1.2891, -1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 692])
attention_mask shape: torch.Size([4, 692])
reward: tensor([-0.4180, -0.2695, -1.7188, -0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1741])
attention_mask shape: torch.Size([4, 1741])
reward: tensor([ 1.6406, -1.7500, -1.6172, -0.6250], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1478])
attention_mask shape: torch.Size([4, 1478])
reward: tensor([ 0.1279, -1.0859, -0.8906,  0.4492], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 533])
attention_mask shape: torch.Size([4, 533])
reward: tensor([-0.6406, -1.1797,  0.1748,  0.8828], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.3828, -0.1289,  0.8086, -1.3047], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1096])
attention_mask shape: torch.Size([4, 1096])
reward: tensor([-1.3984, -1.9609, -0.7344,  0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1759])
attention_mask shape: torch.Size([4, 1759])
reward: tensor([-1.5391, -1.8906,  0.6211, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1325])
attention_mask shape: torch.Size([4, 1325])
reward: tensor([-0.2773, -1.7812, -0.5586,  0.1348], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1322])
attention_mask shape: torch.Size([4, 1322])
reward: tensor([ 0.3574, -1.0078,  2.5312, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1692])
attention_mask shape: torch.Size([4, 1692])
reward: tensor([ 0.9609, -0.9961, -0.4004,  1.4844], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1118])
attention_mask shape: torch.Size([4, 1118])
reward: tensor([-0.5078, -0.7930,  1.8125,  1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1819])
attention_mask shape: torch.Size([4, 1819])
reward: tensor([ 0.5234, -0.6719,  0.1099, -0.0089], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1307])
attention_mask shape: torch.Size([4, 1307])
reward: tensor([-1.9297, -1.0469, -1.2109,  0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6250, -0.9258, -1.2891, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1322])
attention_mask shape: torch.Size([4, 1322])
reward: tensor([ 0.9219,  0.4043, -1.4141,  0.3359], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 867])
attention_mask shape: torch.Size([4, 867])
reward: tensor([ 0.7773, -1.6719,  1.0391,  0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 949])
attention_mask shape: torch.Size([4, 949])
reward: tensor([ 0.0566, -0.0645, -0.6367, -1.3750], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3730, -1.1562, -2.2031,  0.3008], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1109])
attention_mask shape: torch.Size([4, 1109])
reward: tensor([ 0.4746, -0.3652, -1.0312, -0.7305], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1570])
attention_mask shape: torch.Size([4, 1570])
reward: tensor([ 0.9258,  0.9141, -0.5703,  0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1731])
attention_mask shape: torch.Size([4, 1731])
reward: tensor([-0.2266,  0.0344,  1.6250, -1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 945])
attention_mask shape: torch.Size([4, 945])
reward: tensor([-0.6133, -0.5391, -1.1797,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 861])
attention_mask shape: torch.Size([4, 861])
reward: tensor([ 0.1484, -1.3984,  0.0378,  0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1615])
attention_mask shape: torch.Size([4, 1615])
reward: tensor([-0.2178, -0.9609, -0.2021, -0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1544])
attention_mask shape: torch.Size([4, 1544])
reward: tensor([ 0.4297, -1.7656, -1.2188,  0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1078])
attention_mask shape: torch.Size([4, 1078])
reward: tensor([-0.0089, -1.3125, -0.9414, -1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 621])
attention_mask shape: torch.Size([4, 621])
reward: tensor([ 0.3164,  0.0967, -0.6641,  0.0767], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1328])
attention_mask shape: torch.Size([4, 1328])
reward: tensor([-1.9375,  0.6406,  0.5195, -1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1135])
attention_mask shape: torch.Size([4, 1135])
reward: tensor([-0.0289, -0.2734, -0.6562, -1.6875], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
[2024-10-23 11:52:40,370] [INFO] [launch.py:351:main] Process 655610 exits successfully.
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([-1.5234, -0.2930,  1.0078, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1777])
attention_mask shape: torch.Size([4, 1777])
reward: tensor([1.5078, 0.8086, 1.2969, 0.0322], device='cuda:0', dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
[2024-10-23 11:53:08,398] [INFO] [launch.py:351:main] Process 655611 exits successfully.
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-1.1094, -0.7930,  1.2656,  0.3320], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-23 11:53:28,418] [INFO] [launch.py:351:main] Process 655609 exits successfully.
[2024-10-23 11:55:42,553] [INFO] [launch.py:351:main] Process 655612 exits successfully.
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# [K(base) root@autodl-container-ec234bbd2e-925c6d34:~# bash run_eval_reward_openrlhf.sh
[?2004l+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-7th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-7th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_7 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-23 20:59:11,713] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-23 20:59:13,468] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-23 20:59:13,468] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-7th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_7 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-23 20:59:14,828] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-23 20:59:19,520] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-23 20:59:19,520] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-23 20:59:19,520] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-23 20:59:19,520] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-23 20:59:19,520] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-23 20:59:19,520] [INFO] [launch.py:256:main] process 702078 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-7th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_7', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-23 20:59:19,521] [INFO] [launch.py:256:main] process 702079 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-7th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_7', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-23 20:59:19,521] [INFO] [launch.py:256:main] process 702080 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-7th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_7', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-23 20:59:19,521] [INFO] [launch.py:256:main] process 702081 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-7th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_7', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-23 20:59:21,158] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-23 20:59:21,188] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-23 20:59:21,192] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-23 20:59:21,200] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-23 20:59:25,242] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-23 20:59:25,242] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-23 20:59:26,145] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-23 20:59:26,147] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-23 20:59:27,273] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  8.35it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  8.51it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  8.70it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.02it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.84it/s]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  8.13it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  8.40it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.01it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  8.29it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.00it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.61it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.49it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.04it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.31it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.20it/s]
[2024-10-23 20:59:38,095] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  6.20it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  6.22it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  6.23it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.40it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.33it/s]
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-23 20:59:45,550] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-23 20:59:45,550] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-23 20:59:45,551] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 20:59:48,179] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 20:59:50,532] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 20:59:51,367] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-23 20:59:51,368] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-23 20:59:51,369] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 20:59:51,371] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 20:59:51,374] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 20:59:51,493] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-23 20:59:51,494] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-23 20:59:51,494] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.72 GB, percent = 3.0%
[2024-10-23 20:59:51,619] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-23 20:59:51,620] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-23 20:59:51,620] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.72 GB, percent = 3.0%
[2024-10-23 20:59:51,621] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f51e9829d50>
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-23 20:59:51,621] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-23 20:59:51,622] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-23 20:59:51,623] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-23 20:59:51,623] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-23 20:59:51,623] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-23 20:59:51,623] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-23 20:59:51,623] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-23 20:59:57,473] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-23 20:59:57,475] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-23 20:59:57,601] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-23 20:59:57,601] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-23 20:59:57,602] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.72 GB, percent = 3.0%
[2024-10-23 20:59:57,712] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-23 20:59:57,713] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-23 20:59:57,713] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.72 GB, percent = 3.0%
[2024-10-23 20:59:57,714] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-23 20:59:57,714] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-23 20:59:57,714] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-23 20:59:57,714] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-23 20:59:57,714] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f51e83c6890>
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-23 20:59:57,715] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-23 20:59:57,716] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-23 20:59:57,716] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 650/100000 [00:00<00:15, 6496.82it/s]Preprocessing data:   2%|█▌                                                                                          | 1694/100000 [00:00<00:11, 8813.43it/s]Preprocessing data:   3%|██▌                                                                                         | 2749/100000 [00:00<00:10, 9603.25it/s]Preprocessing data:   4%|███▍                                                                                        | 3790/100000 [00:00<00:09, 9920.41it/s]Preprocessing data:   5%|████▍                                                                                      | 4833/100000 [00:00<00:09, 10101.00it/s]Preprocessing data:   6%|█████▎                                                                                     | 5855/100000 [00:00<00:09, 10139.72it/s]Preprocessing data:   7%|██████▎                                                                                    | 6896/100000 [00:00<00:09, 10227.14it/s]Preprocessing data:   8%|███████▏                                                                                   | 7938/100000 [00:00<00:08, 10287.05it/s]Preprocessing data:   9%|████████▏                                                                                  | 8975/100000 [00:00<00:08, 10311.24it/s]Preprocessing data:  10%|█████████                                                                                 | 10018/100000 [00:01<00:08, 10346.61it/s]Preprocessing data:  11%|█████████▉                                                                                | 11108/100000 [00:01<00:08, 10515.14it/s]Preprocessing data:  12%|██████████▉                                                                               | 12181/100000 [00:01<00:08, 10577.68it/s]Preprocessing data:  13%|███████████▉                                                                              | 13271/100000 [00:01<00:08, 10672.79it/s]Preprocessing data:  14%|████████████▉                                                                             | 14358/100000 [00:01<00:07, 10731.31it/s]Preprocessing data:  15%|█████████████▉                                                                            | 15448/100000 [00:01<00:07, 10781.86it/s]Preprocessing data:  17%|██████████████▉                                                                           | 16538/100000 [00:01<00:07, 10814.97it/s]Preprocessing data:  18%|███████████████▊                                                                          | 17623/100000 [00:01<00:07, 10823.24it/s]Preprocessing data:  19%|████████████████▊                                                                         | 18711/100000 [00:01<00:07, 10839.30it/s]Preprocessing data:  20%|█████████████████▊                                                                        | 19800/100000 [00:01<00:07, 10852.88it/s]Preprocessing data:  21%|██████████████████▊                                                                       | 20905/100000 [00:02<00:07, 10909.50it/s]Preprocessing data:  22%|███████████████████▊                                                                      | 22009/100000 [00:02<00:07, 10947.35it/s]Preprocessing data:  23%|████████████████████▊                                                                     | 23104/100000 [00:02<00:07, 10889.43it/s]Preprocessing data:  24%|█████████████████████▊                                                                    | 24194/100000 [00:02<00:07, 10747.89it/s]Preprocessing data:  25%|██████████████████████▋                                                                   | 25270/100000 [00:02<00:06, 10736.67it/s]Preprocessing data:  26%|███████████████████████▋                                                                  | 26344/100000 [00:02<00:06, 10726.36it/s]Preprocessing data:  27%|████████████████████████▋                                                                 | 27417/100000 [00:02<00:06, 10720.84it/s]Preprocessing data:  28%|█████████████████████████▋                                                                | 28490/100000 [00:02<00:06, 10719.15it/s]Preprocessing data:  30%|██████████████████████████▌                                                               | 29563/100000 [00:02<00:06, 10676.63it/s]Preprocessing data:  31%|███████████████████████████▌                                                              | 30636/100000 [00:02<00:06, 10691.53it/s]Preprocessing data:  32%|████████████████████████████▌                                                             | 31706/100000 [00:03<00:06, 10682.35it/s]Preprocessing data:  33%|█████████████████████████████▍                                                            | 32775/100000 [00:03<00:06, 10678.90it/s]Preprocessing data:  34%|██████████████████████████████▍                                                           | 33843/100000 [00:03<00:06, 10672.48it/s]Preprocessing data:  35%|███████████████████████████████▍                                                          | 34911/100000 [00:03<00:06, 10658.43it/s]Preprocessing data:  36%|████████████████████████████████▍                                                         | 35977/100000 [00:03<00:06, 10596.15it/s]Preprocessing data:  37%|█████████████████████████████████▎                                                        | 37037/100000 [00:03<00:05, 10588.66it/s]Preprocessing data:  38%|██████████████████████████████████▎                                                       | 38100/100000 [00:03<00:05, 10598.28it/s]Preprocessing data:  39%|███████████████████████████████████▏                                                      | 39161/100000 [00:03<00:05, 10598.97it/s]Preprocessing data:  40%|████████████████████████████████████▏                                                     | 40230/100000 [00:03<00:05, 10625.09it/s]Preprocessing data:  41%|█████████████████████████████████████▏                                                    | 41299/100000 [00:03<00:05, 10642.29it/s]Preprocessing data:  42%|██████████████████████████████████████▏                                                   | 42364/100000 [00:04<00:05, 10618.45it/s]Preprocessing data:  43%|███████████████████████████████████████                                                   | 43440/100000 [00:04<00:05, 10659.78it/s]Preprocessing data:  45%|████████████████████████████████████████                                                  | 44526/100000 [00:04<00:05, 10718.65it/s]Preprocessing data:  46%|█████████████████████████████████████████                                                 | 45615/100000 [00:04<00:05, 10769.58it/s]Preprocessing data:  47%|██████████████████████████████████████████                                                | 46703/100000 [00:04<00:04, 10800.87it/s]Preprocessing data:  48%|███████████████████████████████████████████                                               | 47793/100000 [00:04<00:04, 10828.99it/s]Preprocessing data:  49%|███████████████████████████████████████████▉                                              | 48886/100000 [00:04<00:04, 10857.18it/s]Preprocessing data:  50%|████████████████████████████████████████████▉                                             | 49974/100000 [00:04<00:04, 10862.13it/s]Preprocessing data:  51%|█████████████████████████████████████████████▉                                            | 51063/100000 [00:04<00:04, 10867.73it/s]Preprocessing data:  52%|██████████████████████████████████████████████▉                                           | 52150/100000 [00:04<00:04, 10860.83it/s]Preprocessing data:  53%|███████████████████████████████████████████████▉                                          | 53239/100000 [00:05<00:04, 10867.79it/s]Preprocessing data:  54%|████████████████████████████████████████████████▉                                         | 54326/100000 [00:05<00:04, 10864.42it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▊                                        | 55415/100000 [00:05<00:04, 10871.76it/s]Preprocessing data:  57%|██████████████████████████████████████████████████▊                                       | 56503/100000 [00:05<00:04, 10872.09it/s]Preprocessing data:  58%|███████████████████████████████████████████████████▊                                      | 57593/100000 [00:05<00:03, 10877.86it/s]Preprocessing data:  59%|████████████████████████████████████████████████████▊                                     | 58681/100000 [00:05<00:03, 10820.63it/s]Preprocessing data:  60%|█████████████████████████████████████████████████████▊                                    | 59770/100000 [00:05<00:03, 10840.59it/s]Preprocessing data:  61%|██████████████████████████████████████████████████████▊                                   | 60857/100000 [00:05<00:03, 10847.32it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▊                                  | 61945/100000 [00:05<00:03, 10855.72it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▋                                 | 63041/100000 [00:05<00:03, 10884.86it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▋                                | 64130/100000 [00:06<00:03, 10870.44it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▋                               | 65218/100000 [00:06<00:03, 10854.43it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▋                              | 66304/100000 [00:06<00:03, 10842.26it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████▋                             | 67389/100000 [00:06<00:03, 10841.82it/s]Preprocessing data:  68%|█████████████████████████████████████████████████████████████▋                            | 68474/100000 [00:06<00:02, 10824.43it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▌                           | 69557/100000 [00:06<00:02, 10754.49it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▌                          | 70633/100000 [00:06<00:02, 10750.54it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▌                         | 71714/100000 [00:06<00:02, 10768.08it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▌                        | 72791/100000 [00:06<00:02, 10488.87it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▏                       | 73842/100000 [00:06<00:02, 9676.29it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████                       | 74823/100000 [00:07<00:02, 9162.33it/s]Preprocessing data:  76%|████████████████████████████████████████████████████████████████████▉                      | 75752/100000 [00:07<00:02, 8857.31it/s]Preprocessing data:  77%|█████████████████████████████████████████████████████████████████████▋                     | 76647/100000 [00:07<00:02, 8644.64it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▌                    | 77517/100000 [00:07<00:02, 8493.10it/s]Preprocessing data:  78%|███████████████████████████████████████████████████████████████████████▎                   | 78370/100000 [00:07<00:02, 8391.79it/s]Preprocessing data:  79%|████████████████████████████████████████████████████████████████████████                   | 79211/100000 [00:07<00:02, 8293.96it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▊                  | 80042/100000 [00:07<00:02, 8137.95it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▌                 | 80857/100000 [00:07<00:02, 8027.07it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▌                | 81889/100000 [00:07<00:02, 8682.76it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▎               | 82761/100000 [00:08<00:02, 8545.10it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████               | 83618/100000 [00:08<00:01, 8515.73it/s]Preprocessing data:  85%|████████████████████████████████████████████████████████████████████████████▉              | 84544/100000 [00:08<00:01, 8731.09it/s]Preprocessing data:  86%|█████████████████████████████████████████████████████████████████████████████▉             | 85613/100000 [00:08<00:01, 9305.39it/s]Preprocessing data:  87%|██████████████████████████████████████████████████████████████████████████████▊            | 86546/100000 [00:08<00:01, 8982.44it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▌           | 87448/100000 [00:08<00:01, 8976.49it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████▍          | 88455/100000 [00:08<00:01, 9294.15it/s]Preprocessing data:  89%|█████████████████████████████████████████████████████████████████████████████████▍         | 89474/100000 [00:08<00:01, 9555.97it/s]Preprocessing data:  90%|██████████████████████████████████████████████████████████████████████████████████▎        | 90432/100000 [00:08<00:01, 9330.48it/s]Preprocessing data:  91%|███████████████████████████████████████████████████████████████████████████████████▏       | 91368/100000 [00:08<00:00, 8685.24it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▉       | 92247/100000 [00:09<00:00, 8269.25it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▋      | 93083/100000 [00:09<00:00, 8015.15it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▍     | 93891/100000 [00:09<00:00, 7995.24it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▎    | 94898/100000 [00:09<00:00, 8576.61it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▎   | 95932/100000 [00:09<00:00, 9081.09it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▏  | 96848/100000 [00:09<00:00, 8956.80it/s]Preprocessing data:  98%|████████████████████████████████████████████████████████████████████████████████████████▉  | 97780/100000 [00:09<00:00, 9061.63it/s]Preprocessing data:  99%|█████████████████████████████████████████████████████████████████████████████████████████▊ | 98761/100000 [00:09<00:00, 9280.33it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████▊| 99781/100000 [00:09<00:00, 9550.30it/s]Preprocessing data: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:09<00:00, 10049.92it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.6250, -2.1094, -2.1250, -0.4355], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 1.2969, -1.2422, -2.1562, -1.9375], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.9844, -1.4766, -1.8672, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 207])
attention_mask shape: torch.Size([4, 207])
reward: tensor([-1.9219, -1.7500, -1.5703, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 0.9570,  0.0723, -2.1250, -1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 352])
attention_mask shape: torch.Size([4, 352])
reward: tensor([-2.1406, -2.1250, -2.1719, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 932])
attention_mask shape: torch.Size([4, 932])
reward: tensor([-2.2031, -2.0938, -2.0781, -1.7578], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 363])
attention_mask shape: torch.Size([4, 363])
reward: tensor([-1.8828, -1.0156, -2.1562, -1.7891], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 810])
attention_mask shape: torch.Size([4, 810])
reward: tensor([-2.0781, -2.1406, -1.0312, -1.9297], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 0.1553, -1.2812, -1.9609, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 597])
attention_mask shape: torch.Size([4, 597])
reward: tensor([-2.2031, -0.8984, -1.9297, -1.8438], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 678])
attention_mask shape: torch.Size([4, 678])
reward: tensor([-1.6094, -1.7266, -2.2031, -1.9531], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 482])
attention_mask shape: torch.Size([4, 482])
reward: tensor([-1.7422, -1.3984, -2.1094, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 985])
attention_mask shape: torch.Size([4, 985])
reward: tensor([-1.0781, -1.5312, -2.0469, -2.2188], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 156])
attention_mask shape: torch.Size([4, 156])
reward: tensor([-1.6406, -1.8984, -2.1094, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.4766, -1.7891, -0.4746, -1.5703], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 896])
attention_mask shape: torch.Size([4, 896])
reward: tensor([-1.8828, -2.0000, -0.9414, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.0781, -0.0466, -0.9961, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 357])
attention_mask shape: torch.Size([4, 357])
reward: tensor([-1.9453, -1.3047, -2.2188, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 203])
attention_mask shape: torch.Size([4, 203])
reward: tensor([-2.2031, -2.1562, -2.1094, -1.4453], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 656])
attention_mask shape: torch.Size([4, 656])
reward: tensor([-1.6797, -1.7578, -1.8984, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 676])
attention_mask shape: torch.Size([4, 676])
reward: tensor([-2.0781, -1.9922, -1.7578, -0.3691], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 477])
attention_mask shape: torch.Size([4, 477])
reward: tensor([-1.7422, -2.0938, -1.9766, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 715])
attention_mask shape: torch.Size([4, 715])
reward: tensor([-2.1562, -2.1094, -0.8164, -1.7578], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.9258, -0.7383, -1.8438, -2.2188], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.7656, -2.0781, -2.1719, -1.8047], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-2.0000, -2.0000, -0.0356, -1.5938], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 188])
attention_mask shape: torch.Size([4, 188])
reward: tensor([-2.2031, -2.0312, -2.1094, -1.5938], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 462])
attention_mask shape: torch.Size([4, 462])
reward: tensor([-2.2188, -2.0469, -2.1094, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1002])
attention_mask shape: torch.Size([4, 1002])
reward: tensor([-1.5469, -2.1719, -1.8125, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.0312,  0.4336, -1.8984, -1.9531], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 565])
attention_mask shape: torch.Size([4, 565])
reward: tensor([-2.1094, -2.0469, -1.5312, -1.8984], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1406, -0.4746, -2.1562, -1.5938], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([ 0.1406, -2.1562, -1.8281, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 724])
attention_mask shape: torch.Size([4, 724])
reward: tensor([-2.1250, -2.0469, -2.0000, -1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1028])
attention_mask shape: torch.Size([4, 1028])
reward: tensor([-2.1719, -2.1094,  0.4336, -1.7031], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 609])
attention_mask shape: torch.Size([4, 609])
reward: tensor([-1.9609, -1.9922, -1.7500, -0.6250], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.0400, -1.9141, -2.0156, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 143])
attention_mask shape: torch.Size([4, 143])
reward: tensor([-1.9375, -2.1562, -2.2031, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.0781, -1.8203, -2.2031,  0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 755])
attention_mask shape: torch.Size([4, 755])
reward: tensor([-1.9922, -2.1719, -2.1719, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 594])
attention_mask shape: torch.Size([4, 594])
reward: tensor([-2.1406, -2.1250, -1.6641, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 216])
attention_mask shape: torch.Size([4, 216])
reward: tensor([-1.5391, -1.9531, -1.5078, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 447])
attention_mask shape: torch.Size([4, 447])
reward: tensor([-1.7891, -1.9531, -1.8984, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1031])
attention_mask shape: torch.Size([4, 1031])
reward: tensor([-2.1719, -2.1719, -0.7969,  0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 615])
attention_mask shape: torch.Size([4, 615])
reward: tensor([-2.1719, -1.2266, -1.8047, -1.6250], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 164])
attention_mask shape: torch.Size([4, 164])
reward: tensor([-1.1719, -2.1719, -2.2031, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.9219, -2.2031, -0.4883, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.7891, -1.7266, -0.0334, -1.5234], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 406])
attention_mask shape: torch.Size([4, 406])
reward: tensor([-1.7266, -2.0938, -1.1562, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.8750, -1.6250, -1.8047, -0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1029])
attention_mask shape: torch.Size([4, 1029])
reward: tensor([-1.2422,  1.0703, -2.0156,  0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 960])
attention_mask shape: torch.Size([4, 960])
reward: tensor([-1.6641, -1.9609, -1.9609, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 847])
attention_mask shape: torch.Size([4, 847])
reward: tensor([-2.1719, -1.2031, -2.0156, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1562, -1.6797, -2.0469, -0.3906], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 248])
attention_mask shape: torch.Size([4, 248])
reward: tensor([-1.8828, -1.8672, -2.0781, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 701])
attention_mask shape: torch.Size([4, 701])
reward: tensor([-1.1641, -2.0781, -1.9922, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 440])
attention_mask shape: torch.Size([4, 440])
reward: tensor([-2.1719, -1.5234, -2.1094, -1.9297], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 534])
attention_mask shape: torch.Size([4, 534])
reward: tensor([-2.1562, -1.8438, -2.1719, -1.8516], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1250, -2.2031, -2.1250, -0.3691], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 909])
attention_mask shape: torch.Size([4, 909])
reward: tensor([-2.1562, -1.8125, -1.9297, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-1.6719, -1.7969,  0.9414, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.8047, -1.8672, -2.0000, -0.0579], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 340])
attention_mask shape: torch.Size([4, 340])
reward: tensor([-2.2188, -2.1406, -1.7188, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 803])
attention_mask shape: torch.Size([4, 803])
reward: tensor([-0.7734, -1.8125, -2.2031, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 380])
attention_mask shape: torch.Size([4, 380])
reward: tensor([-1.4922, -1.8906, -1.9141, -0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 550])
attention_mask shape: torch.Size([4, 550])
reward: tensor([-2.0156, -1.7500, -1.8047, -1.9219], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.2188, -1.7188, -2.2031,  0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 228])
attention_mask shape: torch.Size([4, 228])
reward: tensor([-2.1406, -1.8906, -2.1406, -1.6094], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.0938, -2.0781, -2.0312,  0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1008])
attention_mask shape: torch.Size([4, 1008])
reward: tensor([-2.0156, -1.2812, -1.7500, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 342])
attention_mask shape: torch.Size([4, 342])
reward: tensor([-2.1250, -1.9844, -2.1406, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 659])
attention_mask shape: torch.Size([4, 659])
reward: tensor([-1.9609, -2.1406, -2.1250, -1.7188], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 938])
attention_mask shape: torch.Size([4, 938])
reward: tensor([-0.8320, -1.7969, -0.6836, -0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 174])
attention_mask shape: torch.Size([4, 174])
reward: tensor([-2.0469, -2.0312, -1.7969, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 462])
attention_mask shape: torch.Size([4, 462])
reward: tensor([-1.1719, -1.8516, -2.1562, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 1.6172, -1.9531, -1.6953, -1.8906], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1020])
attention_mask shape: torch.Size([4, 1020])
reward: tensor([-2.1562, -1.8906, -1.9453, -1.8984], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 725])
attention_mask shape: torch.Size([4, 725])
reward: tensor([-1.6016, -2.0469, -2.1719, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 408])
attention_mask shape: torch.Size([4, 408])
reward: tensor([-2.1562, -2.1719, -2.2031, -1.9531], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 566])
attention_mask shape: torch.Size([4, 566])
reward: tensor([-2.0000, -1.0703, -0.8594, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 177])
attention_mask shape: torch.Size([4, 177])
reward: tensor([-2.2031, -1.6719, -2.2031, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.6719, -2.0781, -0.9766, -0.2285], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 256])
attention_mask shape: torch.Size([4, 256])
reward: tensor([-1.0938, -2.1562, -2.0781, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 872])
attention_mask shape: torch.Size([4, 872])
reward: tensor([-1.5859, -1.8047, -1.8828, -0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.2188, -0.8477, -2.1562, -2.2188], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 587])
attention_mask shape: torch.Size([4, 587])
reward: tensor([-2.0938, -2.0000, -2.1250, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.9766, -2.0938, -1.4297, -0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.0938, -1.8516,  0.3828, -1.8438], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 181])
attention_mask shape: torch.Size([4, 181])
reward: tensor([-2.2031, -2.0938, -2.1719, -1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 327])
attention_mask shape: torch.Size([4, 327])
reward: tensor([-2.1719, -0.9883, -1.7891, -2.2188], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 885])
attention_mask shape: torch.Size([4, 885])
reward: tensor([-0.4141, -2.0312, -2.1719, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 223])
attention_mask shape: torch.Size([4, 223])
reward: tensor([-1.9609, -1.8516, -2.2188, -1.4922], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1034])
attention_mask shape: torch.Size([4, 1034])
reward: tensor([-2.2031, -0.2891, -0.9883, -1.7500], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.2188, -1.6250,  0.5391, -1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 385])
attention_mask shape: torch.Size([4, 385])
reward: tensor([-2.0312, -2.1406, -2.1406, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1406, -0.0244, -2.1719, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 131])
attention_mask shape: torch.Size([4, 131])
reward: tensor([-2.0469, -2.1094, -1.8750, -2.2188], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 452])
attention_mask shape: torch.Size([4, 452])
reward: tensor([-1.5312, -2.1406, -0.6367, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 757])
attention_mask shape: torch.Size([4, 757])
reward: tensor([-2.1562, -1.9141, -1.0469, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 655])
attention_mask shape: torch.Size([4, 655])
reward: tensor([-2.1406, -1.5469, -1.7188, -2.0938], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 123])
attention_mask shape: torch.Size([4, 123])
reward: tensor([-2.1562, -1.8906, -2.1406, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 770])
attention_mask shape: torch.Size([4, 770])
reward: tensor([-2.1719, -1.9453, -2.1250, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 822])
attention_mask shape: torch.Size([4, 822])
reward: tensor([-2.2031, -1.9844, -0.8438, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 276])
attention_mask shape: torch.Size([4, 276])
reward: tensor([-1.9922, -1.8281, -2.1094, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.7109, -1.5703, -2.1719,  1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 757])
attention_mask shape: torch.Size([4, 757])
reward: tensor([-2.0156, -1.0391, -1.9922, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 325])
attention_mask shape: torch.Size([4, 325])
reward: tensor([-2.0312, -1.6328, -1.5078, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 120])
attention_mask shape: torch.Size([4, 120])
reward: tensor([-1.9844, -2.0938, -1.9219, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1562, -0.8164, -2.0469, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 905])
attention_mask shape: torch.Size([4, 905])
reward: tensor([-1.9141, -1.4609, -2.2031, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 190])
attention_mask shape: torch.Size([4, 190])
reward: tensor([-1.9297, -1.0859, -2.0312, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-1.8203, -2.1250, -1.8828,  0.0977], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 454])
attention_mask shape: torch.Size([4, 454])
reward: tensor([-1.6172, -1.5547, -2.0938, -1.5469], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 891])
attention_mask shape: torch.Size([4, 891])
reward: tensor([-2.0781, -0.6016, -1.7812, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 251])
attention_mask shape: torch.Size([4, 251])
reward: tensor([-1.5391, -2.0781, -2.2188, -1.9375], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.5391, -0.0801, -0.7695, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 645])
attention_mask shape: torch.Size([4, 645])
reward: tensor([-1.6875, -1.9453, -2.0469, -1.9922], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 336])
attention_mask shape: torch.Size([4, 336])
reward: tensor([-1.3359, -2.2031, -2.0469, -2.0938], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-2.0156, -0.2754,  1.0312, -1.7422], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 589])
attention_mask shape: torch.Size([4, 589])
reward: tensor([-1.4141, -1.8125, -1.9453, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.2188, -1.4141, -2.1719, -0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 807])
attention_mask shape: torch.Size([4, 807])
reward: tensor([-1.5234, -2.0469, -2.2031, -1.9766], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 619])
attention_mask shape: torch.Size([4, 619])
reward: tensor([-1.1250, -2.2188, -2.0312, -1.9922], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 221])
attention_mask shape: torch.Size([4, 221])
reward: tensor([-1.7266, -2.0000, -2.1094, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 394])
attention_mask shape: torch.Size([4, 394])
reward: tensor([-2.1250, -2.1094, -2.1094, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.2441, -2.0781, -1.8828, -2.2188], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 895])
attention_mask shape: torch.Size([4, 895])
reward: tensor([-2.0312, -1.3984, -0.7930, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-1.5859, -1.0938, -2.1406,  0.8828], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.6875, -2.0781,  0.7500, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.0781, -0.0713, -2.0469, -1.8438], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 572])
attention_mask shape: torch.Size([4, 572])
reward: tensor([-1.1016, -2.1719, -1.7344, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-0.1396, -1.8594, -2.1094, -2.0938], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 114])
attention_mask shape: torch.Size([4, 114])
reward: tensor([-2.1562, -1.9141, -1.5078, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 229])
attention_mask shape: torch.Size([4, 229])
reward: tensor([-2.1406, -1.6719, -1.8984, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1033])
attention_mask shape: torch.Size([4, 1033])
reward: tensor([-0.8047, -2.1719, -2.1719, -2.2188], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.6406, -1.2656, -1.4688, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 834])
attention_mask shape: torch.Size([4, 834])
reward: tensor([-2.1562, -1.6328, -2.2188, -2.0938], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 590])
attention_mask shape: torch.Size([4, 590])
reward: tensor([-0.7617, -0.8086, -0.8594, -1.6562], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.0378, -1.7188, -1.3516, -1.8516], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 260])
attention_mask shape: torch.Size([4, 260])
reward: tensor([-1.9844, -1.9609, -2.2031, -1.6172], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 408])
attention_mask shape: torch.Size([4, 408])
reward: tensor([-2.0781, -1.8203, -1.9766, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 611])
attention_mask shape: torch.Size([4, 611])
reward: tensor([-2.1719, -2.2031, -2.0938, -2.0938], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 507])
attention_mask shape: torch.Size([4, 507])
reward: tensor([-1.6797, -2.2031, -1.7969, -1.8125], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 480])
attention_mask shape: torch.Size([4, 480])
reward: tensor([-2.1250, -2.1562, -1.8281, -1.6875], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-0.1396, -1.9219, -1.8750, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 400])
attention_mask shape: torch.Size([4, 400])
reward: tensor([-2.0469, -1.9453, -2.2031, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 475])
attention_mask shape: torch.Size([4, 475])
reward: tensor([-1.8984, -2.1250, -2.1406, -1.7031], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 305])
attention_mask shape: torch.Size([4, 305])
reward: tensor([-2.1562, -0.5469, -2.1562, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 446])
attention_mask shape: torch.Size([4, 446])
reward: tensor([-1.8125, -2.1406, -2.2031, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 518])
attention_mask shape: torch.Size([4, 518])
reward: tensor([-1.8594, -2.0469, -1.8594, -1.1797], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 219])
attention_mask shape: torch.Size([4, 219])
reward: tensor([-1.2656, -1.8047, -2.0156, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 499])
attention_mask shape: torch.Size([4, 499])
reward: tensor([-2.1406, -2.1719, -1.8516, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 591])
attention_mask shape: torch.Size([4, 591])
reward: tensor([-1.9297, -2.2188, -2.1719, -1.8984], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 323])
attention_mask shape: torch.Size([4, 323])
reward: tensor([-2.0156, -2.1094, -1.7656, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 522])
attention_mask shape: torch.Size([4, 522])
reward: tensor([-1.9375, -1.7812, -1.8750, -1.9219], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 625])
attention_mask shape: torch.Size([4, 625])
reward: tensor([-2.0781, -2.0781, -1.9297, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 805])
attention_mask shape: torch.Size([4, 805])
reward: tensor([-1.6719, -1.4062, -2.2031, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-2.0781, -2.1562, -0.5352, -0.3691], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 781])
attention_mask shape: torch.Size([4, 781])
reward: tensor([-1.4375, -2.0000, -2.1406, -1.6250], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 172])
attention_mask shape: torch.Size([4, 172])
reward: tensor([-1.4844, -2.2188, -2.2031, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 544])
attention_mask shape: torch.Size([4, 544])
reward: tensor([-0.9258, -1.8906, -1.2422, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 342])
attention_mask shape: torch.Size([4, 342])
reward: tensor([-1.4219, -2.1094, -2.1719, -1.8125], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 876])
attention_mask shape: torch.Size([4, 876])
reward: tensor([-2.0469, -1.0703, -1.6953, -1.6016], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 318])
attention_mask shape: torch.Size([4, 318])
reward: tensor([-2.1562, -1.9844, -2.1562, -1.9297], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 0.5234, -2.1250, -2.1250, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 0.5430, -2.0781, -2.1406, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 110])
attention_mask shape: torch.Size([4, 110])
reward: tensor([-2.0938, -1.9531, -1.8906, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 1.8594, -1.8750, -2.1094, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 546])
attention_mask shape: torch.Size([4, 546])
reward: tensor([-2.1562, -1.8984, -1.9609, -1.7812], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 835])
attention_mask shape: torch.Size([4, 835])
reward: tensor([-2.1406, -1.6953, -2.1562, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 0.7461, -0.8516, -1.0312, -1.8594], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 98])
attention_mask shape: torch.Size([4, 98])
reward: tensor([-2.0312, -2.1250, -1.3516, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 658])
attention_mask shape: torch.Size([4, 658])
reward: tensor([-2.1562, -2.1406, -2.2031, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 713])
attention_mask shape: torch.Size([4, 713])
reward: tensor([-2.1094, -2.0781, -0.5820, -1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.6719, -2.1250, -2.1406, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.2031, -2.0938, -2.2188, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.0469, -2.1719, -2.2188,  0.1348], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 915])
attention_mask shape: torch.Size([4, 915])
reward: tensor([-1.6016, -0.8203, -1.5234, -1.6016], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1028])
attention_mask shape: torch.Size([4, 1028])
reward: tensor([-2.1406, -1.1562, -0.4082, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 442])
attention_mask shape: torch.Size([4, 442])
reward: tensor([-2.1406, -2.1562, -1.8438, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 559])
attention_mask shape: torch.Size([4, 559])
reward: tensor([-1.9922, -1.8672, -1.6875, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1406, -0.5820, -1.6641, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 211])
attention_mask shape: torch.Size([4, 211])
reward: tensor([-1.9766, -1.8438, -2.1562, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1562, -0.6562, -1.9922, -1.8750], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.9844, -2.0156, -1.9141, -0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.6797, -1.1016, -1.5312, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 0.3418, -2.1719, -2.0469, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.0469, -2.1406,  0.5859, -1.7422], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 319])
attention_mask shape: torch.Size([4, 319])
reward: tensor([-2.2031, -1.7891, -2.1719, -1.9531], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 160])
attention_mask shape: torch.Size([4, 160])
reward: tensor([-0.4316, -1.9766, -2.1250, -1.9375], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.2188, -0.5703, -2.1406, -2.0938], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.6367, -1.9609, -1.5312, -0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 198])
attention_mask shape: torch.Size([4, 198])
reward: tensor([-2.1719, -1.7422, -2.0156, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.0469, -0.5508, -1.7188, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 282])
attention_mask shape: torch.Size([4, 282])
reward: tensor([-1.4375, -2.1562, -2.0469, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 796])
attention_mask shape: torch.Size([4, 796])
reward: tensor([-0.8633, -1.7266, -1.1875, -0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 480])
attention_mask shape: torch.Size([4, 480])
reward: tensor([-1.6406, -1.6328, -1.8828, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 830])
attention_mask shape: torch.Size([4, 830])
reward: tensor([-2.0938, -2.0312, -2.1719, -1.8281], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.5000, -0.1865, -1.8750, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-1.8984, -0.6914, -1.4766, -1.9375], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 267])
attention_mask shape: torch.Size([4, 267])
reward: tensor([-2.1094, -2.2031, -2.1562, -2.2188], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 174])
attention_mask shape: torch.Size([4, 174])
reward: tensor([-2.0781, -2.1094, -2.1250, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 93])
attention_mask shape: torch.Size([4, 93])
reward: tensor([-2.1094, -2.1250, -2.2031, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 553])
attention_mask shape: torch.Size([4, 553])
reward: tensor([-2.0781, -2.1719, -2.0781, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 382])
attention_mask shape: torch.Size([4, 382])
reward: tensor([-1.3984, -0.5898, -2.0938, -1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 227])
attention_mask shape: torch.Size([4, 227])
reward: tensor([-2.0312, -1.8906, -2.1250, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 857])
attention_mask shape: torch.Size([4, 857])
reward: tensor([-1.1406, -1.5625, -2.1406, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1035])
attention_mask shape: torch.Size([4, 1035])
reward: tensor([ 0.6523, -0.6055, -2.1094, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.2031, -2.1406,  0.4180, -0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 261])
attention_mask shape: torch.Size([4, 261])
reward: tensor([-2.2188, -2.0312, -2.2188, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1406, -0.5742, -2.1719, -1.8906], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 978])
attention_mask shape: torch.Size([4, 978])
reward: tensor([-1.7812, -2.1562, -2.1562, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 115])
attention_mask shape: torch.Size([4, 115])
reward: tensor([-2.1562, -1.8984, -2.0781, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.0312,  0.3750, -2.1719, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([ 0.6133, -2.2188, -2.1094, -1.5938], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 108])
attention_mask shape: torch.Size([4, 108])
reward: tensor([-2.1562, -2.2031, -2.1719, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 112])
attention_mask shape: torch.Size([4, 112])
reward: tensor([-2.0156, -2.1562, -2.1719, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 845])
attention_mask shape: torch.Size([4, 845])
reward: tensor([-1.8750, -2.1406, -0.9766, -0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 284])
attention_mask shape: torch.Size([4, 284])
reward: tensor([-1.6797, -1.4922, -1.6250, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 153])
attention_mask shape: torch.Size([4, 153])
reward: tensor([-2.1250, -1.8516, -1.9844, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 823])
attention_mask shape: torch.Size([4, 823])
reward: tensor([-1.5234, -1.8984, -1.3359, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.2969, -1.7969, -0.5859, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 213])
attention_mask shape: torch.Size([4, 213])
reward: tensor([-2.0938, -2.0781, -1.9141, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 423])
attention_mask shape: torch.Size([4, 423])
reward: tensor([-2.0938, -2.1094, -1.8516, -1.8750], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 719])
attention_mask shape: torch.Size([4, 719])
reward: tensor([-0.7344, -1.5547, -2.0469, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 700])
attention_mask shape: torch.Size([4, 700])
reward: tensor([-2.1406, -1.8906, -1.9375, -1.8281], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 449])
attention_mask shape: torch.Size([4, 449])
reward: tensor([-2.1406, -1.4453, -1.5625, -0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.0000, -2.1719, -1.7188,  0.4785], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 586])
attention_mask shape: torch.Size([4, 586])
reward: tensor([-1.2344, -1.7656, -2.0469, -1.7969], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 737])
attention_mask shape: torch.Size([4, 737])
reward: tensor([-2.0781, -2.2031, -2.0938, -1.9141], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1094, -1.7031, -1.9219, -0.2090], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-1.7188, -1.6875,  2.0938, -1.9922], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 1.8125, -1.7812, -1.1875, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 488])
attention_mask shape: torch.Size([4, 488])
reward: tensor([-1.8516, -1.9766, -2.2031, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.6523, -0.5195, -1.9922, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 285])
attention_mask shape: torch.Size([4, 285])
reward: tensor([-2.0938, -2.1562, -1.9531, -1.9609], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-0.8438, -2.1719,  0.9102, -2.0938], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 844])
attention_mask shape: torch.Size([4, 844])
reward: tensor([-1.7266, -1.6172, -1.6328, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 617])
attention_mask shape: torch.Size([4, 617])
reward: tensor([-1.7422, -1.8281, -2.0469, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 487])
attention_mask shape: torch.Size([4, 487])
reward: tensor([-0.7148, -2.1719, -1.7812, -1.6641], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1406, -0.9258, -1.8203, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 709])
attention_mask shape: torch.Size([4, 709])
reward: tensor([-1.7500, -0.4355, -2.1719, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-2.1719, -0.8438, -2.0781,  0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.4980, -2.1094, -2.2031, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 664])
attention_mask shape: torch.Size([4, 664])
reward: tensor([-0.8711, -2.1250, -1.7500, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 498])
attention_mask shape: torch.Size([4, 498])
reward: tensor([-2.0312, -1.6641, -2.0312, -2.0781], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 593])
attention_mask shape: torch.Size([4, 593])
reward: tensor([-0.7734, -1.9609, -1.4453, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 965])
attention_mask shape: torch.Size([4, 965])
reward: tensor([-2.1562, -2.2031, -2.0156, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1094, -1.6406, -0.8711, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 87])
attention_mask shape: torch.Size([4, 87])
reward: tensor([-2.1406, -2.0469, -1.6719, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-2.1250, -0.9336, -2.1719, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 113])
attention_mask shape: torch.Size([4, 113])
reward: tensor([-2.0938, -2.1562, -2.1406, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 171])
attention_mask shape: torch.Size([4, 171])
reward: tensor([-2.1406, -2.2031, -2.1094, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1028])
attention_mask shape: torch.Size([4, 1028])
reward: tensor([ 1.0312, -2.2031, -2.2188, -2.0938], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 305])
attention_mask shape: torch.Size([4, 305])
reward: tensor([-2.1094, -2.1562, -2.1250, -1.7031], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-23 21:03:15,359] [INFO] [launch.py:351:main] Process 702078 exits successfully.
[2024-10-23 21:03:16,361] [INFO] [launch.py:351:main] Process 702079 exits successfully.
[2024-10-23 21:03:19,365] [INFO] [launch.py:351:main] Process 702081 exits successfully.
[2024-10-23 21:03:22,368] [INFO] [launch.py:351:main] Process 702080 exits successfully.
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# [K(base) root@autodl-container-ec234bbd2e-925c6d34:~# bash run_eval_reward_openrlhf.sh
[?2004l+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-8th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-8th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_8 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-24 19:36:46,654] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-24 19:36:49,405] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-24 19:36:49,406] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-8th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_8 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-24 19:36:50,780] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-24 19:36:52,626] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-24 19:36:52,626] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-24 19:36:52,626] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-24 19:36:52,626] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-24 19:36:52,627] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-24 19:36:52,627] [INFO] [launch.py:256:main] process 745529 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-8th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_8', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-24 19:36:52,627] [INFO] [launch.py:256:main] process 745530 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-8th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_8', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-24 19:36:52,628] [INFO] [launch.py:256:main] process 745531 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-8th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_8', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-24 19:36:52,628] [INFO] [launch.py:256:main] process 745532 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-8th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_8', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-24 19:36:56,403] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-24 19:36:56,404] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-24 19:36:56,404] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-24 19:36:56,414] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-24 19:36:58,824] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-24 19:36:58,824] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-24 19:36:59,217] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-24 19:36:59,239] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-24 19:36:59,257] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.68it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.55it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.67it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.57it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.61it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.53it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.17it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.36it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.81it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.74it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.71it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.65it/s]
Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.31it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.54it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.41it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.64it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.64it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.52it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.11it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.26it/s]
[2024-10-24 19:37:04,158] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-24 19:37:04,495] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-24 19:37:04,603] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-24 19:37:04,603] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-24 19:37:04,604] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-24 19:37:05,198] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-24 19:37:05,656] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-24 19:37:05,657] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-24 19:37:05,662] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-24 19:37:05,663] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-24 19:37:05,664] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-24 19:37:05,867] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-24 19:37:05,868] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-24 19:37:05,868] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 26.94 GB, percent = 2.7%
[2024-10-24 19:37:06,054] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-24 19:37:06,055] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-24 19:37:06,055] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 26.94 GB, percent = 2.7%
[2024-10-24 19:37:06,056] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f32a41c57b0>
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-24 19:37:06,056] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-24 19:37:06,057] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-24 19:37:06,058] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-24 19:37:06,058] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-24 19:37:06,058] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-24 19:37:06,058] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-24 19:37:06,058] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-24 19:37:11,710] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-24 19:37:11,712] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-24 19:37:11,857] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-24 19:37:11,857] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-24 19:37:11,857] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 26.96 GB, percent = 2.7%
[2024-10-24 19:37:11,976] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-24 19:37:11,976] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-24 19:37:11,977] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 26.97 GB, percent = 2.7%
[2024-10-24 19:37:11,978] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f329c30e8f0>
[2024-10-24 19:37:11,978] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-24 19:37:11,979] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-24 19:37:11,980] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-24 19:37:11,980] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 632/100000 [00:00<00:15, 6319.73it/s]Preprocessing data:   2%|█▌                                                                                          | 1655/100000 [00:00<00:11, 8619.54it/s]Preprocessing data:   3%|██▍                                                                                         | 2671/100000 [00:00<00:10, 9321.94it/s]Preprocessing data:   4%|███▍                                                                                        | 3693/100000 [00:00<00:09, 9673.96it/s]Preprocessing data:   5%|████▎                                                                                       | 4693/100000 [00:00<00:09, 9790.55it/s]Preprocessing data:   6%|█████▏                                                                                      | 5705/100000 [00:00<00:09, 9901.82it/s]Preprocessing data:   7%|██████▏                                                                                     | 6719/100000 [00:00<00:09, 9978.15it/s]Preprocessing data:   8%|███████                                                                                    | 7737/100000 [00:00<00:09, 10040.78it/s]Preprocessing data:   9%|███████▉                                                                                   | 8753/100000 [00:00<00:09, 10077.36it/s]Preprocessing data:  10%|████████▉                                                                                  | 9761/100000 [00:01<00:08, 10075.35it/s]Preprocessing data:  11%|█████████▊                                                                                | 10837/100000 [00:01<00:08, 10282.15it/s]Preprocessing data:  12%|██████████▋                                                                               | 11922/100000 [00:01<00:08, 10454.65it/s]Preprocessing data:  13%|███████████▋                                                                              | 13002/100000 [00:01<00:08, 10558.79it/s]Preprocessing data:  14%|████████████▋                                                                             | 14071/100000 [00:01<00:08, 10596.51it/s]Preprocessing data:  15%|█████████████▋                                                                            | 15150/100000 [00:01<00:07, 10652.17it/s]Preprocessing data:  16%|██████████████▌                                                                           | 16226/100000 [00:01<00:07, 10683.44it/s]Preprocessing data:  17%|███████████████▌                                                                          | 17303/100000 [00:01<00:07, 10707.73it/s]Preprocessing data:  18%|████████████████▌                                                                         | 18379/100000 [00:01<00:07, 10721.57it/s]Preprocessing data:  19%|█████████████████▌                                                                        | 19455/100000 [00:01<00:07, 10732.79it/s]Preprocessing data:  21%|██████████████████▍                                                                       | 20529/100000 [00:02<00:07, 10720.31it/s]Preprocessing data:  22%|███████████████████▍                                                                      | 21625/100000 [00:02<00:07, 10790.88it/s]Preprocessing data:  23%|████████████████████▍                                                                     | 22705/100000 [00:02<00:07, 10769.53it/s]Preprocessing data:  24%|█████████████████████▍                                                                    | 23782/100000 [00:02<00:07, 10732.25it/s]Preprocessing data:  25%|██████████████████████▎                                                                   | 24856/100000 [00:02<00:07, 10697.11it/s]Preprocessing data:  26%|███████████████████████▎                                                                  | 25926/100000 [00:02<00:06, 10655.17it/s]Preprocessing data:  27%|████████████████████████▎                                                                 | 26992/100000 [00:02<00:06, 10579.72it/s]Preprocessing data:  28%|█████████████████████████▏                                                                | 28051/100000 [00:02<00:06, 10566.99it/s]Preprocessing data:  29%|██████████████████████████▏                                                               | 29108/100000 [00:02<00:06, 10537.85it/s]Preprocessing data:  30%|███████████████████████████▏                                                              | 30162/100000 [00:02<00:06, 10514.29it/s]Preprocessing data:  31%|████████████████████████████                                                              | 31215/100000 [00:03<00:06, 10517.99it/s]Preprocessing data:  32%|█████████████████████████████                                                             | 32267/100000 [00:03<00:06, 10515.56it/s]Preprocessing data:  33%|█████████████████████████████▉                                                            | 33319/100000 [00:03<00:06, 10468.57it/s]Preprocessing data:  34%|██████████████████████████████▉                                                           | 34366/100000 [00:03<00:06, 10458.71it/s]Preprocessing data:  35%|███████████████████████████████▊                                                          | 35412/100000 [00:03<00:06, 10411.61it/s]Preprocessing data:  36%|████████████████████████████████▊                                                         | 36454/100000 [00:03<00:06, 10373.00it/s]Preprocessing data:  37%|█████████████████████████████████▋                                                        | 37492/100000 [00:03<00:06, 10290.05it/s]Preprocessing data:  39%|██████████████████████████████████▋                                                       | 38526/100000 [00:03<00:05, 10302.47it/s]Preprocessing data:  40%|███████████████████████████████████▌                                                      | 39557/100000 [00:03<00:05, 10258.65it/s]Preprocessing data:  41%|████████████████████████████████████▌                                                     | 40594/100000 [00:03<00:05, 10291.21it/s]Preprocessing data:  42%|█████████████████████████████████████▍                                                    | 41624/100000 [00:04<00:05, 10283.72it/s]Preprocessing data:  43%|██████████████████████████████████████▍                                                   | 42653/100000 [00:04<00:05, 10252.66it/s]Preprocessing data:  44%|███████████████████████████████████████▎                                                  | 43718/100000 [00:04<00:05, 10368.40it/s]Preprocessing data:  45%|████████████████████████████████████████▎                                                 | 44787/100000 [00:04<00:05, 10464.26it/s]Preprocessing data:  46%|█████████████████████████████████████████▎                                                | 45859/100000 [00:04<00:05, 10539.38it/s]Preprocessing data:  47%|██████████████████████████████████████████▏                                               | 46933/100000 [00:04<00:05, 10597.58it/s]Preprocessing data:  48%|███████████████████████████████████████████▏                                              | 48014/100000 [00:04<00:04, 10660.35it/s]Preprocessing data:  49%|████████████████████████████████████████████▏                                             | 49092/100000 [00:04<00:04, 10694.50it/s]Preprocessing data:  50%|█████████████████████████████████████████████▏                                            | 50169/100000 [00:04<00:04, 10715.16it/s]Preprocessing data:  51%|██████████████████████████████████████████████                                            | 51247/100000 [00:04<00:04, 10731.74it/s]Preprocessing data:  52%|███████████████████████████████████████████████                                           | 52321/100000 [00:05<00:04, 10707.89it/s]Preprocessing data:  53%|████████████████████████████████████████████████                                          | 53403/100000 [00:05<00:04, 10739.99it/s]Preprocessing data:  54%|█████████████████████████████████████████████████                                         | 54482/100000 [00:05<00:04, 10754.64it/s]Preprocessing data:  56%|██████████████████████████████████████████████████                                        | 55558/100000 [00:05<00:04, 10750.38it/s]Preprocessing data:  57%|██████████████████████████████████████████████████▉                                       | 56638/100000 [00:05<00:04, 10763.54it/s]Preprocessing data:  58%|███████████████████████████████████████████████████▉                                      | 57715/100000 [00:05<00:03, 10749.12it/s]Preprocessing data:  59%|████████████████████████████████████████████████████▉                                     | 58790/100000 [00:05<00:03, 10742.63it/s]Preprocessing data:  60%|█████████████████████████████████████████████████████▉                                    | 59865/100000 [00:05<00:03, 10600.58it/s]Preprocessing data:  61%|██████████████████████████████████████████████████████▊                                   | 60935/100000 [00:05<00:03, 10628.82it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▊                                  | 62007/100000 [00:05<00:03, 10655.02it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▊                                 | 63088/100000 [00:06<00:03, 10698.84it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▋                                | 64159/100000 [00:06<00:03, 10674.93it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▋                               | 65227/100000 [00:06<00:03, 10652.37it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▋                              | 66293/100000 [00:06<00:03, 10627.25it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████▌                             | 67356/100000 [00:06<00:03, 10581.56it/s]Preprocessing data:  68%|█████████████████████████████████████████████████████████████▌                            | 68415/100000 [00:06<00:02, 10557.50it/s]Preprocessing data:  69%|██████████████████████████████████████████████████████████████▌                           | 69472/100000 [00:06<00:02, 10560.92it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▍                          | 70531/100000 [00:06<00:02, 10567.97it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▍                         | 71601/100000 [00:06<00:02, 10605.93it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▍                        | 72662/100000 [00:06<00:02, 10412.65it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████                        | 73705/100000 [00:07<00:02, 9505.53it/s]Preprocessing data:  75%|███████████████████████████████████████████████████████████████████▉                       | 74671/100000 [00:07<00:02, 9020.94it/s]Preprocessing data:  76%|████████████████████████████████████████████████████████████████████▊                      | 75587/100000 [00:07<00:02, 8715.84it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████▌                     | 76468/100000 [00:07<00:02, 8509.21it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▎                    | 77325/100000 [00:07<00:02, 8327.30it/s]Preprocessing data:  78%|███████████████████████████████████████████████████████████████████████▏                   | 78162/100000 [00:07<00:02, 8191.95it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▉                   | 78984/100000 [00:07<00:02, 7924.65it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▌                  | 79779/100000 [00:07<00:02, 7778.76it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▎                 | 80558/100000 [00:07<00:02, 7574.86it/s]Preprocessing data:  81%|██████████████████████████████████████████████████████████████████████████▏                | 81491/100000 [00:08<00:02, 8066.19it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▉                | 82387/100000 [00:08<00:02, 8320.98it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▋               | 83223/100000 [00:08<00:02, 8313.65it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▍              | 84058/100000 [00:08<00:01, 8106.76it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▍             | 85112/100000 [00:08<00:01, 8810.13it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▎            | 86057/100000 [00:08<00:01, 8994.52it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▏           | 86960/100000 [00:08<00:01, 8516.92it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████           | 87962/100000 [00:08<00:01, 8944.26it/s]Preprocessing data:  89%|████████████████████████████████████████████████████████████████████████████████▉          | 88964/100000 [00:08<00:01, 9252.97it/s]Preprocessing data:  90%|█████████████████████████████████████████████████████████████████████████████████▊         | 89956/100000 [00:08<00:01, 9446.25it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▋        | 90906/100000 [00:09<00:01, 8763.45it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▌       | 91795/100000 [00:09<00:00, 8232.03it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▎      | 92632/100000 [00:09<00:00, 7890.91it/s]Preprocessing data:  93%|█████████████████████████████████████████████████████████████████████████████████████      | 93431/100000 [00:09<00:00, 7743.86it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▊     | 94274/100000 [00:09<00:00, 7932.05it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▋    | 95283/100000 [00:09<00:00, 8537.66it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▌   | 96226/100000 [00:09<00:00, 8791.19it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▎  | 97112/100000 [00:09<00:00, 8677.54it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▏ | 97985/100000 [00:09<00:00, 8681.57it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████ | 98977/100000 [00:10<00:00, 9041.96it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████▉| 99992/100000 [00:10<00:00, 9367.41it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9827.00it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1260])
attention_mask shape: torch.Size([4, 1260])
reward: tensor([-0.1289, -1.0547, -0.0845, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.9922, -0.8789,  1.2656,  0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1065])
attention_mask shape: torch.Size([4, 1065])
reward: tensor([-1.2188, -0.7227, -1.3047, -0.5156], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 345])
attention_mask shape: torch.Size([4, 345])
reward: tensor([-0.8789, -1.6797,  0.2227,  0.1099], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1481])
attention_mask shape: torch.Size([4, 1481])
reward: tensor([ 1.1875,  0.3867, -0.2109, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([ 1.9375, -0.8203, -0.5508, -0.6367], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1431])
attention_mask shape: torch.Size([4, 1431])
reward: tensor([ 0.0698, -1.4141,  0.4707, -0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 762])
attention_mask shape: torch.Size([4, 762])
reward: tensor([ 0.8633, -0.2354,  1.1484,  0.7227], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 936])
attention_mask shape: torch.Size([4, 936])
reward: tensor([-0.4141, -0.1445, -0.4492, -0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1056])
attention_mask shape: torch.Size([4, 1056])
reward: tensor([ 0.3027, -0.9258, -0.9766, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 906])
attention_mask shape: torch.Size([4, 906])
reward: tensor([-1.9375,  0.2285,  0.7070, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 862])
attention_mask shape: torch.Size([4, 862])
reward: tensor([ 0.2451, -0.1465, -0.1934, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 615])
attention_mask shape: torch.Size([4, 615])
reward: tensor([-1.3750, -0.7148, -0.7070, -0.1177], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1246])
attention_mask shape: torch.Size([4, 1246])
reward: tensor([-0.4043,  0.4082,  0.0100, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 560])
attention_mask shape: torch.Size([4, 560])
reward: tensor([ 0.1406, -0.5352,  1.4062, -0.0889], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1551])
attention_mask shape: torch.Size([4, 1551])
reward: tensor([ 0.3281,  0.7461, -0.2441, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1332])
attention_mask shape: torch.Size([4, 1332])
reward: tensor([-1.7031, -1.3047,  0.0444,  0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1393])
attention_mask shape: torch.Size([4, 1393])
reward: tensor([ 0.4355,  0.0776, -0.1045,  0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 708])
attention_mask shape: torch.Size([4, 708])
reward: tensor([-0.6992, -0.9766,  0.0923, -1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 494])
attention_mask shape: torch.Size([4, 494])
reward: tensor([ 1.1094,  0.0156, -0.3164, -0.2598], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1038])
attention_mask shape: torch.Size([4, 1038])
reward: tensor([-1.5547,  0.3906, -0.2676, -0.0045], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1101])
attention_mask shape: torch.Size([4, 1101])
reward: tensor([-0.3027,  0.3867,  0.7773,  0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1499])
attention_mask shape: torch.Size([4, 1499])
reward: tensor([ 0.0579,  1.0000, -0.5117, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1124])
attention_mask shape: torch.Size([4, 1124])
reward: tensor([ 0.2451, -0.5625,  0.9023,  1.5547], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1438])
attention_mask shape: torch.Size([4, 1438])
reward: tensor([-0.2109, -0.3164,  0.0623, -0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1410])
attention_mask shape: torch.Size([4, 1410])
reward: tensor([ 0.6562, -0.4668, -0.0400, -0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1769])
attention_mask shape: torch.Size([4, 1769])
reward: tensor([-0.5742,  0.3242,  0.8906,  0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 409])
attention_mask shape: torch.Size([4, 409])
reward: tensor([ 0.6914, -1.1406, -0.9609, -1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 955])
attention_mask shape: torch.Size([4, 955])
reward: tensor([ 1.2422, -1.2500,  0.5820,  0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1431])
attention_mask shape: torch.Size([4, 1431])
reward: tensor([ 0.0791, -0.5156, -0.6367, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1109])
attention_mask shape: torch.Size([4, 1109])
reward: tensor([-1.3125,  0.5273, -0.9414, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 825])
attention_mask shape: torch.Size([4, 825])
reward: tensor([-1.0312,  0.0378, -0.8164, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([-1.1094, -0.3867, -0.7344,  0.0011], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([ 0.4453, -0.9062, -0.6250, -0.6094], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 775])
attention_mask shape: torch.Size([4, 775])
reward: tensor([-0.5039, -0.4629, -0.6719, -0.7227], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1665])
attention_mask shape: torch.Size([4, 1665])
reward: tensor([-0.3652, -0.2246,  1.0078, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1037])
attention_mask shape: torch.Size([4, 1037])
reward: tensor([-0.2578, -0.8984, -0.7695,  0.3008], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1144])
attention_mask shape: torch.Size([4, 1144])
reward: tensor([ 0.4141,  0.4395, -0.2559,  0.9922], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 956])
attention_mask shape: torch.Size([4, 956])
reward: tensor([-0.7422,  0.3594,  1.2344, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1525])
attention_mask shape: torch.Size([4, 1525])
reward: tensor([ 1.3438, -0.9258, -1.3672,  0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1461])
attention_mask shape: torch.Size([4, 1461])
reward: tensor([0.6680, 0.3613, 2.1562, 0.0522], device='cuda:0', dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1271])
attention_mask shape: torch.Size([4, 1271])
reward: tensor([ 0.6484,  0.3047, -0.9258,  1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 294])
attention_mask shape: torch.Size([4, 294])
reward: tensor([-0.6172, -0.2197, -1.0078, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 803])
attention_mask shape: torch.Size([4, 803])
reward: tensor([-1.7891, -0.3652, -0.5078, -0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1717])
attention_mask shape: torch.Size([4, 1717])
reward: tensor([-0.9883,  1.9453,  0.8906,  1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 743])
attention_mask shape: torch.Size([4, 743])
reward: tensor([-1.0312, -0.9609, -0.1826,  0.2236], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 803])
attention_mask shape: torch.Size([4, 803])
reward: tensor([ 0.1143, -0.4082,  1.5469,  0.0654], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1451])
attention_mask shape: torch.Size([4, 1451])
reward: tensor([-0.0977, -0.9336, -0.2373,  0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1392])
attention_mask shape: torch.Size([4, 1392])
reward: tensor([-0.7070, -0.0532,  0.4219, -1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 801])
attention_mask shape: torch.Size([4, 801])
reward: tensor([-1.4844,  0.3066, -0.3418,  0.4375], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1463])
attention_mask shape: torch.Size([4, 1463])
reward: tensor([-0.8164, -0.7383, -0.2070, -0.3516], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1230])
attention_mask shape: torch.Size([4, 1230])
reward: tensor([-0.7930,  1.5469,  0.0566,  0.1318], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1234])
attention_mask shape: torch.Size([4, 1234])
reward: tensor([-1.0938, -0.8984,  0.6562, -0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1237])
attention_mask shape: torch.Size([4, 1237])
reward: tensor([ 0.2520, -1.6797, -0.7227,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1437])
attention_mask shape: torch.Size([4, 1437])
reward: tensor([-0.0732, -0.5938,  0.9102, -0.0776], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 703])
attention_mask shape: torch.Size([4, 703])
reward: tensor([-0.0864, -0.7031,  0.8711, -1.9297], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1069])
attention_mask shape: torch.Size([4, 1069])
reward: tensor([-1.0234, -0.0889, -0.5859,  0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 570])
attention_mask shape: torch.Size([4, 570])
reward: tensor([-0.9141,  0.4414, -0.2246, -1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1147])
attention_mask shape: torch.Size([4, 1147])
reward: tensor([-0.2969, -0.1709,  0.3848, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1368])
attention_mask shape: torch.Size([4, 1368])
reward: tensor([-1.5000,  0.8750, -0.8203, -0.1514], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 994])
attention_mask shape: torch.Size([4, 994])
reward: tensor([-0.4980, -1.0078, -1.7344, -0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1536])
attention_mask shape: torch.Size([4, 1536])
reward: tensor([-1.1250, -0.2158,  1.2812,  0.2197], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1171])
attention_mask shape: torch.Size([4, 1171])
reward: tensor([-0.1445, -0.9141, -1.7344, -0.0022], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 517])
attention_mask shape: torch.Size([4, 517])
reward: tensor([-0.5195,  0.3242, -0.5938,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 949])
attention_mask shape: torch.Size([4, 949])
reward: tensor([ 0.6055, -0.8633, -0.8125, -1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 840])
attention_mask shape: torch.Size([4, 840])
reward: tensor([-0.0579,  0.5547, -1.3984,  0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 970])
attention_mask shape: torch.Size([4, 970])
reward: tensor([-0.5859, -0.5352, -0.5430, -0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1300])
attention_mask shape: torch.Size([4, 1300])
reward: tensor([-0.7227, -0.9258, -0.6094,  1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 763])
attention_mask shape: torch.Size([4, 763])
reward: tensor([ 1.3438, -0.2734,  0.2451, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1182])
attention_mask shape: torch.Size([4, 1182])
reward: tensor([-1.2656, -1.5078, -0.4004,  0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1638])
attention_mask shape: torch.Size([4, 1638])
reward: tensor([ 0.2490, -0.1426, -0.3828,  0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 794])
attention_mask shape: torch.Size([4, 794])
reward: tensor([ 0.1797, -1.0391, -0.5469, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 994])
attention_mask shape: torch.Size([4, 994])
reward: tensor([-1.0391, -0.2471,  0.0233, -0.6172], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1340])
attention_mask shape: torch.Size([4, 1340])
reward: tensor([-0.3281,  1.1719,  0.4004,  0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 315])
attention_mask shape: torch.Size([4, 315])
reward: tensor([-1.6406, -0.3828,  0.1338, -0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 898])
attention_mask shape: torch.Size([4, 898])
reward: tensor([-0.0400,  1.2109, -1.1172, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1475])
attention_mask shape: torch.Size([4, 1475])
reward: tensor([ 1.8828,  0.4043, -0.0067, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1202])
attention_mask shape: torch.Size([4, 1202])
reward: tensor([ 0.2852, -0.2227, -0.1553, -0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1254])
attention_mask shape: torch.Size([4, 1254])
reward: tensor([-0.4844, -0.3203,  1.2656,  0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 446])
attention_mask shape: torch.Size([4, 446])
reward: tensor([-0.3906, -1.6172, -0.6016, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 879])
attention_mask shape: torch.Size([4, 879])
reward: tensor([-0.9336, -0.4141,  1.6797, -1.8281], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 761])
attention_mask shape: torch.Size([4, 761])
reward: tensor([ 0.7695,  0.3730, -0.3906, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1227])
attention_mask shape: torch.Size([4, 1227])
reward: tensor([ 0.0300,  0.1885,  0.9062, -0.0801], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 500])
attention_mask shape: torch.Size([4, 500])
reward: tensor([-0.3770,  0.1914, -0.2676, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1329])
attention_mask shape: torch.Size([4, 1329])
reward: tensor([ 0.4199, -1.4453, -0.8086, -0.2158], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1183])
attention_mask shape: torch.Size([4, 1183])
reward: tensor([-0.2246, -0.6055, -0.8359, -0.7617], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 833])
attention_mask shape: torch.Size([4, 833])
reward: tensor([-0.1982,  0.0776, -0.1709, -0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2005])
attention_mask shape: torch.Size([4, 2005])
reward: tensor([-0.4570, -0.7305,  0.4609, -0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([ 1.1641, -0.6875,  0.8750, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1045])
attention_mask shape: torch.Size([4, 1045])
reward: tensor([ 1.6250, -0.6328,  1.2656, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 695])
attention_mask shape: torch.Size([4, 695])
reward: tensor([-0.7500,  1.6094, -0.0864,  1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1458])
attention_mask shape: torch.Size([4, 1458])
reward: tensor([ 1.1328,  0.6836, -1.1484, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 599])
attention_mask shape: torch.Size([4, 599])
reward: tensor([-0.0532, -0.4805,  1.1016, -0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1448])
attention_mask shape: torch.Size([4, 1448])
reward: tensor([ 0.4961,  0.8945, -0.4805,  1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1498])
attention_mask shape: torch.Size([4, 1498])
reward: tensor([ 0.0222, -0.2002,  1.3125, -1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 707])
attention_mask shape: torch.Size([4, 707])
reward: tensor([ 0.3203,  0.6836, -1.4453, -0.0732], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1288])
attention_mask shape: torch.Size([4, 1288])
reward: tensor([-0.6094, -0.3105,  0.7500, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-0.5898,  0.0845,  0.1641, -0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 790])
attention_mask shape: torch.Size([4, 790])
reward: tensor([-0.5156, -1.7266,  0.2334, -0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1132])
attention_mask shape: torch.Size([4, 1132])
reward: tensor([-0.0732, -0.0334,  1.0391, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 712])
attention_mask shape: torch.Size([4, 712])
reward: tensor([-0.5703, -0.6797, -0.7812, -0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 609])
attention_mask shape: torch.Size([4, 609])
reward: tensor([-0.7383, -0.2812, -0.4258,  0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 942])
attention_mask shape: torch.Size([4, 942])
reward: tensor([-0.1621, -0.3281, -0.6562, -0.0378], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1374])
attention_mask shape: torch.Size([4, 1374])
reward: tensor([ 1.0547, -0.4316, -0.1865,  0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 813])
attention_mask shape: torch.Size([4, 813])
reward: tensor([-1.4844, -0.9766,  1.3984, -0.1670], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([-0.7695, -0.8203, -0.3555,  1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([-1.5469,  0.8945, -1.4609,  0.1807], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 748])
attention_mask shape: torch.Size([4, 748])
reward: tensor([ 0.2490, -0.2578, -0.0067,  0.4883], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 585])
attention_mask shape: torch.Size([4, 585])
reward: tensor([-0.3203, -1.3281, -1.4219, -0.0732], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1948])
attention_mask shape: torch.Size([4, 1948])
reward: tensor([-0.6797, -0.7773,  1.0312,  1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1177])
attention_mask shape: torch.Size([4, 1177])
reward: tensor([ 0.0200, -0.6211, -0.1689,  0.6328], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 237])
attention_mask shape: torch.Size([4, 237])
reward: tensor([ 0.6797, -1.1875,  1.1719, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1537])
attention_mask shape: torch.Size([4, 1537])
reward: tensor([ 0.2051, -0.4941,  0.4805,  1.4297], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 678])
attention_mask shape: torch.Size([4, 678])
reward: tensor([-0.5273, -0.0356, -1.5000, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 989])
attention_mask shape: torch.Size([4, 989])
reward: tensor([-0.2930, -0.5195, -0.0400,  0.2734], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 538])
attention_mask shape: torch.Size([4, 538])
reward: tensor([-0.7188, -0.9336, -0.2871, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1236])
attention_mask shape: torch.Size([4, 1236])
reward: tensor([-0.6719, -0.0400,  0.3965, -0.1953], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 899])
attention_mask shape: torch.Size([4, 899])
reward: tensor([0.0889, 0.6836, 0.2334, 0.1621], device='cuda:0', dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 805])
attention_mask shape: torch.Size([4, 805])
reward: tensor([ 0.0500, -1.1641, -0.4629, -0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1445])
attention_mask shape: torch.Size([4, 1445])
reward: tensor([ 0.3555, -0.1953,  0.4629, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1055])
attention_mask shape: torch.Size([4, 1055])
reward: tensor([ 0.2275, -0.7617,  0.3340, -0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1683])
attention_mask shape: torch.Size([4, 1683])
reward: tensor([ 1.8125, -1.0234, -1.9219, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1420])
attention_mask shape: torch.Size([4, 1420])
reward: tensor([-1.0781,  0.7773,  1.5391, -0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 865])
attention_mask shape: torch.Size([4, 865])
reward: tensor([-0.9883,  0.1914,  0.5430, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1095])
attention_mask shape: torch.Size([4, 1095])
reward: tensor([ 1.3359, -0.1201, -0.9141, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 661])
attention_mask shape: torch.Size([4, 661])
reward: tensor([-1.4922,  0.6250, -0.5703,  0.1245], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1514])
attention_mask shape: torch.Size([4, 1514])
reward: tensor([ 0.2617, -0.2812, -0.4707, -0.2314], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1187])
attention_mask shape: torch.Size([4, 1187])
reward: tensor([ 0.2617, -0.0801,  0.3496, -0.1201], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1377])
attention_mask shape: torch.Size([4, 1377])
reward: tensor([-1.1797, -0.2373, -0.3555,  0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1406])
attention_mask shape: torch.Size([4, 1406])
reward: tensor([-0.1846,  0.6914,  0.4160, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1706])
attention_mask shape: torch.Size([4, 1706])
reward: tensor([-1.2812, -0.0557,  0.2314, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1001])
attention_mask shape: torch.Size([4, 1001])
reward: tensor([-0.2246, -0.6211, -0.2930, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1119])
attention_mask shape: torch.Size([4, 1119])
reward: tensor([-0.0732, -0.6523, -0.2197, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 479])
attention_mask shape: torch.Size([4, 479])
reward: tensor([ 0.5078, -0.9414, -0.5195, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 632])
attention_mask shape: torch.Size([4, 632])
reward: tensor([ 1.1484,  0.1045, -0.7930,  0.5234], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1384])
attention_mask shape: torch.Size([4, 1384])
reward: tensor([-0.4492, -0.4805, -0.7344,  0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1125])
attention_mask shape: torch.Size([4, 1125])
reward: tensor([-1.0547, -0.0488, -0.3828, -0.0820], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1415])
attention_mask shape: torch.Size([4, 1415])
reward: tensor([ 1.2969, -0.8594, -0.3164, -0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 682])
attention_mask shape: torch.Size([4, 682])
reward: tensor([-0.5547,  0.6914,  0.4883,  0.1729], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1658])
attention_mask shape: torch.Size([4, 1658])
reward: tensor([ 0.2441, -0.9062, -1.3750, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 587])
attention_mask shape: torch.Size([4, 587])
reward: tensor([-0.6172, -0.6328, -0.9414, -0.2021], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 978])
attention_mask shape: torch.Size([4, 978])
reward: tensor([-1.3594, -0.0510,  1.2656,  0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1226])
attention_mask shape: torch.Size([4, 1226])
reward: tensor([ 1.3828, -0.7656, -0.8320, -0.0378], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 800])
attention_mask shape: torch.Size([4, 800])
reward: tensor([ 0.5117, -1.6641, -1.5703, -0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 675])
attention_mask shape: torch.Size([4, 675])
reward: tensor([-0.5547, -0.7539, -0.1113,  0.0300], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1300])
attention_mask shape: torch.Size([4, 1300])
reward: tensor([ 0.2002, -1.6250, -1.0391, -0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1141])
attention_mask shape: torch.Size([4, 1141])
reward: tensor([-0.8633,  0.6406,  0.6484, -0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 731])
attention_mask shape: torch.Size([4, 731])
reward: tensor([ 0.9688, -0.1553, -0.7344, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 593])
attention_mask shape: torch.Size([4, 593])
reward: tensor([ 0.6680,  0.2188, -0.6016, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1183])
attention_mask shape: torch.Size([4, 1183])
reward: tensor([-0.9609, -0.7539, -1.8906,  0.0835], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 819])
attention_mask shape: torch.Size([4, 819])
reward: tensor([-0.3594, -1.3125, -0.0801,  1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-1.4609, -0.3242,  0.1533, -1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1291])
attention_mask shape: torch.Size([4, 1291])
reward: tensor([-1.4766,  1.8984, -0.5391,  0.3262], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 712])
attention_mask shape: torch.Size([4, 712])
reward: tensor([-1.2969, -1.2500, -1.1797,  1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 491])
attention_mask shape: torch.Size([4, 491])
reward: tensor([-0.2070, -1.0938,  0.0156, -0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 577])
attention_mask shape: torch.Size([4, 577])
reward: tensor([-0.6172, -1.1094, -0.7461,  0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1131])
attention_mask shape: torch.Size([4, 1131])
reward: tensor([ 0.0222, -0.6719, -0.7461,  1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1357])
attention_mask shape: torch.Size([4, 1357])
reward: tensor([-0.7852, -0.7969, -0.5430,  1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1748])
attention_mask shape: torch.Size([4, 1748])
reward: tensor([ 0.4961,  0.2090, -1.3359, -0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1325])
attention_mask shape: torch.Size([4, 1325])
reward: tensor([-0.0311,  0.8945,  0.7070, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 858])
attention_mask shape: torch.Size([4, 858])
reward: tensor([-0.8711, -0.1045, -0.2285, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 859])
attention_mask shape: torch.Size([4, 859])
reward: tensor([ 0.9258,  0.2070,  0.2949, -0.7617], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 827])
attention_mask shape: torch.Size([4, 827])
reward: tensor([-0.4746,  0.5469,  0.8008, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1073])
attention_mask shape: torch.Size([4, 1073])
reward: tensor([-0.1221, -0.4570, -0.7656,  0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 988])
attention_mask shape: torch.Size([4, 988])
reward: tensor([-0.1289,  0.0610, -2.0312, -0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1465])
attention_mask shape: torch.Size([4, 1465])
reward: tensor([ 0.6328,  0.2695,  0.6328, -0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1132])
attention_mask shape: torch.Size([4, 1132])
reward: tensor([ 1.1562, -1.1250, -0.2930, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 521])
attention_mask shape: torch.Size([4, 521])
reward: tensor([ 0.2637, -0.7109, -0.3867, -1.3047], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1247])
attention_mask shape: torch.Size([4, 1247])
reward: tensor([ 2.2812,  0.8008, -1.6016, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1380])
attention_mask shape: torch.Size([4, 1380])
reward: tensor([ 1.3125,  0.8477, -0.7305,  0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1267])
attention_mask shape: torch.Size([4, 1267])
reward: tensor([-0.8086,  0.4102, -1.6562,  1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1437])
attention_mask shape: torch.Size([4, 1437])
reward: tensor([ 1.3672, -0.1309,  1.5312, -0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 630])
attention_mask shape: torch.Size([4, 630])
reward: tensor([-0.7812, -1.5703, -0.1885,  1.6094], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1308])
attention_mask shape: torch.Size([4, 1308])
reward: tensor([-0.4043, -0.2539, -1.5625,  1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1091])
attention_mask shape: torch.Size([4, 1091])
reward: tensor([-1.1406, -0.1758,  0.0645,  0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1125])
attention_mask shape: torch.Size([4, 1125])
reward: tensor([-0.0510, -0.7734, -0.4746, -0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1882])
attention_mask shape: torch.Size([4, 1882])
reward: tensor([ 0.0432, -0.2334,  0.1865, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([-0.1377, -2.1406,  0.7383,  0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1311])
attention_mask shape: torch.Size([4, 1311])
reward: tensor([-0.9258,  0.4629,  0.8008, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1551])
attention_mask shape: torch.Size([4, 1551])
reward: tensor([-0.7656, -1.1562, -0.3691,  0.1709], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 934])
attention_mask shape: torch.Size([4, 934])
reward: tensor([-0.6211, -0.2021, -0.4355, -0.2578], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 623])
attention_mask shape: torch.Size([4, 623])
reward: tensor([-0.7344, -1.0703, -1.5391, -1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1367])
attention_mask shape: torch.Size([4, 1367])
reward: tensor([-0.4141, -0.4043, -1.9453,  0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 380])
attention_mask shape: torch.Size([4, 380])
reward: tensor([ 0.6992,  1.1172, -1.1406, -0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1552])
attention_mask shape: torch.Size([4, 1552])
reward: tensor([-1.5703, -0.4082,  0.3770, -0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1698])
attention_mask shape: torch.Size([4, 1698])
reward: tensor([-0.9062,  1.6406, -0.5195, -0.0820], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1213])
attention_mask shape: torch.Size([4, 1213])
reward: tensor([ 0.2471,  0.2617,  0.4473, -0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1700])
attention_mask shape: torch.Size([4, 1700])
reward: tensor([ 0.0410,  0.7422, -1.2734,  0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1963])
attention_mask shape: torch.Size([4, 1963])
reward: tensor([-0.5781,  0.0011,  0.9141, -0.6328], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 705])
attention_mask shape: torch.Size([4, 705])
reward: tensor([ 0.4746,  0.3652,  0.2559, -0.3164], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 607])
attention_mask shape: torch.Size([4, 607])
reward: tensor([-0.0889, -1.2891,  0.3535,  0.1143], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1332])
attention_mask shape: torch.Size([4, 1332])
reward: tensor([-2.2188, -0.1602,  0.9141, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1635])
attention_mask shape: torch.Size([4, 1635])
reward: tensor([-0.4805,  0.1187, -0.3066, -0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-0.9258, -0.4141, -0.0178, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1469])
attention_mask shape: torch.Size([4, 1469])
reward: tensor([-0.6797, -0.1826,  1.0703,  0.0977], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 627])
attention_mask shape: torch.Size([4, 627])
reward: tensor([-1.0078, -1.8047, -0.4316,  0.9023], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 926])
attention_mask shape: torch.Size([4, 926])
reward: tensor([-0.8086, -0.7812,  0.7969,  0.3164], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 569])
attention_mask shape: torch.Size([4, 569])
reward: tensor([-0.4258, -0.3652, -0.4355, -0.2402], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1127])
attention_mask shape: torch.Size([4, 1127])
reward: tensor([-0.4141, -0.5039, -0.3418, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1310])
attention_mask shape: torch.Size([4, 1310])
reward: tensor([-1.0547, -0.0244, -0.6055, -1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1121])
attention_mask shape: torch.Size([4, 1121])
reward: tensor([-0.0532, -0.4395, -0.3281, -0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 762])
attention_mask shape: torch.Size([4, 762])
reward: tensor([ 0.0200, -1.4141,  0.2812,  0.8008], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 793])
attention_mask shape: torch.Size([4, 793])
reward: tensor([-1.2734, -0.0045,  0.2812, -0.1309], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 973])
attention_mask shape: torch.Size([4, 973])
reward: tensor([ 0.1099, -1.7656, -2.1250,  0.1064], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 927])
attention_mask shape: torch.Size([4, 927])
reward: tensor([ 0.5781,  0.3613, -0.7109,  0.1387], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 475])
attention_mask shape: torch.Size([4, 475])
reward: tensor([ 0.1245,  0.3105, -0.4629, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1132])
attention_mask shape: torch.Size([4, 1132])
reward: tensor([-0.7344, -1.0547,  1.0078,  0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1055])
attention_mask shape: torch.Size([4, 1055])
reward: tensor([ 0.1045, -0.7461, -0.3730, -0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1297])
attention_mask shape: torch.Size([4, 1297])
reward: tensor([ 1.1562,  0.4570, -0.7812,  0.2158], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1371])
attention_mask shape: torch.Size([4, 1371])
reward: tensor([-1.9375, -1.2031, -0.3730,  0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 776])
attention_mask shape: torch.Size([4, 776])
reward: tensor([-0.1396, -0.4668, -1.5234, -0.2373], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1133])
attention_mask shape: torch.Size([4, 1133])
reward: tensor([-0.7852, -0.3340, -2.0469, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-0.9062, -1.4453,  1.1328,  0.2832], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 803])
attention_mask shape: torch.Size([4, 803])
reward: tensor([-1.1875, -1.1875, -1.0781, -1.5234], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1905])
attention_mask shape: torch.Size([4, 1905])
reward: tensor([0.2178, 1.0312, 0.1865, 1.3047], device='cuda:0', dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1375])
attention_mask shape: torch.Size([4, 1375])
reward: tensor([ 0.8828,  0.3828, -1.0234, -0.2852], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([-0.8047, -0.5703,  1.3125, -1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 666])
attention_mask shape: torch.Size([4, 666])
reward: tensor([-0.6406,  1.1016,  0.3398, -0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1238])
attention_mask shape: torch.Size([4, 1238])
reward: tensor([-0.2598, -0.1582,  0.4746,  0.3164], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1155])
attention_mask shape: torch.Size([4, 1155])
reward: tensor([-0.5078,  0.1689,  0.1279, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 679])
attention_mask shape: torch.Size([4, 679])
reward: tensor([ 0.5234, -1.0469, -1.3672,  1.1328], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1037])
attention_mask shape: torch.Size([4, 1037])
reward: tensor([ 0.3008,  0.0864, -1.4297, -0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1524])
attention_mask shape: torch.Size([4, 1524])
reward: tensor([ 0.0732,  0.3027, -0.3730,  1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1189])
attention_mask shape: torch.Size([4, 1189])
reward: tensor([-0.7656, -0.2002, -1.6250, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 637])
attention_mask shape: torch.Size([4, 637])
reward: tensor([-0.0688, -0.4453, -1.2812, -0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1222])
attention_mask shape: torch.Size([4, 1222])
reward: tensor([ 1.6406, -0.6133,  0.1279, -0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1312])
attention_mask shape: torch.Size([4, 1312])
reward: tensor([ 0.8320, -1.2188,  0.1123, -0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 488])
attention_mask shape: torch.Size([4, 488])
reward: tensor([-1.0703, -0.8281,  0.0444,  0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1368])
attention_mask shape: torch.Size([4, 1368])
reward: tensor([-1.0938,  0.4629,  0.2021,  1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 800])
attention_mask shape: torch.Size([4, 800])
reward: tensor([ 0.1709, -1.8125, -1.1875,  0.9922], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1181])
attention_mask shape: torch.Size([4, 1181])
reward: tensor([ 0.3262, -1.5234, -1.0469, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1167])
attention_mask shape: torch.Size([4, 1167])
reward: tensor([ 0.1475, -1.2734, -0.8008, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1223])
attention_mask shape: torch.Size([4, 1223])
reward: tensor([ 0.1768, -1.1484,  2.5625, -0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1539])
attention_mask shape: torch.Size([4, 1539])
reward: tensor([ 2.0469, -0.1221,  0.0933,  0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 771])
attention_mask shape: torch.Size([4, 771])
reward: tensor([-0.3594, -1.7031, -0.1133,  1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1440])
attention_mask shape: torch.Size([4, 1440])
reward: tensor([-0.0864, -0.4707, -0.8086,  0.2617], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 701])
attention_mask shape: torch.Size([4, 701])
reward: tensor([ 0.0457, -1.2891,  0.1484,  0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1210])
attention_mask shape: torch.Size([4, 1210])
reward: tensor([-0.6328, -0.1982,  0.4355, -0.8398], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1236])
attention_mask shape: torch.Size([4, 1236])
reward: tensor([ 0.8789, -1.0391, -0.3340,  0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 859])
attention_mask shape: torch.Size([4, 859])
reward: tensor([ 0.7031, -2.0781,  0.5391, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1006])
attention_mask shape: torch.Size([4, 1006])
reward: tensor([-0.1709,  1.5469,  1.1016, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1564])
attention_mask shape: torch.Size([4, 1564])
reward: tensor([-0.5273, -0.9609, -0.9609, -0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1252])
attention_mask shape: torch.Size([4, 1252])
reward: tensor([ 0.0532, -0.2441, -0.8438,  0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1378])
attention_mask shape: torch.Size([4, 1378])
reward: tensor([ 1.1719,  0.2021, -1.3516,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1498])
attention_mask shape: torch.Size([4, 1498])
reward: tensor([-0.5781,  0.2949,  1.4609, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 810])
attention_mask shape: torch.Size([4, 810])
reward: tensor([-0.6914,  0.0466, -1.0938,  0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1028])
attention_mask shape: torch.Size([4, 1028])
reward: tensor([ 0.5547, -0.3594,  0.9219, -0.0820], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 878])
attention_mask shape: torch.Size([4, 878])
reward: tensor([-0.2178, -0.9414, -1.0156, -0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1240])
attention_mask shape: torch.Size([4, 1240])
reward: tensor([-0.8711, -0.8711,  0.0654,  0.0422], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1152])
attention_mask shape: torch.Size([4, 1152])
reward: tensor([ 0.2891, -0.3770, -0.8359,  0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 446])
attention_mask shape: torch.Size([4, 446])
reward: tensor([-0.5625,  0.7227, -0.4844,  0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1147])
attention_mask shape: torch.Size([4, 1147])
reward: tensor([-1.1406,  0.4043, -0.6875, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 565])
attention_mask shape: torch.Size([4, 565])
reward: tensor([-0.6836, -0.4570,  0.3496, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([-0.1602, -2.0156, -0.1641, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1516])
attention_mask shape: torch.Size([4, 1516])
reward: tensor([ 1.5625,  0.0200,  1.5625, -0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1194])
attention_mask shape: torch.Size([4, 1194])
reward: tensor([-0.0444, -0.6797,  0.2393,  0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-24 20:14:09,179] [INFO] [launch.py:351:main] Process 745529 exits successfully.
[2024-10-24 20:14:16,186] [INFO] [launch.py:351:main] Process 745531 exits successfully.
[2024-10-24 20:14:49,219] [INFO] [launch.py:351:main] Process 745530 exits successfully.
[2024-10-24 20:16:10,302] [INFO] [launch.py:351:main] Process 745532 exits successfully.
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# [K(base) root@autodl-container-ec234bbd2e-925c6d34:~# bash run_eval_reward_openrlhf.sj[Kh
[?2004l+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-9th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-9th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_9 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-26 23:51:03,439] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-26 23:51:05,306] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-26 23:51:05,307] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-9th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_9 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-26 23:51:07,584] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-26 23:51:10,467] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-26 23:51:10,467] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-26 23:51:10,467] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-26 23:51:10,467] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-26 23:51:10,467] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-26 23:51:10,468] [INFO] [launch.py:256:main] process 768613 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-9th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_9', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-26 23:51:10,468] [INFO] [launch.py:256:main] process 768614 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-9th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_9', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-26 23:51:10,469] [INFO] [launch.py:256:main] process 768615 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-9th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_9', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-26 23:51:10,469] [INFO] [launch.py:256:main] process 768616 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-9th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_9', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-26 23:51:11,908] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-26 23:51:12,015] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-26 23:51:12,045] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-26 23:51:12,048] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-26 23:51:14,778] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-26 23:51:14,778] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-26 23:51:15,298] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-26 23:51:15,299] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-26 23:51:15,307] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.24it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.36it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.24it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.99it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.20it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.37it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.26it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  8.31it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.37it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.27it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.20it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  8.53it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  9.02it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.76it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.60it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.51it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.49it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.40it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.33it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.28it/s]
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-26 23:51:37,272] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-26 23:51:37,273] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-26 23:51:37,273] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-26 23:51:37,365] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-26 23:51:37,390] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-26 23:51:37,449] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-26 23:51:39,278] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-26 23:51:39,279] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-26 23:51:39,280] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-26 23:51:39,280] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-26 23:51:39,281] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-26 23:51:39,445] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-26 23:51:39,445] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-26 23:51:39,446] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 33.51 GB, percent = 3.3%
[2024-10-26 23:51:39,630] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-26 23:51:39,631] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-26 23:51:39,631] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 33.51 GB, percent = 3.3%
[2024-10-26 23:51:39,632] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efd01722770>
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-26 23:51:39,633] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-26 23:51:39,634] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-26 23:51:39,635] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-26 23:51:39,635] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-26 23:51:39,635] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-26 23:51:39,635] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-26 23:51:39,635] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-26 23:51:39,635] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-26 23:51:39,635] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-26 23:51:39,635] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-26 23:51:39,635] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-26 23:51:39,635] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-26 23:51:39,635] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-26 23:51:39,635] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-26 23:51:42,640] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-26 23:51:42,642] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-26 23:51:42,776] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-26 23:51:42,776] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-26 23:51:42,776] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 33.52 GB, percent = 3.3%
[2024-10-26 23:51:42,880] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-26 23:51:42,880] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-26 23:51:42,880] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 33.52 GB, percent = 3.3%
[2024-10-26 23:51:42,882] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efcf8f3a860>
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-26 23:51:42,882] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-26 23:51:42,883] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-26 23:51:42,884] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-26 23:51:42,884] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 654/100000 [00:00<00:15, 6534.09it/s]Preprocessing data:   2%|█▌                                                                                          | 1688/100000 [00:00<00:11, 8767.90it/s]Preprocessing data:   3%|██▌                                                                                         | 2728/100000 [00:00<00:10, 9512.03it/s]Preprocessing data:   4%|███▍                                                                                        | 3757/100000 [00:00<00:09, 9816.27it/s]Preprocessing data:   5%|████▍                                                                                       | 4787/100000 [00:00<00:09, 9989.27it/s]Preprocessing data:   6%|█████▎                                                                                     | 5816/100000 [00:00<00:09, 10090.00it/s]Preprocessing data:   7%|██████▏                                                                                    | 6845/100000 [00:00<00:09, 10154.60it/s]Preprocessing data:   8%|███████▏                                                                                   | 7887/100000 [00:00<00:08, 10237.75it/s]Preprocessing data:   9%|████████                                                                                   | 8926/100000 [00:00<00:08, 10282.58it/s]Preprocessing data:  10%|█████████                                                                                  | 9969/100000 [00:01<00:08, 10327.76it/s]Preprocessing data:  11%|█████████▉                                                                                | 11055/100000 [00:01<00:08, 10487.94it/s]Preprocessing data:  12%|██████████▉                                                                               | 12135/100000 [00:01<00:08, 10580.91it/s]Preprocessing data:  13%|███████████▉                                                                              | 13213/100000 [00:01<00:08, 10639.15it/s]Preprocessing data:  14%|████████████▊                                                                             | 14286/100000 [00:01<00:08, 10665.45it/s]Preprocessing data:  15%|█████████████▊                                                                            | 15368/100000 [00:01<00:07, 10710.07it/s]Preprocessing data:  16%|██████████████▊                                                                           | 16440/100000 [00:01<00:07, 10712.56it/s]Preprocessing data:  18%|███████████████▊                                                                          | 17520/100000 [00:01<00:07, 10738.29it/s]Preprocessing data:  19%|████████████████▋                                                                         | 18594/100000 [00:01<00:07, 10664.25it/s]Preprocessing data:  20%|█████████████████▋                                                                        | 19679/100000 [00:01<00:07, 10717.70it/s]Preprocessing data:  21%|██████████████████▋                                                                       | 20777/100000 [00:02<00:07, 10795.42it/s]Preprocessing data:  22%|███████████████████▋                                                                      | 21863/100000 [00:02<00:07, 10812.20it/s]Preprocessing data:  23%|████████████████████▋                                                                     | 22945/100000 [00:02<00:07, 10776.83it/s]Preprocessing data:  24%|█████████████████████▌                                                                    | 24023/100000 [00:02<00:07, 10742.83it/s]Preprocessing data:  25%|██████████████████████▌                                                                   | 25098/100000 [00:02<00:06, 10702.71it/s]Preprocessing data:  26%|███████████████████████▌                                                                  | 26169/100000 [00:02<00:06, 10675.57it/s]Preprocessing data:  27%|████████████████████████▌                                                                 | 27237/100000 [00:02<00:06, 10534.96it/s]Preprocessing data:  28%|█████████████████████████▍                                                                | 28291/100000 [00:02<00:06, 10483.23it/s]Preprocessing data:  29%|██████████████████████████▍                                                               | 29340/100000 [00:02<00:06, 10434.77it/s]Preprocessing data:  30%|███████████████████████████▎                                                              | 30384/100000 [00:02<00:06, 10412.42it/s]Preprocessing data:  31%|████████████████████████████▎                                                             | 31426/100000 [00:03<00:06, 10411.18it/s]Preprocessing data:  32%|█████████████████████████████▏                                                            | 32468/100000 [00:03<00:06, 10379.23it/s]Preprocessing data:  34%|██████████████████████████████▏                                                           | 33506/100000 [00:03<00:06, 10376.09it/s]Preprocessing data:  35%|███████████████████████████████                                                           | 34544/100000 [00:03<00:06, 10366.72it/s]Preprocessing data:  36%|████████████████████████████████                                                          | 35581/100000 [00:03<00:06, 10349.18it/s]Preprocessing data:  37%|████████████████████████████████▉                                                         | 36616/100000 [00:03<00:06, 10227.70it/s]Preprocessing data:  38%|█████████████████████████████████▉                                                        | 37640/100000 [00:03<00:06, 10230.68it/s]Preprocessing data:  39%|██████████████████████████████████▊                                                       | 38674/100000 [00:03<00:05, 10262.46it/s]Preprocessing data:  40%|███████████████████████████████████▋                                                      | 39709/100000 [00:03<00:05, 10286.22it/s]Preprocessing data:  41%|████████████████████████████████████▋                                                     | 40751/100000 [00:03<00:05, 10326.02it/s]Preprocessing data:  42%|█████████████████████████████████████▌                                                    | 41787/100000 [00:04<00:05, 10335.00it/s]Preprocessing data:  43%|██████████████████████████████████████▌                                                   | 42821/100000 [00:04<00:05, 10278.57it/s]Preprocessing data:  44%|███████████████████████████████████████▍                                                  | 43880/100000 [00:04<00:05, 10370.43it/s]Preprocessing data:  45%|████████████████████████████████████████▍                                                 | 44944/100000 [00:04<00:05, 10449.52it/s]Preprocessing data:  46%|█████████████████████████████████████████▍                                                | 45990/100000 [00:04<00:05, 10412.12it/s]Preprocessing data:  47%|██████████████████████████████████████████▎                                               | 47054/100000 [00:04<00:05, 10479.10it/s]Preprocessing data:  48%|███████████████████████████████████████████▎                                              | 48118/100000 [00:04<00:04, 10527.11it/s]Preprocessing data:  49%|████████████████████████████████████████████▎                                             | 49183/100000 [00:04<00:04, 10562.66it/s]Preprocessing data:  50%|█████████████████████████████████████████████▏                                            | 50249/100000 [00:04<00:04, 10590.29it/s]Preprocessing data:  51%|██████████████████████████████████████████████▏                                           | 51313/100000 [00:04<00:04, 10603.69it/s]Preprocessing data:  52%|███████████████████████████████████████████████▏                                          | 52374/100000 [00:05<00:04, 10575.10it/s]Preprocessing data:  53%|████████████████████████████████████████████████                                          | 53441/100000 [00:05<00:04, 10602.18it/s]Preprocessing data:  55%|█████████████████████████████████████████████████                                         | 54506/100000 [00:05<00:04, 10613.42it/s]Preprocessing data:  56%|██████████████████████████████████████████████████                                        | 55568/100000 [00:05<00:04, 10595.10it/s]Preprocessing data:  57%|██████████████████████████████████████████████████▉                                       | 56628/100000 [00:05<00:04, 10486.06it/s]Preprocessing data:  58%|███████████████████████████████████████████████████▉                                      | 57681/100000 [00:05<00:04, 10495.62it/s]Preprocessing data:  59%|████████████████████████████████████████████████████▊                                     | 58747/100000 [00:05<00:03, 10541.75it/s]Preprocessing data:  60%|█████████████████████████████████████████████████████▊                                    | 59812/100000 [00:05<00:03, 10571.15it/s]Preprocessing data:  61%|██████████████████████████████████████████████████████▊                                   | 60870/100000 [00:05<00:03, 10573.28it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▋                                  | 61930/100000 [00:05<00:03, 10580.94it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▋                                 | 63002/100000 [00:06<00:03, 10621.95it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▋                                | 64065/100000 [00:06<00:03, 10613.42it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▌                               | 65127/100000 [00:06<00:03, 10608.19it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▌                              | 66188/100000 [00:06<00:03, 10597.55it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████▌                             | 67249/100000 [00:06<00:03, 10599.96it/s]Preprocessing data:  68%|█████████████████████████████████████████████████████████████▍                            | 68310/100000 [00:06<00:02, 10593.62it/s]Preprocessing data:  69%|██████████████████████████████████████████████████████████████▍                           | 69370/100000 [00:06<00:02, 10529.75it/s]Preprocessing data:  70%|███████████████████████████████████████████████████████████████▍                          | 70424/100000 [00:06<00:02, 10515.68it/s]Preprocessing data:  71%|████████████████████████████████████████████████████████████████▎                         | 71478/100000 [00:06<00:02, 10522.70it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▎                        | 72531/100000 [00:06<00:02, 10473.88it/s]Preprocessing data:  74%|██████████████████████████████████████████████████████████████████▉                        | 73579/100000 [00:07<00:02, 9461.50it/s]Preprocessing data:  75%|███████████████████████████████████████████████████████████████████▊                       | 74544/100000 [00:07<00:02, 8938.25it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▋                      | 75455/100000 [00:07<00:02, 8618.52it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████▍                     | 76329/100000 [00:07<00:02, 8390.29it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▏                    | 77176/100000 [00:07<00:02, 8219.57it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▉                    | 78003/100000 [00:07<00:02, 8124.70it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▋                   | 78819/100000 [00:07<00:02, 8055.82it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▍                  | 79627/100000 [00:07<00:02, 7996.57it/s]Preprocessing data:  80%|█████████████████████████████████████████████████████████████████████████▏                 | 80428/100000 [00:07<00:02, 7739.51it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▉                 | 81308/100000 [00:08<00:02, 8039.32it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▊                | 82207/100000 [00:08<00:02, 8311.51it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▌               | 83072/100000 [00:08<00:02, 8408.04it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▎              | 83916/100000 [00:08<00:02, 8026.81it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▎             | 84918/100000 [00:08<00:01, 8595.36it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▏            | 85918/100000 [00:08<00:01, 9002.61it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████            | 86824/100000 [00:08<00:01, 8513.16it/s]Preprocessing data:  88%|███████████████████████████████████████████████████████████████████████████████▊           | 87765/100000 [00:08<00:01, 8766.31it/s]Preprocessing data:  89%|████████████████████████████████████████████████████████████████████████████████▊          | 88758/100000 [00:08<00:01, 9100.33it/s]Preprocessing data:  90%|█████████████████████████████████████████████████████████████████████████████████▋         | 89745/100000 [00:08<00:01, 9322.55it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▌        | 90683/100000 [00:09<00:01, 8799.68it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▎       | 91573/100000 [00:09<00:01, 8240.09it/s]Preprocessing data:  92%|████████████████████████████████████████████████████████████████████████████████████       | 92409/100000 [00:09<00:00, 7866.95it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▊      | 93206/100000 [00:09<00:00, 7626.60it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▌     | 93975/100000 [00:09<00:00, 7600.49it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▍    | 94961/100000 [00:09<00:00, 8229.83it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▎   | 95933/100000 [00:09<00:00, 8653.50it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████   | 96806/100000 [00:09<00:00, 8485.07it/s]Preprocessing data:  98%|████████████████████████████████████████████████████████████████████████████████████████▉  | 97695/100000 [00:09<00:00, 8600.99it/s]Preprocessing data:  99%|█████████████████████████████████████████████████████████████████████████████████████████▊ | 98638/100000 [00:10<00:00, 8841.80it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████▋| 99638/100000 [00:10<00:00, 9179.98it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9787.38it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1659])
attention_mask shape: torch.Size([4, 1659])
reward: tensor([-0.0067, -0.6836, -0.3555, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1624])
attention_mask shape: torch.Size([4, 1624])
reward: tensor([ 1.8984, -0.7656,  0.6836, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1081])
attention_mask shape: torch.Size([4, 1081])
reward: tensor([-1.5547, -1.0078, -0.2070, -0.5156], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 420])
attention_mask shape: torch.Size([4, 420])
reward: tensor([ 0.1484, -1.9375,  0.2236,  0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1501])
attention_mask shape: torch.Size([4, 1501])
reward: tensor([ 0.9922,  0.0991, -0.4805, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 560])
attention_mask shape: torch.Size([4, 560])
reward: tensor([-0.7188, -0.8281, -0.0244,  0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1200])
attention_mask shape: torch.Size([4, 1200])
reward: tensor([-0.3555, -0.6562, -2.0156, -0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1020])
attention_mask shape: torch.Size([4, 1020])
reward: tensor([ 0.5703, -0.8438,  0.1445, -0.1553], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1066])
attention_mask shape: torch.Size([4, 1066])
reward: tensor([-0.6680,  0.9688,  0.0767, -0.2441], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1059])
attention_mask shape: torch.Size([4, 1059])
reward: tensor([ 0.3164,  1.4375,  0.0300, -0.3906], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 841])
attention_mask shape: torch.Size([4, 841])
reward: tensor([-1.1094,  0.0669,  0.7461, -0.3516], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 752])
attention_mask shape: torch.Size([4, 752])
reward: tensor([ 0.8008, -0.9688, -0.0510,  0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 811])
attention_mask shape: torch.Size([4, 811])
reward: tensor([-1.4297, -0.8711, -0.5430, -0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1562])
attention_mask shape: torch.Size([4, 1562])
reward: tensor([-0.4316,  0.1079,  0.3457,  0.0688], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 575])
attention_mask shape: torch.Size([4, 575])
reward: tensor([-0.2354,  1.2422, -0.0178,  0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1504])
attention_mask shape: torch.Size([4, 1504])
reward: tensor([ 0.6484, -0.5508, -0.2246, -1.5703], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1381])
attention_mask shape: torch.Size([4, 1381])
reward: tensor([-1.1250, -2.0312, -0.1582,  0.8672], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1473])
attention_mask shape: torch.Size([4, 1473])
reward: tensor([ 0.6836,  0.1875,  0.0864, -1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 826])
attention_mask shape: torch.Size([4, 826])
reward: tensor([ 0.8516,  0.1533, -1.9219, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 643])
attention_mask shape: torch.Size([4, 643])
reward: tensor([ 0.1885, -0.2910, -0.3594, -0.4453], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 839])
attention_mask shape: torch.Size([4, 839])
reward: tensor([-0.2002,  0.6562,  0.0211, -0.0045], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1118])
attention_mask shape: torch.Size([4, 1118])
reward: tensor([-0.0776,  1.0234, -0.0356, -0.4629], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 919])
attention_mask shape: torch.Size([4, 919])
reward: tensor([ 0.2695,  0.4316, -0.0776, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1400])
attention_mask shape: torch.Size([4, 1400])
reward: tensor([ 0.6836,  0.4531, -0.2773,  1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1406])
attention_mask shape: torch.Size([4, 1406])
reward: tensor([ 1.5000,  0.0388,  0.0255, -0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1566])
attention_mask shape: torch.Size([4, 1566])
reward: tensor([ 0.3555, -0.7031,  0.6406, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1512])
attention_mask shape: torch.Size([4, 1512])
reward: tensor([-0.6680,  0.3418,  0.2256, -1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 633])
attention_mask shape: torch.Size([4, 633])
reward: tensor([ 0.1719, -0.4570, -0.5234, -0.6719], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 892])
attention_mask shape: torch.Size([4, 892])
reward: tensor([ 0.3926, -0.8984, -0.2246, -0.2285], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1694])
attention_mask shape: torch.Size([4, 1694])
reward: tensor([-0.2578,  0.3242,  0.0378, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1390])
attention_mask shape: torch.Size([4, 1390])
reward: tensor([-0.6094,  0.4180, -0.0156, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 686])
attention_mask shape: torch.Size([4, 686])
reward: tensor([-1.1875,  0.2041, -0.8984, -0.1641], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1461])
attention_mask shape: torch.Size([4, 1461])
reward: tensor([-1.0391, -0.1377,  0.7148, -1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1225])
attention_mask shape: torch.Size([4, 1225])
reward: tensor([ 0.4355,  0.0300, -1.4922,  0.0145], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 902])
attention_mask shape: torch.Size([4, 902])
reward: tensor([-0.3906,  0.7422, -0.3164, -0.0244], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1689])
attention_mask shape: torch.Size([4, 1689])
reward: tensor([ 0.6484,  1.6094,  0.4883, -1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1117])
attention_mask shape: torch.Size([4, 1117])
reward: tensor([ 0.2695, -0.5898, -0.4004,  0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1188])
attention_mask shape: torch.Size([4, 1188])
reward: tensor([ 0.3691,  1.3047, -0.2129, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1154])
attention_mask shape: torch.Size([4, 1154])
reward: tensor([-0.1221,  0.4668,  1.7422, -1.6250], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1345])
attention_mask shape: torch.Size([4, 1345])
reward: tensor([ 0.2793,  0.4805, -0.5469,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1271])
attention_mask shape: torch.Size([4, 1271])
reward: tensor([ 0.0654,  1.1016,  0.7773, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1132])
attention_mask shape: torch.Size([4, 1132])
reward: tensor([ 0.6758,  1.3672, -0.7422,  0.0723], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 317])
attention_mask shape: torch.Size([4, 317])
reward: tensor([-1.0312, -0.8633,  0.3066, -0.5742], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 937])
attention_mask shape: torch.Size([4, 937])
reward: tensor([-1.4609, -0.5195, -0.6719, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1754])
attention_mask shape: torch.Size([4, 1754])
reward: tensor([-1.0391,  1.8750,  0.8945,  1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 767])
attention_mask shape: torch.Size([4, 767])
reward: tensor([-1.1875, -0.3965, -0.1934, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 876])
attention_mask shape: torch.Size([4, 876])
reward: tensor([-0.2129,  0.0488,  1.4141, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1933])
attention_mask shape: torch.Size([4, 1933])
reward: tensor([ 0.0654,  0.5117, -0.2334,  0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([-1.1719e+00,  1.1139e-03, -2.0469e+00,  9.5703e-02], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 875])
attention_mask shape: torch.Size([4, 875])
reward: tensor([-0.9883, -0.3965,  0.0820,  0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1309])
attention_mask shape: torch.Size([4, 1309])
reward: tensor([-0.9766, -0.4668,  0.0510, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1191])
attention_mask shape: torch.Size([4, 1191])
reward: tensor([-0.9336,  1.5938, -0.2314,  0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([-0.1245, -1.2344,  0.8164, -1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1222])
attention_mask shape: torch.Size([4, 1222])
reward: tensor([-1.8984, -1.4688,  0.1553,  0.1201], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1453])
attention_mask shape: torch.Size([4, 1453])
reward: tensor([ 0.1309, -0.3770,  0.1709, -0.2471], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-0.3652, -0.8086,  0.9766, -1.6328], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1016])
attention_mask shape: torch.Size([4, 1016])
reward: tensor([-1.4375, -0.5938, -2.0156, -0.0845], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 594])
attention_mask shape: torch.Size([4, 594])
reward: tensor([-0.8125,  0.0111,  0.2109, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1269])
attention_mask shape: torch.Size([4, 1269])
reward: tensor([ 0.1914,  0.0011, -0.1602, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1432])
attention_mask shape: torch.Size([4, 1432])
reward: tensor([-0.2520,  1.0938, -0.4355, -0.0845], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1010])
attention_mask shape: torch.Size([4, 1010])
reward: tensor([-0.3516, -1.4375, -1.0391, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1562])
attention_mask shape: torch.Size([4, 1562])
reward: tensor([-1.5078, -0.3906,  1.2812,  0.7305], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1434])
attention_mask shape: torch.Size([4, 1434])
reward: tensor([-1.6094, -0.8633, -1.3281,  0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 666])
attention_mask shape: torch.Size([4, 666])
reward: tensor([ 0.7461,  0.1367, -0.2930, -0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1381])
attention_mask shape: torch.Size([4, 1381])
reward: tensor([ 0.8750, -0.1846,  1.2812, -0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 718])
attention_mask shape: torch.Size([4, 718])
reward: tensor([ 0.3105,  0.7305, -0.5703, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 647])
attention_mask shape: torch.Size([4, 647])
reward: tensor([-0.3242, -0.4043, -0.0444, -0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([-0.9883, -1.3594,  0.0500,  1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1115])
attention_mask shape: torch.Size([4, 1115])
reward: tensor([ 1.6953, -0.1045,  1.1562, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1320])
attention_mask shape: torch.Size([4, 1320])
reward: tensor([-0.1670, -0.1934, -2.0781,  0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1303])
attention_mask shape: torch.Size([4, 1303])
reward: tensor([-0.5820, -0.4531, -0.3770, -0.1582], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1167])
attention_mask shape: torch.Size([4, 1167])
reward: tensor([-0.3418, -0.8711, -0.9258, -1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1005])
attention_mask shape: torch.Size([4, 1005])
reward: tensor([-0.5039,  0.7656,  0.2949,  0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1172])
attention_mask shape: torch.Size([4, 1172])
reward: tensor([-0.0488,  1.2188,  0.2148, -0.4844], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 300])
attention_mask shape: torch.Size([4, 300])
reward: tensor([-1.4922, -0.5859, -1.3438, -0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 784])
attention_mask shape: torch.Size([4, 784])
reward: tensor([ 0.5547, -0.4883,  0.9805, -1.8047], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1113])
attention_mask shape: torch.Size([4, 1113])
reward: tensor([ 1.4141, -0.8711, -1.4609, -0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1188])
attention_mask shape: torch.Size([4, 1188])
reward: tensor([ 1.0859, -0.5625, -0.5469, -0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1405])
attention_mask shape: torch.Size([4, 1405])
reward: tensor([-0.0713,  0.6055,  0.5625,  0.1309], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 582])
attention_mask shape: torch.Size([4, 582])
reward: tensor([-0.5625, -2.1094,  0.1641, -1.4922], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1036])
attention_mask shape: torch.Size([4, 1036])
reward: tensor([-0.3828,  0.1484,  1.7969, -0.1377], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1129])
attention_mask shape: torch.Size([4, 1129])
reward: tensor([ 2.1719, -0.0356, -2.1562, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1297])
attention_mask shape: torch.Size([4, 1297])
reward: tensor([-0.0378,  0.1641,  0.2188, -0.0579], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 761])
attention_mask shape: torch.Size([4, 761])
reward: tensor([-0.0422, -0.5156, -0.4453,  0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1640])
attention_mask shape: torch.Size([4, 1640])
reward: tensor([-0.1445, -0.1289, -0.1865, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1504])
attention_mask shape: torch.Size([4, 1504])
reward: tensor([-0.7383, -0.6328, -0.0688,  1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1269])
attention_mask shape: torch.Size([4, 1269])
reward: tensor([-0.2812,  0.1133,  1.0859,  1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1726])
attention_mask shape: torch.Size([4, 1726])
reward: tensor([-0.3730,  1.1250,  0.2021, -0.2354], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1445])
attention_mask shape: torch.Size([4, 1445])
reward: tensor([ 0.3262, -1.1641,  0.6328, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 531])
attention_mask shape: torch.Size([4, 531])
reward: tensor([ 0.2363, -0.7617,  0.6133, -0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 793])
attention_mask shape: torch.Size([4, 793])
reward: tensor([-0.7852,  1.0312, -0.8203,  0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1525])
attention_mask shape: torch.Size([4, 1525])
reward: tensor([ 1.4062,  0.8828, -0.3906, -1.2344], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 669])
attention_mask shape: torch.Size([4, 669])
reward: tensor([0.1826, 0.7188, 0.9727, 0.6133], device='cuda:0', dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1821])
attention_mask shape: torch.Size([4, 1821])
reward: tensor([ 1.2109,  0.5273, -0.2246,  0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1221])
attention_mask shape: torch.Size([4, 1221])
reward: tensor([-1.0859, -1.1250, -0.2490, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 756])
attention_mask shape: torch.Size([4, 756])
reward: tensor([-0.6914,  0.3691, -0.8594,  0.4160], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1347])
attention_mask shape: torch.Size([4, 1347])
reward: tensor([-0.6094,  0.3457,  0.5703, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1010])
attention_mask shape: torch.Size([4, 1010])
reward: tensor([-0.8477, -0.1533,  2.1562, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([-0.6172, -0.0889,  0.5781, -0.5156], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1230])
attention_mask shape: torch.Size([4, 1230])
reward: tensor([ 0.3184, -0.0334,  1.4609, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1257])
attention_mask shape: torch.Size([4, 1257])
reward: tensor([-1.4219, -0.7461, -0.2969, -0.2734], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 473])
attention_mask shape: torch.Size([4, 473])
reward: tensor([-0.9961, -1.1016,  0.8203, -0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1275])
attention_mask shape: torch.Size([4, 1275])
reward: tensor([ 0.1982,  0.3105, -0.0011,  0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1176])
attention_mask shape: torch.Size([4, 1176])
reward: tensor([ 0.1553, -0.5820, -0.2520,  0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([-0.5820, -0.6367,  1.2969, -1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1440])
attention_mask shape: torch.Size([4, 1440])
reward: tensor([-0.6172,  0.6094, -0.2285,  1.6328], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1225])
attention_mask shape: torch.Size([4, 1225])
reward: tensor([-1.5078,  0.2910, -1.8125,  0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 879])
attention_mask shape: torch.Size([4, 879])
reward: tensor([-1.7578, -0.1089, -0.3770, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 494])
attention_mask shape: torch.Size([4, 494])
reward: tensor([-1.0391, -0.3281,  0.0776,  0.1621], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1303])
attention_mask shape: torch.Size([4, 1303])
reward: tensor([-1.9922, -0.7656,  0.0957, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1305])
attention_mask shape: torch.Size([4, 1305])
reward: tensor([ 0.2812, -0.1934,  0.9883, -0.3906], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 277])
attention_mask shape: torch.Size([4, 277])
reward: tensor([ 0.8086, -1.0234,  1.1016, -0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1402])
attention_mask shape: torch.Size([4, 1402])
reward: tensor([-0.4219,  1.0547,  0.5703,  0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 710])
attention_mask shape: torch.Size([4, 710])
reward: tensor([-0.7422,  0.2285, -1.1797, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1093])
attention_mask shape: torch.Size([4, 1093])
reward: tensor([-1.4766,  0.4531,  0.2910,  0.9727], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1133])
attention_mask shape: torch.Size([4, 1133])
reward: tensor([-0.8633, -1.7188,  1.0156,  0.1689], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1166])
attention_mask shape: torch.Size([4, 1166])
reward: tensor([-1.2188,  0.3828,  0.4316, -0.0244], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 899])
attention_mask shape: torch.Size([4, 899])
reward: tensor([-0.0776,  0.3555,  0.6211,  0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 723])
attention_mask shape: torch.Size([4, 723])
reward: tensor([ 0.1377, -1.1875, -0.0178, -0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1537])
attention_mask shape: torch.Size([4, 1537])
reward: tensor([-0.3594, -0.2090,  1.1719, -1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1046])
attention_mask shape: torch.Size([4, 1046])
reward: tensor([ 0.6133, -0.5938,  0.3340,  0.0835], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1478])
attention_mask shape: torch.Size([4, 1478])
reward: tensor([ 0.0056, -0.9961, -0.0133, -0.0133], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1400])
attention_mask shape: torch.Size([4, 1400])
reward: tensor([-0.9766,  0.3652,  0.8398, -0.2754], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 996])
attention_mask shape: torch.Size([4, 996])
reward: tensor([-1.6641, -0.2002, -0.4004, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([ 0.7109, -0.8477, -0.2812, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 618])
attention_mask shape: torch.Size([4, 618])
reward: tensor([-0.7070,  1.0234,  0.3125, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1314])
attention_mask shape: torch.Size([4, 1314])
reward: tensor([-0.0732, -0.2812, -0.5625, -1.4297], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([ 0.1064,  0.0300,  0.2617, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1191])
attention_mask shape: torch.Size([4, 1191])
reward: tensor([-0.8438,  0.0610, -2.1094,  1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1618])
attention_mask shape: torch.Size([4, 1618])
reward: tensor([ 0.1157,  0.5898,  0.8086, -0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1734])
attention_mask shape: torch.Size([4, 1734])
reward: tensor([-0.2227, -0.0178, -0.6250, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1173])
attention_mask shape: torch.Size([4, 1173])
reward: tensor([ 0.6797, -1.0469,  0.5820, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1167])
attention_mask shape: torch.Size([4, 1167])
reward: tensor([ 0.0211, -0.2441, -0.0933, -1.3047], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 771])
attention_mask shape: torch.Size([4, 771])
reward: tensor([ 0.0791,  0.0444, -0.1865,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 603])
attention_mask shape: torch.Size([4, 603])
reward: tensor([ 1.1016, -0.4629,  0.6484,  0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1489])
attention_mask shape: torch.Size([4, 1489])
reward: tensor([-0.7422, -0.0222, -0.4707,  1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1124])
attention_mask shape: torch.Size([4, 1124])
reward: tensor([-1.2422,  0.0233, -0.9609, -0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1526])
attention_mask shape: torch.Size([4, 1526])
reward: tensor([ 0.7109, -0.7383,  0.0100,  0.2539], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 825])
attention_mask shape: torch.Size([4, 825])
reward: tensor([-0.1201, -0.0579,  1.1641, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([ 0.3164,  0.4141, -0.6172, -0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.6211, -0.8281,  0.2793, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 917])
attention_mask shape: torch.Size([4, 917])
reward: tensor([-1.0234,  1.4141,  1.0625,  1.2656], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1082])
attention_mask shape: torch.Size([4, 1082])
reward: tensor([-0.2285, -0.3965, -1.0859,  0.3438], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1401])
attention_mask shape: torch.Size([4, 1401])
reward: tensor([-0.4141, -1.4453, -1.0781, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 809])
attention_mask shape: torch.Size([4, 809])
reward: tensor([-0.5547, -0.1245, -1.5469, -0.0311], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1224])
attention_mask shape: torch.Size([4, 1224])
reward: tensor([ 0.1445, -0.6328, -1.2344, -0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 752])
attention_mask shape: torch.Size([4, 752])
reward: tensor([-0.1777,  1.2812, -0.3164, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 729])
attention_mask shape: torch.Size([4, 729])
reward: tensor([ 0.1245, -0.3418, -0.2930, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 663])
attention_mask shape: torch.Size([4, 663])
reward: tensor([ 1.0859, -0.1309, -0.3242,  0.0732], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1137])
attention_mask shape: torch.Size([4, 1137])
reward: tensor([-1.2500,  0.6211,  0.5938,  0.0356], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1174])
attention_mask shape: torch.Size([4, 1174])
reward: tensor([ 0.7188, -1.3125, -0.0089,  1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-0.7422, -0.3242,  0.5234, -0.1797], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1178])
attention_mask shape: torch.Size([4, 1178])
reward: tensor([-1.0234,  1.5547, -1.0938,  1.4766], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 765])
attention_mask shape: torch.Size([4, 765])
reward: tensor([-0.0776, -0.1357, -1.8438,  0.1523], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 672])
attention_mask shape: torch.Size([4, 672])
reward: tensor([ 0.0588, -0.7617,  0.2930, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 654])
attention_mask shape: torch.Size([4, 654])
reward: tensor([-0.1641,  0.3047, -0.5469,  0.2559], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1236])
attention_mask shape: torch.Size([4, 1236])
reward: tensor([-0.4707, -0.8320, -0.9766,  0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-0.7109, -0.8203,  0.4785,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1646])
attention_mask shape: torch.Size([4, 1646])
reward: tensor([-1.1250,  0.3086, -1.7578,  0.0913], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1171])
attention_mask shape: torch.Size([4, 1171])
reward: tensor([-1.4688,  0.4141, -0.3730, -1.7188], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 674])
attention_mask shape: torch.Size([4, 674])
reward: tensor([-0.7812,  0.7031, -0.9141, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 746])
attention_mask shape: torch.Size([4, 746])
reward: tensor([ 0.7344,  0.5195, -0.4570, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 820])
attention_mask shape: torch.Size([4, 820])
reward: tensor([ 0.3828,  0.7695, -0.2402,  1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1074])
attention_mask shape: torch.Size([4, 1074])
reward: tensor([-0.4258,  0.1689, -1.2109,  0.6445], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1340])
attention_mask shape: torch.Size([4, 1340])
reward: tensor([-0.2373, -0.4883, -0.9492, -0.1865], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1218])
attention_mask shape: torch.Size([4, 1218])
reward: tensor([ 0.6406, -0.3281,  0.5469, -1.8906], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1286])
attention_mask shape: torch.Size([4, 1286])
reward: tensor([ 1.1250, -1.3359, -0.2734, -0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 674])
attention_mask shape: torch.Size([4, 674])
reward: tensor([ 0.9062, -0.6797, -0.3906, -0.0820], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1101])
attention_mask shape: torch.Size([4, 1101])
reward: tensor([ 2.0156, -1.4297,  0.1201, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 923])
attention_mask shape: torch.Size([4, 923])
reward: tensor([ 1.0859, -0.3027, -0.1514, -1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1397])
attention_mask shape: torch.Size([4, 1397])
reward: tensor([-0.6719,  0.2695,  0.3320,  1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1369])
attention_mask shape: torch.Size([4, 1369])
reward: tensor([ 1.3672,  0.1387,  1.4219, -0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 571])
attention_mask shape: torch.Size([4, 571])
reward: tensor([-0.6406, -0.8984, -0.8789,  0.9102], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1223])
attention_mask shape: torch.Size([4, 1223])
reward: tensor([-0.6719, -0.1177, -1.5000,  0.3574], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1087])
attention_mask shape: torch.Size([4, 1087])
reward: tensor([-1.0391,  0.9492,  0.7188,  0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1240])
attention_mask shape: torch.Size([4, 1240])
reward: tensor([-0.1309, -0.6641, -0.7227, -0.1797], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1748])
attention_mask shape: torch.Size([4, 1748])
reward: tensor([ 1.1562, -0.0311, -0.1641, -0.2471], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([ 0.7812, -2.1094,  0.5938,  0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1180])
attention_mask shape: torch.Size([4, 1180])
reward: tensor([-0.0400,  0.7188,  0.6172, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1522])
attention_mask shape: torch.Size([4, 1522])
reward: tensor([-0.1396, -1.1484, -0.1885,  0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 624])
attention_mask shape: torch.Size([4, 624])
reward: tensor([-0.9883, -1.9531, -0.5938,  0.2969], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 757])
attention_mask shape: torch.Size([4, 757])
reward: tensor([-0.8008, -1.6562, -0.7969,  0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1504])
attention_mask shape: torch.Size([4, 1504])
reward: tensor([-0.2002, -0.5820, -2.1250,  0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 630])
attention_mask shape: torch.Size([4, 630])
reward: tensor([ 0.4180,  1.5234, -0.2812, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1924])
attention_mask shape: torch.Size([4, 1924])
reward: tensor([-1.2188, -0.3906, -0.6641, -0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1535])
attention_mask shape: torch.Size([4, 1535])
reward: tensor([-0.0045, -0.0178,  0.5898,  0.2285], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1198])
attention_mask shape: torch.Size([4, 1198])
reward: tensor([-0.0820, -0.1953, -0.1133, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1639])
attention_mask shape: torch.Size([4, 1639])
reward: tensor([ 0.3574,  0.0078, -0.0156, -0.1885], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2017])
attention_mask shape: torch.Size([4, 2017])
reward: tensor([-1.7969,  0.7500,  0.9414, -0.1709], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 613])
attention_mask shape: torch.Size([4, 613])
reward: tensor([ 0.5742,  1.1875,  0.5430, -0.8125], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 698])
attention_mask shape: torch.Size([4, 698])
reward: tensor([-0.4805, -0.6133,  1.1875,  0.4414], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1472])
attention_mask shape: torch.Size([4, 1472])
reward: tensor([-0.4082,  0.4023,  1.0391, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1195])
attention_mask shape: torch.Size([4, 1195])
reward: tensor([-0.5195, -1.0156, -0.0889, -0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-1.2344, -0.9961, -0.7656, -0.1465], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1362])
attention_mask shape: torch.Size([4, 1362])
reward: tensor([-1.3828, -0.1621,  1.0547, -0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 657])
attention_mask shape: torch.Size([4, 657])
reward: tensor([-0.3242, -1.0469, -0.3164,  0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1176])
attention_mask shape: torch.Size([4, 1176])
reward: tensor([-1.1172, -0.8320,  1.3359,  0.2021], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 617])
attention_mask shape: torch.Size([4, 617])
reward: tensor([-0.3906, -0.8984, -0.1777, -0.1865], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1305])
attention_mask shape: torch.Size([4, 1305])
reward: tensor([-0.6523,  0.2598, -0.0977, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1258])
attention_mask shape: torch.Size([4, 1258])
reward: tensor([-0.9688,  0.1836, -0.1982, -0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1126])
attention_mask shape: torch.Size([4, 1126])
reward: tensor([-0.6133, -0.4707, -0.3281,  0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([ 0.0623, -1.2188,  1.7891,  0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 501])
attention_mask shape: torch.Size([4, 501])
reward: tensor([-1.1562, -0.8320, -0.0045, -0.0378], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 438])
attention_mask shape: torch.Size([4, 438])
reward: tensor([ 0.7188, -1.9766, -0.5781, -0.1533], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 915])
attention_mask shape: torch.Size([4, 915])
reward: tensor([ 0.0510, -0.9258, -0.4004,  0.1089], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 452])
attention_mask shape: torch.Size([4, 452])
reward: tensor([-0.4570, -0.3828, -0.5938,  0.1113], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1050])
attention_mask shape: torch.Size([4, 1050])
reward: tensor([-0.0623,  0.6836,  0.1348,  0.0056], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1351])
attention_mask shape: torch.Size([4, 1351])
reward: tensor([ 0.0222, -0.4629, -0.1885, -0.2812], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1314])
attention_mask shape: torch.Size([4, 1314])
reward: tensor([ 1.2109,  0.3184, -1.5859,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1248])
attention_mask shape: torch.Size([4, 1248])
reward: tensor([-1.6250, -0.8203,  0.9727,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 723])
attention_mask shape: torch.Size([4, 723])
reward: tensor([-0.4219, -0.2852, -0.5820, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1271])
attention_mask shape: torch.Size([4, 1271])
reward: tensor([ 0.2891, -0.3281, -1.4062, -1.6641], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-1.2812, -0.3730,  0.9375, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 593])
attention_mask shape: torch.Size([4, 593])
reward: tensor([-0.5586, -1.1875, -1.1641, -0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1760])
attention_mask shape: torch.Size([4, 1760])
reward: tensor([-0.2930,  0.6992, -0.0222,  0.8008], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1429])
attention_mask shape: torch.Size([4, 1429])
reward: tensor([ 1.0312,  1.1875, -0.2266, -1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([ 0.1865, -0.8047,  0.7227, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 901])
attention_mask shape: torch.Size([4, 901])
reward: tensor([-1.2969, -0.7227,  0.2158, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1350])
attention_mask shape: torch.Size([4, 1350])
reward: tensor([-0.1797,  1.0781, -0.3516,  0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 695])
attention_mask shape: torch.Size([4, 695])
reward: tensor([-0.3594, -1.2734,  0.4141, -1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1175])
attention_mask shape: torch.Size([4, 1175])
reward: tensor([ 1.5312, -1.5000, -0.9688,  0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1212])
attention_mask shape: torch.Size([4, 1212])
reward: tensor([ 0.3008, -0.1934, -1.4297,  0.2969], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1612])
attention_mask shape: torch.Size([4, 1612])
reward: tensor([ 0.0167, -0.7930, -0.3730,  1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 601])
attention_mask shape: torch.Size([4, 601])
reward: tensor([-0.1826,  0.7461, -0.7070, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 585])
attention_mask shape: torch.Size([4, 585])
reward: tensor([-0.2559, -0.9414, -0.8438,  0.4629], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1246])
attention_mask shape: torch.Size([4, 1246])
reward: tensor([ 0.9570, -0.4805, -0.7812,  0.2617], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1120])
attention_mask shape: torch.Size([4, 1120])
reward: tensor([-0.0532, -1.7031,  0.1143, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 520])
attention_mask shape: torch.Size([4, 520])
reward: tensor([-1.0703,  0.4355, -0.1377,  0.8750], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1348])
attention_mask shape: torch.Size([4, 1348])
reward: tensor([-0.9414,  0.7852, -0.3867,  0.8750], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1139])
attention_mask shape: torch.Size([4, 1139])
reward: tensor([-0.5547, -1.6250,  0.4629,  0.2422], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1261])
attention_mask shape: torch.Size([4, 1261])
reward: tensor([ 0.8164, -0.4219,  0.3223,  0.1992], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1113])
attention_mask shape: torch.Size([4, 1113])
reward: tensor([ 0.5273, -0.8320, -0.3340,  0.2168], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1365])
attention_mask shape: torch.Size([4, 1365])
reward: tensor([ 0.7734, -1.4375,  2.5938, -1.1641], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([ 1.9141, -0.9961, -0.2969,  0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1092])
attention_mask shape: torch.Size([4, 1092])
reward: tensor([ 0.1396, -0.8906,  1.5312, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1417])
attention_mask shape: torch.Size([4, 1417])
reward: tensor([ 0.2500, -0.4219,  0.7812, -0.1777], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 572])
attention_mask shape: torch.Size([4, 572])
reward: tensor([-0.5352, -0.4316,  0.1504,  0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1160])
attention_mask shape: torch.Size([4, 1160])
reward: tensor([-0.6914, -0.9062, -0.0532, -0.0111], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1099])
attention_mask shape: torch.Size([4, 1099])
reward: tensor([ 1.0781, -0.3594, -0.1045,  0.1167], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 866])
attention_mask shape: torch.Size([4, 866])
reward: tensor([ 1.2109, -1.2500,  0.9688,  0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 912])
attention_mask shape: torch.Size([4, 912])
reward: tensor([ 0.2656,  1.0469,  0.2031, -1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-0.7422, -1.1875, -0.6719,  0.4023], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 970])
attention_mask shape: torch.Size([4, 970])
reward: tensor([-0.1865, -0.5898,  0.0011, -0.0801], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1747])
attention_mask shape: torch.Size([4, 1747])
reward: tensor([ 2.1250,  0.1338, -1.1094,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1336])
attention_mask shape: torch.Size([4, 1336])
reward: tensor([-0.3164, -0.7031,  0.9766, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 789])
attention_mask shape: torch.Size([4, 789])
reward: tensor([-0.6328, -0.7461, -1.2891,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 911])
attention_mask shape: torch.Size([4, 911])
reward: tensor([0.7109, 0.4883, 0.8086, 0.6016], device='cuda:0', dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1267])
attention_mask shape: torch.Size([4, 1267])
reward: tensor([-0.2402, -0.3652, -0.3203, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1178])
attention_mask shape: torch.Size([4, 1178])
reward: tensor([-0.6328, -1.2656, -0.3066,  0.1279], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1277])
attention_mask shape: torch.Size([4, 1277])
reward: tensor([ 0.2773, -0.6094, -0.8516,  0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 482])
attention_mask shape: torch.Size([4, 482])
reward: tensor([-0.2334, -1.1797,  0.0811,  0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1419])
attention_mask shape: torch.Size([4, 1419])
reward: tensor([ 0.4883,  0.4082, -0.4668, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 576])
attention_mask shape: torch.Size([4, 576])
reward: tensor([-0.2891, -0.2910, -0.1445, -1.7344], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 708])
attention_mask shape: torch.Size([4, 708])
reward: tensor([ 0.3516, -0.6875,  1.3125, -1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1376])
attention_mask shape: torch.Size([4, 1376])
reward: tensor([1.7266, 0.0977, 0.0145, 0.2197], device='cuda:0', dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 787])
attention_mask shape: torch.Size([4, 787])
reward: tensor([-0.0801,  0.6055,  0.8633,  0.2041], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-27 00:31:42,047] [INFO] [launch.py:351:main] Process 768613 exits successfully.
[2024-10-27 00:33:45,169] [INFO] [launch.py:351:main] Process 768614 exits successfully.
[2024-10-27 00:36:22,327] [INFO] [launch.py:351:main] Process 768615 exits successfully.
[2024-10-27 00:37:12,378] [INFO] [launch.py:351:main] Process 768616 exits successfully.
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# [K(base) root@autodl-container-ec234bbd2e-925c6d34:~# bash run_eval_reward_openrlhf.sh
[?2004l+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-10th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-10th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_10 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-27 11:46:48,809] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 11:46:51,886] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-27 11:46:51,887] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-10th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_10 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-27 11:46:53,274] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 11:46:56,720] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-27 11:46:56,720] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-27 11:46:56,720] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-27 11:46:56,720] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-27 11:46:56,720] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-27 11:46:56,720] [INFO] [launch.py:256:main] process 790619 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-10th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_10', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-27 11:46:56,721] [INFO] [launch.py:256:main] process 790620 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-10th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_10', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-27 11:46:56,721] [INFO] [launch.py:256:main] process 790621 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-10th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_10', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-27 11:46:56,721] [INFO] [launch.py:256:main] process 790622 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-10th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_10', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-27 11:46:58,313] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 11:46:58,354] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 11:46:58,361] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 11:46:58,368] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-27 11:47:02,076] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-27 11:47:02,076] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-27 11:47:02,759] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-27 11:47:02,760] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-27 11:47:02,760] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.57it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.34it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.52it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.34it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.33it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.50it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.08it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.32it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.38it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.72it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.64it/s]
Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.14it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.63it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.52it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.33it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.14it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.48it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.42it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.21it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.17it/s]
[2024-10-27 11:47:05,174] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 11:47:05,266] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 11:47:05,497] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-27 11:47:05,701] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-27 11:47:05,702] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-27 11:47:05,702] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 11:47:07,156] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-27 11:47:07,157] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-27 11:47:07,158] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 11:47:07,158] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 11:47:07,160] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 11:47:07,294] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-27 11:47:07,295] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-27 11:47:07,295] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.78 GB, percent = 2.8%
[2024-10-27 11:47:07,429] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-27 11:47:07,430] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-27 11:47:07,430] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.79 GB, percent = 2.8%
[2024-10-27 11:47:07,431] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-27 11:47:07,431] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-27 11:47:07,431] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-27 11:47:07,431] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-27 11:47:07,431] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8ff87aa590>
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-27 11:47:07,432] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-27 11:47:07,433] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-27 11:47:07,433] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-27 11:47:07,434] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-27 11:47:07,434] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-27 11:47:07,434] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
[2024-10-27 11:47:11,371] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-27 11:47:11,373] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
[2024-10-27 11:47:11,506] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-27 11:47:11,507] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-27 11:47:11,507] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.82 GB, percent = 2.8%
[2024-10-27 11:47:11,630] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-27 11:47:11,630] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-27 11:47:11,631] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.82 GB, percent = 2.8%
[2024-10-27 11:47:11,632] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-27 11:47:11,632] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-27 11:47:11,632] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-27 11:47:11,632] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-27 11:47:11,632] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-27 11:47:11,632] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8fe1f2a950>
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-27 11:47:11,633] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-27 11:47:11,634] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-27 11:47:11,634] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 585/100000 [00:00<00:17, 5844.78it/s]Preprocessing data:   2%|█▍                                                                                          | 1578/100000 [00:00<00:11, 8244.21it/s]Preprocessing data:   3%|██▎                                                                                         | 2561/100000 [00:00<00:10, 8965.46it/s]Preprocessing data:   4%|███▎                                                                                        | 3562/100000 [00:00<00:10, 9374.93it/s]Preprocessing data:   5%|████▏                                                                                       | 4546/100000 [00:00<00:10, 9541.72it/s]Preprocessing data:   6%|█████                                                                                       | 5507/100000 [00:00<00:09, 9564.37it/s]Preprocessing data:   6%|█████▉                                                                                      | 6472/100000 [00:00<00:09, 9589.92it/s]Preprocessing data:   7%|██████▊                                                                                     | 7431/100000 [00:00<00:09, 9581.95it/s]Preprocessing data:   8%|███████▋                                                                                    | 8390/100000 [00:00<00:09, 9574.26it/s]Preprocessing data:   9%|████████▋                                                                                   | 9388/100000 [00:01<00:09, 9698.42it/s]Preprocessing data:  10%|█████████▍                                                                                 | 10416/100000 [00:01<00:09, 9876.11it/s]Preprocessing data:  11%|██████████▎                                                                               | 11472/100000 [00:01<00:08, 10081.31it/s]Preprocessing data:  13%|███████████▎                                                                              | 12530/100000 [00:01<00:08, 10230.49it/s]Preprocessing data:  14%|████████████▏                                                                             | 13557/100000 [00:01<00:08, 10242.05it/s]Preprocessing data:  15%|█████████████▏                                                                            | 14618/100000 [00:01<00:08, 10351.65it/s]Preprocessing data:  16%|██████████████                                                                            | 15679/100000 [00:01<00:08, 10428.26it/s]Preprocessing data:  17%|███████████████                                                                           | 16743/100000 [00:01<00:07, 10491.71it/s]Preprocessing data:  18%|████████████████                                                                          | 17803/100000 [00:01<00:07, 10523.04it/s]Preprocessing data:  19%|████████████████▉                                                                         | 18856/100000 [00:01<00:07, 10513.18it/s]Preprocessing data:  20%|█████████████████▉                                                                        | 19915/100000 [00:02<00:07, 10533.69it/s]Preprocessing data:  21%|██████████████████▉                                                                       | 20987/100000 [00:02<00:07, 10589.19it/s]Preprocessing data:  22%|███████████████████▊                                                                      | 22061/100000 [00:02<00:07, 10633.16it/s]Preprocessing data:  23%|████████████████████▊                                                                     | 23125/100000 [00:02<00:07, 10576.52it/s]Preprocessing data:  24%|█████████████████████▊                                                                    | 24183/100000 [00:02<00:07, 10441.42it/s]Preprocessing data:  25%|██████████████████████▋                                                                   | 25228/100000 [00:02<00:07, 10435.01it/s]Preprocessing data:  26%|███████████████████████▋                                                                  | 26272/100000 [00:02<00:07, 10420.63it/s]Preprocessing data:  27%|████████████████████████▌                                                                 | 27315/100000 [00:02<00:06, 10410.96it/s]Preprocessing data:  28%|█████████████████████████▌                                                                | 28357/100000 [00:02<00:06, 10397.67it/s]Preprocessing data:  29%|██████████████████████████▍                                                               | 29397/100000 [00:02<00:06, 10260.11it/s]Preprocessing data:  30%|███████████████████████████▍                                                              | 30424/100000 [00:03<00:06, 10230.08it/s]Preprocessing data:  31%|████████████████████████████▎                                                             | 31448/100000 [00:03<00:06, 10213.10it/s]Preprocessing data:  32%|█████████████████████████████▏                                                            | 32475/100000 [00:03<00:06, 10229.60it/s]Preprocessing data:  33%|██████████████████████████████▏                                                           | 33499/100000 [00:03<00:06, 10202.47it/s]Preprocessing data:  35%|███████████████████████████████                                                           | 34520/100000 [00:03<00:06, 10121.90it/s]Preprocessing data:  36%|███████████████████████████████▉                                                          | 35543/100000 [00:03<00:06, 10153.55it/s]Preprocessing data:  37%|████████████████████████████████▉                                                         | 36566/100000 [00:03<00:06, 10174.98it/s]Preprocessing data:  38%|█████████████████████████████████▊                                                        | 37593/100000 [00:03<00:06, 10201.35it/s]Preprocessing data:  39%|██████████████████████████████████▊                                                       | 38626/100000 [00:03<00:05, 10238.31it/s]Preprocessing data:  40%|███████████████████████████████████▋                                                      | 39660/100000 [00:03<00:05, 10266.41it/s]Preprocessing data:  41%|████████████████████████████████████▌                                                     | 40689/100000 [00:04<00:05, 10271.52it/s]Preprocessing data:  42%|█████████████████████████████████████▌                                                    | 41717/100000 [00:04<00:05, 10251.10it/s]Preprocessing data:  43%|██████████████████████████████████████▍                                                   | 42743/100000 [00:04<00:05, 10161.20it/s]Preprocessing data:  44%|███████████████████████████████████████▍                                                  | 43783/100000 [00:04<00:05, 10229.71it/s]Preprocessing data:  45%|████████████████████████████████████████▎                                                 | 44826/100000 [00:04<00:05, 10287.69it/s]Preprocessing data:  46%|█████████████████████████████████████████▎                                                | 45883/100000 [00:04<00:05, 10371.81it/s]Preprocessing data:  47%|██████████████████████████████████████████▏                                               | 46921/100000 [00:04<00:05, 10348.02it/s]Preprocessing data:  48%|███████████████████████████████████████████▏                                              | 47956/100000 [00:04<00:05, 10292.02it/s]Preprocessing data:  49%|████████████████████████████████████████████                                              | 48986/100000 [00:04<00:04, 10280.28it/s]Preprocessing data:  50%|█████████████████████████████████████████████                                             | 50018/100000 [00:04<00:04, 10291.73it/s]Preprocessing data:  51%|█████████████████████████████████████████████▉                                            | 51048/100000 [00:05<00:04, 10285.42it/s]Preprocessing data:  52%|██████████████████████████████████████████████▉                                           | 52090/100000 [00:05<00:04, 10324.84it/s]Preprocessing data:  53%|███████████████████████████████████████████████▊                                          | 53149/100000 [00:05<00:04, 10402.26it/s]Preprocessing data:  54%|████████████████████████████████████████████████▊                                         | 54199/100000 [00:05<00:04, 10430.80it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▋                                        | 55243/100000 [00:05<00:04, 10344.29it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▋                                       | 56292/100000 [00:05<00:04, 10386.54it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▌                                      | 57343/100000 [00:05<00:04, 10421.22it/s]Preprocessing data:  58%|████████████████████████████████████████████████████▌                                     | 58386/100000 [00:05<00:04, 10398.94it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████▍                                    | 59426/100000 [00:05<00:03, 10387.06it/s]Preprocessing data:  60%|██████████████████████████████████████████████████████▍                                   | 60479/100000 [00:05<00:03, 10428.85it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▍                                  | 61529/100000 [00:06<00:03, 10449.48it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▎                                 | 62588/100000 [00:06<00:03, 10491.15it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▎                                | 63638/100000 [00:06<00:03, 10478.35it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▏                               | 64686/100000 [00:06<00:03, 10458.90it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▏                              | 65732/100000 [00:06<00:03, 10403.84it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████                              | 66774/100000 [00:06<00:03, 10406.76it/s]Preprocessing data:  68%|█████████████████████████████████████████████████████████████                             | 67815/100000 [00:06<00:03, 10370.21it/s]Preprocessing data:  69%|█████████████████████████████████████████████████████████████▉                            | 68853/100000 [00:06<00:03, 10339.16it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▉                           | 69887/100000 [00:06<00:02, 10331.99it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▊                          | 70921/100000 [00:06<00:02, 10287.95it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▊                         | 71950/100000 [00:07<00:02, 10259.22it/s]Preprocessing data:  73%|██████████████████████████████████████████████████████████████████▍                        | 72976/100000 [00:07<00:02, 9797.64it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▎                       | 73960/100000 [00:07<00:02, 9101.76it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▏                      | 74882/100000 [00:07<00:02, 8578.91it/s]Preprocessing data:  76%|████████████████████████████████████████████████████████████████████▉                      | 75752/100000 [00:07<00:02, 8319.10it/s]Preprocessing data:  77%|█████████████████████████████████████████████████████████████████████▋                     | 76592/100000 [00:07<00:02, 8153.56it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▍                    | 77412/100000 [00:07<00:02, 8031.43it/s]Preprocessing data:  78%|███████████████████████████████████████████████████████████████████████▏                   | 78218/100000 [00:07<00:02, 7956.51it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▉                   | 79016/100000 [00:07<00:02, 7862.25it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▌                  | 79804/100000 [00:08<00:02, 7767.68it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▎                 | 80582/100000 [00:08<00:02, 7569.72it/s]Preprocessing data:  81%|██████████████████████████████████████████████████████████████████████████▏                | 81499/100000 [00:08<00:02, 8025.91it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▉                | 82366/100000 [00:08<00:02, 8209.87it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▋               | 83190/100000 [00:08<00:02, 8157.82it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▍              | 84008/100000 [00:08<00:02, 7880.13it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▍             | 85032/100000 [00:08<00:01, 8558.46it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▏            | 85989/100000 [00:08<00:01, 8851.92it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████            | 86879/100000 [00:08<00:01, 8357.28it/s]Preprocessing data:  88%|███████████████████████████████████████████████████████████████████████████████▉           | 87809/100000 [00:08<00:01, 8623.12it/s]Preprocessing data:  89%|████████████████████████████████████████████████████████████████████████████████▊          | 88768/100000 [00:09<00:01, 8900.81it/s]Preprocessing data:  90%|█████████████████████████████████████████████████████████████████████████████████▋         | 89719/100000 [00:09<00:01, 9076.90it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▍        | 90632/100000 [00:09<00:01, 8634.22it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▎       | 91503/100000 [00:09<00:01, 8004.88it/s]Preprocessing data:  92%|████████████████████████████████████████████████████████████████████████████████████       | 92316/100000 [00:09<00:01, 7637.25it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▋      | 93090/100000 [00:09<00:00, 7432.08it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▍     | 93840/100000 [00:09<00:00, 7443.53it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▎    | 94787/100000 [00:09<00:00, 8009.31it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▏   | 95778/100000 [00:09<00:00, 8550.81it/s]Preprocessing data:  97%|███████████████████████████████████████████████████████████████████████████████████████▉   | 96641/100000 [00:10<00:00, 8514.17it/s]Preprocessing data:  98%|████████████████████████████████████████████████████████████████████████████████████████▋  | 97524/100000 [00:10<00:00, 8605.54it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▌ | 98457/100000 [00:10<00:00, 8817.29it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████▌| 99459/100000 [00:10<00:00, 9171.68it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9590.45it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1400])
attention_mask shape: torch.Size([4, 1400])
reward: tensor([-1.1016, -1.4062,  0.6562, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1813])
attention_mask shape: torch.Size([4, 1813])
reward: tensor([ 1.3828, -1.3672,  0.4570,  0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1144])
attention_mask shape: torch.Size([4, 1144])
reward: tensor([-2.1719, -0.8516,  0.0311, -0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 326])
attention_mask shape: torch.Size([4, 326])
reward: tensor([ 0.2695, -2.2031,  0.2227, -0.2246], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1643])
attention_mask shape: torch.Size([4, 1643])
reward: tensor([0.1270, 0.5547, 1.0391, 0.0089], device='cuda:0', dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1155])
attention_mask shape: torch.Size([4, 1155])
reward: tensor([ 1.7656, -0.9766, -0.0732, -0.1357], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1954])
attention_mask shape: torch.Size([4, 1954])
reward: tensor([-0.2021, -0.0688, -0.6367, -0.7227], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1015])
attention_mask shape: torch.Size([4, 1015])
reward: tensor([-0.7109, -0.5547,  0.0211,  0.0255], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1226])
attention_mask shape: torch.Size([4, 1226])
reward: tensor([-0.1377, -0.3457,  0.0522, -0.8320], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1090])
attention_mask shape: torch.Size([4, 1090])
reward: tensor([ 0.2305,  1.4375, -1.1094, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1619])
attention_mask shape: torch.Size([4, 1619])
reward: tensor([-1.3984, -0.7734, -1.7656, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 995])
attention_mask shape: torch.Size([4, 995])
reward: tensor([ 0.4160, -0.8320, -0.5898, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1055])
attention_mask shape: torch.Size([4, 1055])
reward: tensor([-1.1719, -0.5117, -0.1201, -0.0334], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1553])
attention_mask shape: torch.Size([4, 1553])
reward: tensor([-0.5625,  0.1299,  1.5547, -0.7617], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 752])
attention_mask shape: torch.Size([4, 752])
reward: tensor([-1.4141,  0.0432,  1.1328, -0.1089], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1435])
attention_mask shape: torch.Size([4, 1435])
reward: tensor([ 0.7188, -0.7617, -0.2246, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1563])
attention_mask shape: torch.Size([4, 1563])
reward: tensor([-0.3418, -1.0234, -1.7656,  0.0742], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.8711,  0.1875, -0.3555, -0.3164], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 963])
attention_mask shape: torch.Size([4, 963])
reward: tensor([ 0.8398,  0.0400, -1.0469, -0.0845], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 866])
attention_mask shape: torch.Size([4, 866])
reward: tensor([-0.1689,  0.6758, -1.1250, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 906])
attention_mask shape: torch.Size([4, 906])
reward: tensor([-0.7344,  0.5781,  1.0234, -0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1202])
attention_mask shape: torch.Size([4, 1202])
reward: tensor([-0.3730,  0.6211,  1.0859,  0.4961], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1017])
attention_mask shape: torch.Size([4, 1017])
reward: tensor([-0.0334,  0.2002, -0.0222, -1.8438], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1306])
attention_mask shape: torch.Size([4, 1306])
reward: tensor([-0.8438, -0.2578, -0.7734,  1.4141], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([ 0.8906, -0.7148, -0.0222, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1311])
attention_mask shape: torch.Size([4, 1311])
reward: tensor([ 0.7539, -0.8047,  0.2256, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1576])
attention_mask shape: torch.Size([4, 1576])
reward: tensor([-0.9258, -0.5508,  0.4219,  0.3086], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 382])
attention_mask shape: torch.Size([4, 382])
reward: tensor([ 0.3496, -0.4355, -1.3047,  0.1953], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1047])
attention_mask shape: torch.Size([4, 1047])
reward: tensor([ 0.9492, -0.9883, -0.2695,  2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2024])
attention_mask shape: torch.Size([4, 2024])
reward: tensor([-0.2578, -2.0156, -0.6094, -0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1662])
attention_mask shape: torch.Size([4, 1662])
reward: tensor([-0.1245,  0.4180,  0.3594, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 757])
attention_mask shape: torch.Size([4, 757])
reward: tensor([-0.8789, -0.0532, -1.1484, -0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1304])
attention_mask shape: torch.Size([4, 1304])
reward: tensor([-0.8281, -0.2891,  0.5938, -1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([ 0.4824,  0.2578, -0.9141, -0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 839])
attention_mask shape: torch.Size([4, 839])
reward: tensor([-0.4355, -1.6172, -0.7617, -1.1797], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.9492, -0.1777,  0.8438, -1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 972])
attention_mask shape: torch.Size([4, 972])
reward: tensor([ 0.0801, -0.4082, -0.5508,  0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1500])
attention_mask shape: torch.Size([4, 1500])
reward: tensor([ 0.8398, -1.8906, -0.6680, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1091])
attention_mask shape: torch.Size([4, 1091])
reward: tensor([-1.0859,  0.3320,  1.5469, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1796])
attention_mask shape: torch.Size([4, 1796])
reward: tensor([ 1.9922,  0.5938, -0.4668,  1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1254])
attention_mask shape: torch.Size([4, 1254])
reward: tensor([ 1.2812, -0.3555,  1.8516,  0.4395], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1559])
attention_mask shape: torch.Size([4, 1559])
reward: tensor([ 1.3984,  1.4062, -0.8047,  0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 581])
attention_mask shape: torch.Size([4, 581])
reward: tensor([-0.3027, -0.9141, -0.8633, -0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 831])
attention_mask shape: torch.Size([4, 831])
reward: tensor([-1.5469, -0.3828, -0.8125, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1670])
attention_mask shape: torch.Size([4, 1670])
reward: tensor([-0.2070,  1.7891,  0.2314,  1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1009])
attention_mask shape: torch.Size([4, 1009])
reward: tensor([ 1.0859, -0.8516, -0.2969, -0.7305], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 725])
attention_mask shape: torch.Size([4, 725])
reward: tensor([ 0.2412, -0.2812,  1.5938, -0.8359], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.1221, -0.2871, -0.2334,  0.6328], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1666])
attention_mask shape: torch.Size([4, 1666])
reward: tensor([ 0.1157, -0.2871, -1.0469,  0.0811], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([ 0.0977, -2.1562, -0.3379,  0.1611], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1354])
attention_mask shape: torch.Size([4, 1354])
reward: tensor([-0.6445, -0.9883,  0.6523, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1313])
attention_mask shape: torch.Size([4, 1313])
reward: tensor([-0.8086,  1.6328, -0.4531,  0.2402], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1392])
attention_mask shape: torch.Size([4, 1392])
reward: tensor([-0.6094, -1.2500,  0.0422, -1.5234], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1590])
attention_mask shape: torch.Size([4, 1590])
reward: tensor([-1.8828,  0.1245,  0.2559,  2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1441])
attention_mask shape: torch.Size([4, 1441])
reward: tensor([-0.8281, -0.6133, -0.0820, -0.1177], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 711])
attention_mask shape: torch.Size([4, 711])
reward: tensor([-1.1484, -0.7383, -1.2891, -1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1016])
attention_mask shape: torch.Size([4, 1016])
reward: tensor([-0.8125, -0.8633, -0.0645, -0.2090], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 948])
attention_mask shape: torch.Size([4, 948])
reward: tensor([-1.4062, -0.0289,  0.0422, -1.2031], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([ 0.8750, -0.1177,  1.6406, -0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1485])
attention_mask shape: torch.Size([4, 1485])
reward: tensor([-0.9609,  1.7500, -0.5430, -0.0801], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1107])
attention_mask shape: torch.Size([4, 1107])
reward: tensor([-0.3105, -1.0547, -1.4141, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.5586, -0.3730, -1.1094, -0.0133], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1157])
attention_mask shape: torch.Size([4, 1157])
reward: tensor([-0.2109, -0.9141, -1.2188,  0.2969], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 949])
attention_mask shape: torch.Size([4, 949])
reward: tensor([ 1.5391,  0.0942, -1.0781, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([ 0.1777, -0.4844,  0.3418, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1017])
attention_mask shape: torch.Size([4, 1017])
reward: tensor([-1.2500, -0.0133, -1.8281,  0.0444], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1143])
attention_mask shape: torch.Size([4, 1143])
reward: tensor([-0.4219, -1.5469, -1.3594, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1263])
attention_mask shape: torch.Size([4, 1263])
reward: tensor([-0.9062, -0.6836,  0.1089,  1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1063])
attention_mask shape: torch.Size([4, 1063])
reward: tensor([-0.0669, -0.3516,  0.8281,  0.3320], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.1406, -0.7422, -0.6836,  0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1515])
attention_mask shape: torch.Size([4, 1515])
reward: tensor([ 0.5391, -0.5781, -0.8320,  0.0610], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 811])
attention_mask shape: torch.Size([4, 811])
reward: tensor([ 0.4570, -0.6406, -0.3965, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1148])
attention_mask shape: torch.Size([4, 1148])
reward: tensor([-0.6875,  0.9453,  0.3730, -0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1334])
attention_mask shape: torch.Size([4, 1334])
reward: tensor([-0.0400,  0.6328,  0.2754,  1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 326])
attention_mask shape: torch.Size([4, 326])
reward: tensor([-1.4453, -2.0938, -0.4043, -0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 917])
attention_mask shape: torch.Size([4, 917])
reward: tensor([-0.3066,  0.2363, -0.2773, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([ 1.7188, -0.3105, -0.5781, -0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1750])
attention_mask shape: torch.Size([4, 1750])
reward: tensor([ 0.2285, -0.5820, -0.8164, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1064])
attention_mask shape: torch.Size([4, 1064])
reward: tensor([-0.7656,  1.2422,  0.0178,  0.3047], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1409])
attention_mask shape: torch.Size([4, 1409])
reward: tensor([-0.6445, -0.7656, -1.5000, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 999])
attention_mask shape: torch.Size([4, 999])
reward: tensor([ 0.3125, -0.1045,  1.5859, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1199])
attention_mask shape: torch.Size([4, 1199])
reward: tensor([ 1.6953, -0.9492,  0.6055, -0.2441], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([ 0.0479,  0.4570,  0.4863, -0.5156], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 431])
attention_mask shape: torch.Size([4, 431])
reward: tensor([ 0.4805,  0.0688, -0.4570, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1412])
attention_mask shape: torch.Size([4, 1412])
reward: tensor([ 0.2188,  0.3516, -0.0133, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1723])
attention_mask shape: torch.Size([4, 1723])
reward: tensor([-0.1484, -0.6992,  0.0820,  1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1320])
attention_mask shape: torch.Size([4, 1320])
reward: tensor([-1.2656,  0.1133,  1.5078,  1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1756])
attention_mask shape: torch.Size([4, 1756])
reward: tensor([-0.7539,  0.0544,  0.4609, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1568])
attention_mask shape: torch.Size([4, 1568])
reward: tensor([ 0.4355, -0.7930,  0.4648, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1122])
attention_mask shape: torch.Size([4, 1122])
reward: tensor([ 1.0078,  0.0811,  1.1094, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 766])
attention_mask shape: torch.Size([4, 766])
reward: tensor([-0.8438,  0.7695, -0.0111,  1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1381])
attention_mask shape: torch.Size([4, 1381])
reward: tensor([ 1.3828,  0.8125, -0.7812, -0.6172], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 635])
attention_mask shape: torch.Size([4, 635])
reward: tensor([-0.2695, -1.1641,  0.7188,  0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1858])
attention_mask shape: torch.Size([4, 1858])
reward: tensor([ 1.7266,  1.4688, -0.8984, -0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1423])
attention_mask shape: torch.Size([4, 1423])
reward: tensor([-0.0289, -0.5273,  0.4863, -0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1023])
attention_mask shape: torch.Size([4, 1023])
reward: tensor([ 0.4570,  1.3125, -1.6328,  0.2412], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1375])
attention_mask shape: torch.Size([4, 1375])
reward: tensor([-0.2354,  0.2158,  0.9570, -0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-1.2734, -0.6797,  1.9453, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1474])
attention_mask shape: torch.Size([4, 1474])
reward: tensor([-0.0133, -0.3730, -0.2539,  0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1110])
attention_mask shape: torch.Size([4, 1110])
reward: tensor([ 0.4453, -0.0334,  0.3711, -1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1169])
attention_mask shape: torch.Size([4, 1169])
reward: tensor([-1.0547, -0.3164, -0.5820, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 538])
attention_mask shape: torch.Size([4, 538])
reward: tensor([ 0.3281, -1.0547, -0.0356,  1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1093])
attention_mask shape: torch.Size([4, 1093])
reward: tensor([ 0.1064,  0.0566, -0.6250,  0.5508], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1161])
attention_mask shape: torch.Size([4, 1161])
reward: tensor([0.1133, 0.7305, 0.0757, 0.7227], device='cuda:0', dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([ 0.2871, -1.2812,  0.5234, -1.9375], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1421])
attention_mask shape: torch.Size([4, 1421])
reward: tensor([-0.5625, -0.1289,  0.4629,  1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1280])
attention_mask shape: torch.Size([4, 1280])
reward: tensor([-0.7500,  1.1016,  0.2734,  0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 916])
attention_mask shape: torch.Size([4, 916])
reward: tensor([-0.3652, -1.8438, -0.0845, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 691])
attention_mask shape: torch.Size([4, 691])
reward: tensor([ 0.0801, -0.8320, -1.0547, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1970])
attention_mask shape: torch.Size([4, 1970])
reward: tensor([-0.7461, -0.7109,  1.2109,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1274])
attention_mask shape: torch.Size([4, 1274])
reward: tensor([-1.0547,  0.1177,  0.8906, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 309])
attention_mask shape: torch.Size([4, 309])
reward: tensor([ 1.2344, -0.8125, -0.5352, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1453])
attention_mask shape: torch.Size([4, 1453])
reward: tensor([-0.3066,  1.3828, -0.5859,  1.9219], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 783])
attention_mask shape: torch.Size([4, 783])
reward: tensor([-0.3965, -0.0356, -0.7227, -0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1056])
attention_mask shape: torch.Size([4, 1056])
reward: tensor([-0.2793,  0.4395,  0.6406, -0.0579], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1080])
attention_mask shape: torch.Size([4, 1080])
reward: tensor([-0.6016,  0.2021, -0.2578, -0.5742], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1344])
attention_mask shape: torch.Size([4, 1344])
reward: tensor([-1.2656, -0.4805,  0.8281, -0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1279])
attention_mask shape: torch.Size([4, 1279])
reward: tensor([-0.3242,  0.7305,  0.5508, -0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 768])
attention_mask shape: torch.Size([4, 768])
reward: tensor([ 0.1377, -0.5820,  0.3516, -0.4453], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1437])
attention_mask shape: torch.Size([4, 1437])
reward: tensor([-1.7266, -0.1465,  1.5391, -1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1611])
attention_mask shape: torch.Size([4, 1611])
reward: tensor([-1.3516,  0.0022,  0.2773, -1.2031], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1636])
attention_mask shape: torch.Size([4, 1636])
reward: tensor([ 1.8047, -1.7500, -1.1094,  0.3535], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1444])
attention_mask shape: torch.Size([4, 1444])
reward: tensor([-0.7695,  1.0391,  0.9688, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 962])
attention_mask shape: torch.Size([4, 962])
reward: tensor([-1.5234,  0.5938, -0.6328, -1.3984], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1164])
attention_mask shape: torch.Size([4, 1164])
reward: tensor([ 1.2656, -1.0781, -0.3066, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 637])
attention_mask shape: torch.Size([4, 637])
reward: tensor([ 0.1836,  0.9492, -1.5312, -0.2090], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2021])
attention_mask shape: torch.Size([4, 2021])
reward: tensor([ 0.1309, -0.2812, -0.9062, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1562])
attention_mask shape: torch.Size([4, 1562])
reward: tensor([-0.0311, -0.1533,  1.4688, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.5430, -0.3105, -1.5469,  1.2344], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1534])
attention_mask shape: torch.Size([4, 1534])
reward: tensor([ 0.4531,  1.3750,  0.2812, -0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.9609,  0.1875,  0.2871, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1594])
attention_mask shape: torch.Size([4, 1594])
reward: tensor([ 0.5820, -1.8438, -2.2031, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1242])
attention_mask shape: torch.Size([4, 1242])
reward: tensor([-0.4316,  0.2197, -0.6641, -1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 638])
attention_mask shape: torch.Size([4, 638])
reward: tensor([-0.4004, -0.2695,  0.1001, -0.1270], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 647])
attention_mask shape: torch.Size([4, 647])
reward: tensor([ 0.3027, -0.7188,  0.4551,  1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1403])
attention_mask shape: torch.Size([4, 1403])
reward: tensor([-0.9766, -1.0781,  0.1611,  1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1210])
attention_mask shape: torch.Size([4, 1210])
reward: tensor([-1.6797, -0.0089, -0.7070,  0.0266], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1324])
attention_mask shape: torch.Size([4, 1324])
reward: tensor([-0.0913, -1.2031,  1.0547, -0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 817])
attention_mask shape: torch.Size([4, 817])
reward: tensor([ 1.2734, -0.4707,  0.9023, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1571])
attention_mask shape: torch.Size([4, 1571])
reward: tensor([ 0.0067, -1.4297, -1.3750, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.6719, -0.9492, -0.1484,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1083])
attention_mask shape: torch.Size([4, 1083])
reward: tensor([ 0.1099, -0.4883,  0.9062,  1.5859], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1633])
attention_mask shape: torch.Size([4, 1633])
reward: tensor([ 0.2441, -0.2109, -1.7969,  0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 849])
attention_mask shape: torch.Size([4, 849])
reward: tensor([-1.0859, -0.1582, -0.6445,  0.1514], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 897])
attention_mask shape: torch.Size([4, 897])
reward: tensor([-0.0111,  0.5156, -1.5859, -0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1683])
attention_mask shape: torch.Size([4, 1683])
reward: tensor([ 0.3203, -1.3828, -0.5625, -0.6719], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1268])
attention_mask shape: torch.Size([4, 1268])
reward: tensor([-0.3027,  0.7305,  0.9805, -0.5234], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1429])
attention_mask shape: torch.Size([4, 1429])
reward: tensor([-0.5586, -0.8164, -0.4004, -0.0156], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 642])
attention_mask shape: torch.Size([4, 642])
reward: tensor([ 0.9062,  0.3828, -0.6250, -0.5508], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([-1.3750, -0.9688, -1.3828, -1.1719], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 963])
attention_mask shape: torch.Size([4, 963])
reward: tensor([ 0.7422, -0.2617, -0.6172,  1.3750], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1022])
attention_mask shape: torch.Size([4, 1022])
reward: tensor([-1.9219,  0.4082,  0.5469, -1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-1.0078,  2.0469, -0.7031,  0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 746])
attention_mask shape: torch.Size([4, 746])
reward: tensor([-0.8633, -0.7500, -1.0391,  0.0713], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1057])
attention_mask shape: torch.Size([4, 1057])
reward: tensor([-0.1533, -1.8047,  1.0078,  0.0991], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 727])
attention_mask shape: torch.Size([4, 727])
reward: tensor([-0.0033,  0.2275, -0.2695,  0.5312], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1367])
attention_mask shape: torch.Size([4, 1367])
reward: tensor([-0.5234,  0.3652, -1.4922,  1.5078], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1622])
attention_mask shape: torch.Size([4, 1622])
reward: tensor([-0.6992, -0.2334,  0.6406, -0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1845])
attention_mask shape: torch.Size([4, 1845])
reward: tensor([ 0.9648, -0.2598, -0.0011, -0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([-0.0864,  0.4219,  0.2129, -0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 631])
attention_mask shape: torch.Size([4, 631])
reward: tensor([-0.1309,  0.8984, -0.2910, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1081])
attention_mask shape: torch.Size([4, 1081])
reward: tensor([ 0.1953, -1.4219,  0.8281,  0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 821])
attention_mask shape: torch.Size([4, 821])
reward: tensor([ 0.5234,  0.5156, -0.3594,  0.4297], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1335])
attention_mask shape: torch.Size([4, 1335])
reward: tensor([-1.3516, -0.2871, -0.5938,  0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1340])
attention_mask shape: torch.Size([4, 1340])
reward: tensor([ 0.0688, -0.3594, -1.1172,  0.8008], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1368])
attention_mask shape: torch.Size([4, 1368])
reward: tensor([ 0.6445, -0.4082,  0.2266, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1363])
attention_mask shape: torch.Size([4, 1363])
reward: tensor([ 1.1875, -1.0156,  0.6328, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 782])
attention_mask shape: torch.Size([4, 782])
reward: tensor([ 0.4023, -0.9766, -0.6406, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1276])
attention_mask shape: torch.Size([4, 1276])
reward: tensor([ 2.1875, -1.7969,  0.6562, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1249])
attention_mask shape: torch.Size([4, 1249])
reward: tensor([ 0.5078,  0.5508,  0.1846, -0.6250], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([ 0.0255,  0.0366, -0.0645,  1.6094], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1489])
attention_mask shape: torch.Size([4, 1489])
reward: tensor([ 1.3828, -0.1445,  1.7266, -1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 774])
attention_mask shape: torch.Size([4, 774])
reward: tensor([-0.9141, -0.5195, -0.7734,  1.7578], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1554])
attention_mask shape: torch.Size([4, 1554])
reward: tensor([-0.5898, -1.3672, -0.7773,  1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([-0.1934,  0.5078,  1.1094,  0.9648], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1518])
attention_mask shape: torch.Size([4, 1518])
reward: tensor([-0.7930, -0.5742,  1.3125, -0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1953])
attention_mask shape: torch.Size([4, 1953])
reward: tensor([-0.3516, -0.1602,  0.1484, -0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1561])
attention_mask shape: torch.Size([4, 1561])
reward: tensor([0.7695, 0.4453, 0.9062, 0.3066], device='cuda:0', dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1469])
attention_mask shape: torch.Size([4, 1469])
reward: tensor([-0.1758,  0.5039, -0.7734, -0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1784])
attention_mask shape: torch.Size([4, 1784])
reward: tensor([-0.0378, -1.0781, -0.3281, -0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1398])
attention_mask shape: torch.Size([4, 1398])
reward: tensor([-0.5273, -0.0801, -0.5820,  0.2891], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1049])
attention_mask shape: torch.Size([4, 1049])
reward: tensor([-0.6484,  0.3320, -0.7461,  0.1177], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1387])
attention_mask shape: torch.Size([4, 1387])
reward: tensor([-0.3691, -0.8164, -1.9219,  0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 748])
attention_mask shape: torch.Size([4, 748])
reward: tensor([ 0.1758, -1.2500,  0.2773, -0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1657])
attention_mask shape: torch.Size([4, 1657])
reward: tensor([-0.3281, -0.5586,  1.2656,  0.0698], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1637])
attention_mask shape: torch.Size([4, 1637])
reward: tensor([-0.5391,  0.4316,  0.4883, -0.0089], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1090])
attention_mask shape: torch.Size([4, 1090])
reward: tensor([ 0.4180, -1.1250,  0.3184, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1885])
attention_mask shape: torch.Size([4, 1885])
reward: tensor([ 0.2080,  1.5391, -1.8281,  1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2011])
attention_mask shape: torch.Size([4, 2011])
reward: tensor([ 0.4785,  0.2500,  0.2451, -1.4922], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 686])
attention_mask shape: torch.Size([4, 686])
reward: tensor([ 0.2373, -0.1133,  0.0500,  0.1582], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1098])
attention_mask shape: torch.Size([4, 1098])
reward: tensor([-0.5352, -0.1670,  0.7383,  1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1576])
attention_mask shape: torch.Size([4, 1576])
reward: tensor([-1.3750,  0.0266,  0.7969, -1.1641], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-0.5156, -1.9609, -0.3203, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-2.2031, -0.0178, -0.5039, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1672])
attention_mask shape: torch.Size([4, 1672])
reward: tensor([-0.8398,  0.1289,  0.1001,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1060])
attention_mask shape: torch.Size([4, 1060])
reward: tensor([-1.4297, -1.0391,  0.8945,  1.2656], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1149])
attention_mask shape: torch.Size([4, 1149])
reward: tensor([-0.1245, -0.2793,  1.3828, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 776])
attention_mask shape: torch.Size([4, 776])
reward: tensor([-0.9883, -0.5078, -0.1758, -0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1567])
attention_mask shape: torch.Size([4, 1567])
reward: tensor([-0.3652,  1.6328, -1.9219, -0.8320], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1293])
attention_mask shape: torch.Size([4, 1293])
reward: tensor([-0.9688,  0.2422, -0.2178, -1.4141], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([-0.6250, -0.7539, -0.1089,  0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 569])
attention_mask shape: torch.Size([4, 569])
reward: tensor([-0.8594, -0.8789, -0.1484, -0.0045], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-0.0488, -0.7773, -0.6367, -0.2871], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 503])
attention_mask shape: torch.Size([4, 503])
reward: tensor([ 0.8672, -2.2031, -0.0444, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 880])
attention_mask shape: torch.Size([4, 880])
reward: tensor([ 0.8008,  0.8906, -0.0532,  0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 518])
attention_mask shape: torch.Size([4, 518])
reward: tensor([ 0.1299,  0.3379, -0.4004, -0.2490], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1215])
attention_mask shape: torch.Size([4, 1215])
reward: tensor([-0.0045, -0.5508,  0.5273,  0.1719], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1533])
attention_mask shape: torch.Size([4, 1533])
reward: tensor([-0.3828, -0.4629, -0.2637,  0.2363], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1436])
attention_mask shape: torch.Size([4, 1436])
reward: tensor([ 1.1562,  0.6562, -1.5234,  0.8945], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1509])
attention_mask shape: torch.Size([4, 1509])
reward: tensor([-0.8633, -0.8359, -0.1221,  0.8359], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 584])
attention_mask shape: torch.Size([4, 584])
reward: tensor([-1.7656, -0.1445, -0.7070, -0.6094], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1205])
attention_mask shape: torch.Size([4, 1205])
reward: tensor([-1.1641, -0.3730, -1.2344, -1.8828], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-1.7656, -0.0510, -0.0601, -1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 502])
attention_mask shape: torch.Size([4, 502])
reward: tensor([-0.5195, -1.0234, -0.6758, -0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.1055,  0.8164, -0.2539,  0.6719], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1752])
attention_mask shape: torch.Size([4, 1752])
reward: tensor([ 0.8789, -0.1982, -0.1689, -1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([-0.0111, -0.8984,  1.8906, -0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 908])
attention_mask shape: torch.Size([4, 908])
reward: tensor([-0.3828,  1.9531, -0.4219, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1355])
attention_mask shape: torch.Size([4, 1355])
reward: tensor([-0.6367,  0.6836, -0.1514,  0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 890])
attention_mask shape: torch.Size([4, 890])
reward: tensor([-0.8008, -0.9414, -0.7617, -0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 799])
attention_mask shape: torch.Size([4, 799])
reward: tensor([ 0.5156, -0.5781, -1.1484,  1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1845])
attention_mask shape: torch.Size([4, 1845])
reward: tensor([-0.0078, -0.3828, -2.1562, -0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1464])
attention_mask shape: torch.Size([4, 1464])
reward: tensor([ 0.1729, -0.1934, -0.6133,  1.2031], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 652])
attention_mask shape: torch.Size([4, 652])
reward: tensor([-0.6328, -0.8633, -0.8711, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1090])
attention_mask shape: torch.Size([4, 1090])
reward: tensor([-0.2227, -1.0312, -1.2969, -0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1741])
attention_mask shape: torch.Size([4, 1741])
reward: tensor([ 1.7188, -0.8711, -2.1719, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1667])
attention_mask shape: torch.Size([4, 1667])
reward: tensor([ 1.4609, -1.1875, -0.2891,  0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 565])
attention_mask shape: torch.Size([4, 565])
reward: tensor([-0.1177, -0.9062,  0.2129,  0.0588], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1359])
attention_mask shape: torch.Size([4, 1359])
reward: tensor([-1.1562,  0.5352, -0.6055,  1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-0.5586, -1.5000, -0.7305,  0.8359], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1253])
attention_mask shape: torch.Size([4, 1253])
reward: tensor([ 1.2266, -0.1113, -0.3203, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1512])
attention_mask shape: torch.Size([4, 1512])
reward: tensor([-0.6016,  0.2158, -0.8984,  0.1846], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1537])
attention_mask shape: torch.Size([4, 1537])
reward: tensor([ 1.0547, -0.4004,  2.4375,  0.3320], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1405])
attention_mask shape: torch.Size([4, 1405])
reward: tensor([ 1.5625, -0.7461,  0.2451,  1.4141], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1054])
attention_mask shape: torch.Size([4, 1054])
reward: tensor([-0.5820,  0.3555,  1.5078,  0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1603])
attention_mask shape: torch.Size([4, 1603])
reward: tensor([ 0.5820, -0.4980, -0.7070,  0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 435])
attention_mask shape: torch.Size([4, 435])
reward: tensor([-0.9961, -1.0391, -1.9844, -0.0422], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([-0.6680, -0.4395,  0.8008, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1510])
attention_mask shape: torch.Size([4, 1510])
reward: tensor([ 0.9688,  0.1387, -2.1562,  0.3574], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1639])
attention_mask shape: torch.Size([4, 1639])
reward: tensor([ 0.8750, -2.1406,  0.8516, -0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
[2024-10-27 12:37:10,357] [INFO] [launch.py:351:main] Process 790620 exits successfully.
sequences shape: torch.Size([4, 1509])
attention_mask shape: torch.Size([4, 1509])
reward: tensor([ 0.1709,  1.1094, -0.6211, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1885])
attention_mask shape: torch.Size([4, 1885])
reward: tensor([-0.1934, -0.7812,  0.1641, -1.8125], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1046])
attention_mask shape: torch.Size([4, 1046])
reward: tensor([-0.8789, -0.9336,  1.6797, -0.3906], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1666])
attention_mask shape: torch.Size([4, 1666])
reward: tensor([ 1.8750, -0.4707, -1.6562,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1788])
attention_mask shape: torch.Size([4, 1788])
reward: tensor([-0.2197, -0.0422,  2.0312, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 724])
attention_mask shape: torch.Size([4, 724])
reward: tensor([-0.2715, -1.7812, -0.0178,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1171])
attention_mask shape: torch.Size([4, 1171])
reward: tensor([-0.4707, -1.2656,  1.5078, -1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1330])
attention_mask shape: torch.Size([4, 1330])
reward: tensor([-0.2969, -0.8984, -1.2500, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1232])
attention_mask shape: torch.Size([4, 1232])
reward: tensor([-0.6836, -0.9609, -0.8281, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1491])
attention_mask shape: torch.Size([4, 1491])
reward: tensor([ 0.5234, -0.9414, -0.6523,  0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
[2024-10-27 12:39:07,476] [INFO] [launch.py:351:main] Process 790622 exits successfully.
sequences shape: torch.Size([4, 585])
attention_mask shape: torch.Size([4, 585])
reward: tensor([-0.6758,  0.9805,  0.0801,  0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1364])
attention_mask shape: torch.Size([4, 1364])
reward: tensor([-0.8164,  0.5547, -1.2031, -1.5938], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 475])
attention_mask shape: torch.Size([4, 475])
reward: tensor([-0.3379, -0.5586, -0.2314, -0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([-0.9766, -1.1719,  1.9609, -1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1575])
attention_mask shape: torch.Size([4, 1575])
reward: tensor([ 1.7188, -0.7148,  1.1406,  0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1286])
attention_mask shape: torch.Size([4, 1286])
reward: tensor([ 1.3672, -0.8438, -0.6172, -0.0977], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-27 12:40:20,549] [INFO] [launch.py:351:main] Process 790619 exits successfully.
[2024-10-27 12:43:41,756] [INFO] [launch.py:351:main] Process 790621 exits successfully.
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# [K(base) root@autodl-container-ec234bbd2e-925c6d34:~# bash run_eval_reward_openrlhf.sh
[?2004l+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-11th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-11th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_11 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-27 23:41:20,016] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 23:41:21,906] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-27 23:41:21,907] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-11th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_11 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-27 23:41:23,314] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 23:41:25,535] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-27 23:41:25,535] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-27 23:41:25,535] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-27 23:41:25,535] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-27 23:41:25,535] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-27 23:41:25,536] [INFO] [launch.py:256:main] process 861488 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-11th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_11', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-27 23:41:25,536] [INFO] [launch.py:256:main] process 861489 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-11th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_11', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-27 23:41:25,536] [INFO] [launch.py:256:main] process 861490 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-11th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_11', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-27 23:41:25,536] [INFO] [launch.py:256:main] process 861491 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-11th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_11', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-27 23:41:27,085] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 23:41:27,126] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 23:41:27,127] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-27 23:41:27,135] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-27 23:41:29,682] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-27 23:41:29,682] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-27 23:41:29,966] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-27 23:41:29,967] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-27 23:41:29,967] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.47it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.08it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.48it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.11it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  6.41it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.44it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.02it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.10it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.67it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.59it/s]
Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  6.53it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.10it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.15it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  6.67it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.77it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.49it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.93it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.79it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.26it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.21it/s]
[2024-10-27 23:41:33,363] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 23:41:33,649] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-27 23:41:33,654] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-27 23:41:33,655] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-27 23:41:33,655] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 23:41:33,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 23:41:34,153] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-27 23:41:34,153] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 23:41:34,153] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 23:41:34,154] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-27 23:41:34,154] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-27 23:41:34,320] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-27 23:41:34,321] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-27 23:41:34,321] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.05 GB, percent = 2.9%
[2024-10-27 23:41:34,463] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-27 23:41:34,464] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-27 23:41:34,464] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.05 GB, percent = 2.9%
[2024-10-27 23:41:34,465] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-27 23:41:34,465] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-27 23:41:34,465] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-27 23:41:34,465] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-27 23:41:34,465] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f01b0715c60>
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-27 23:41:34,466] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-27 23:41:34,467] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-27 23:41:34,467] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-27 23:41:34,468] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-27 23:41:34,468] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-27 23:41:34,468] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
[2024-10-27 23:41:39,583] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-27 23:41:39,585] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-27 23:41:39,776] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-27 23:41:39,777] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-27 23:41:39,777] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.38 GB, percent = 2.9%
[2024-10-27 23:41:39,916] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-27 23:41:39,917] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-27 23:41:39,917] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.39 GB, percent = 2.9%
[2024-10-27 23:41:39,918] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-27 23:41:39,918] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-27 23:41:39,918] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-27 23:41:39,918] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-27 23:41:39,918] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f01b02de7a0>
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-27 23:41:39,919] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-27 23:41:39,920] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-27 23:41:39,920] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 616/100000 [00:00<00:16, 6158.42it/s]Preprocessing data:   2%|█▌                                                                                          | 1645/100000 [00:00<00:11, 8587.04it/s]Preprocessing data:   3%|██▍                                                                                         | 2684/100000 [00:00<00:10, 9409.86it/s]Preprocessing data:   4%|███▍                                                                                        | 3682/100000 [00:00<00:09, 9633.50it/s]Preprocessing data:   5%|████▎                                                                                       | 4708/100000 [00:00<00:09, 9856.65it/s]Preprocessing data:   6%|█████▎                                                                                      | 5735/100000 [00:00<00:09, 9993.42it/s]Preprocessing data:   7%|██████▏                                                                                    | 6755/100000 [00:00<00:09, 10059.96it/s]Preprocessing data:   8%|███████                                                                                    | 7781/100000 [00:00<00:09, 10120.85it/s]Preprocessing data:   9%|████████                                                                                   | 8797/100000 [00:00<00:09, 10129.90it/s]Preprocessing data:  10%|████████▉                                                                                  | 9811/100000 [00:01<00:08, 10129.95it/s]Preprocessing data:  11%|█████████▊                                                                                | 10875/100000 [00:01<00:08, 10285.85it/s]Preprocessing data:  12%|██████████▋                                                                               | 11914/100000 [00:01<00:08, 10315.39it/s]Preprocessing data:  13%|███████████▋                                                                              | 12987/100000 [00:01<00:08, 10439.57it/s]Preprocessing data:  14%|████████████▋                                                                             | 14061/100000 [00:01<00:08, 10529.44it/s]Preprocessing data:  15%|█████████████▌                                                                            | 15133/100000 [00:01<00:08, 10586.30it/s]Preprocessing data:  16%|██████████████▌                                                                           | 16206/100000 [00:01<00:07, 10627.86it/s]Preprocessing data:  17%|███████████████▌                                                                          | 17269/100000 [00:01<00:07, 10614.66it/s]Preprocessing data:  18%|████████████████▌                                                                         | 18341/100000 [00:01<00:07, 10643.84it/s]Preprocessing data:  19%|█████████████████▍                                                                        | 19408/100000 [00:01<00:07, 10648.71it/s]Preprocessing data:  20%|██████████████████▍                                                                       | 20473/100000 [00:02<00:07, 10601.49it/s]Preprocessing data:  22%|███████████████████▍                                                                      | 21552/100000 [00:02<00:07, 10656.66it/s]Preprocessing data:  23%|████████████████████▎                                                                     | 22618/100000 [00:02<00:07, 10597.06it/s]Preprocessing data:  24%|█████████████████████▎                                                                    | 23678/100000 [00:02<00:07, 10311.84it/s]Preprocessing data:  25%|██████████████████████▎                                                                   | 24735/100000 [00:02<00:07, 10384.64it/s]Preprocessing data:  26%|███████████████████████▏                                                                  | 25788/100000 [00:02<00:07, 10424.73it/s]Preprocessing data:  27%|████████████████████████▏                                                                 | 26836/100000 [00:02<00:07, 10440.06it/s]Preprocessing data:  28%|█████████████████████████                                                                 | 27881/100000 [00:02<00:06, 10437.72it/s]Preprocessing data:  29%|██████████████████████████                                                                | 28928/100000 [00:02<00:06, 10444.68it/s]Preprocessing data:  30%|██████████████████████████▉                                                               | 29973/100000 [00:02<00:06, 10342.89it/s]Preprocessing data:  31%|███████████████████████████▉                                                              | 31019/100000 [00:03<00:06, 10377.52it/s]Preprocessing data:  32%|████████████████████████████▊                                                             | 32067/100000 [00:03<00:06, 10406.66it/s]Preprocessing data:  33%|█████████████████████████████▊                                                            | 33110/100000 [00:03<00:06, 10410.76it/s]Preprocessing data:  34%|██████████████████████████████▋                                                           | 34152/100000 [00:03<00:06, 10383.04it/s]Preprocessing data:  35%|███████████████████████████████▋                                                          | 35191/100000 [00:03<00:06, 10383.96it/s]Preprocessing data:  36%|████████████████████████████████▌                                                         | 36230/100000 [00:03<00:06, 10379.84it/s]Preprocessing data:  37%|█████████████████████████████████▌                                                        | 37271/100000 [00:03<00:06, 10387.61it/s]Preprocessing data:  38%|██████████████████████████████████▍                                                       | 38310/100000 [00:03<00:05, 10294.54it/s]Preprocessing data:  39%|███████████████████████████████████▍                                                      | 39345/100000 [00:03<00:05, 10310.21it/s]Preprocessing data:  40%|████████████████████████████████████▎                                                     | 40386/100000 [00:03<00:05, 10337.34it/s]Preprocessing data:  41%|█████████████████████████████████████▎                                                    | 41421/100000 [00:04<00:05, 10338.15it/s]Preprocessing data:  42%|██████████████████████████████████████▏                                                   | 42455/100000 [00:04<00:05, 10286.63it/s]Preprocessing data:  44%|███████████████████████████████████████▏                                                  | 43503/100000 [00:04<00:05, 10342.66it/s]Preprocessing data:  45%|████████████████████████████████████████                                                  | 44558/100000 [00:04<00:05, 10402.08it/s]Preprocessing data:  46%|█████████████████████████████████████████                                                 | 45610/100000 [00:04<00:05, 10436.92it/s]Preprocessing data:  47%|█████████████████████████████████████████▉                                                | 46654/100000 [00:04<00:05, 10411.01it/s]Preprocessing data:  48%|██████████████████████████████████████████▉                                               | 47699/100000 [00:04<00:05, 10420.94it/s]Preprocessing data:  49%|███████████████████████████████████████████▊                                              | 48742/100000 [00:04<00:04, 10292.33it/s]Preprocessing data:  50%|████████████████████████████████████████████▊                                             | 49787/100000 [00:04<00:04, 10336.09it/s]Preprocessing data:  51%|█████████████████████████████████████████████▋                                            | 50831/100000 [00:04<00:04, 10364.92it/s]Preprocessing data:  52%|██████████████████████████████████████████████▋                                           | 51868/100000 [00:05<00:04, 10328.39it/s]Preprocessing data:  53%|███████████████████████████████████████████████▋                                          | 52927/100000 [00:05<00:04, 10404.59it/s]Preprocessing data:  54%|████████████████████████████████████████████████▌                                         | 53988/100000 [00:05<00:04, 10464.27it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▌                                        | 55035/100000 [00:05<00:04, 10437.07it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▍                                       | 56096/100000 [00:05<00:04, 10488.06it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▍                                      | 57161/100000 [00:05<00:04, 10534.94it/s]Preprocessing data:  58%|████████████████████████████████████████████████████▍                                     | 58226/100000 [00:05<00:03, 10567.22it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████▎                                    | 59283/100000 [00:05<00:03, 10529.43it/s]Preprocessing data:  60%|██████████████████████████████████████████████████████▎                                   | 60344/100000 [00:05<00:03, 10550.69it/s]Preprocessing data:  61%|███████████████████████████████████████████████████████▎                                  | 61406/100000 [00:05<00:03, 10568.45it/s]Preprocessing data:  62%|████████████████████████████████████████████████████████▏                                 | 62480/100000 [00:06<00:03, 10617.88it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▏                                | 63542/100000 [00:06<00:03, 10615.87it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▏                               | 64604/100000 [00:06<00:03, 10443.76it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████                               | 65662/100000 [00:06<00:03, 10483.13it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████                              | 66722/100000 [00:06<00:03, 10515.04it/s]Preprocessing data:  68%|████████████████████████████████████████████████████████████▉                             | 67774/100000 [00:06<00:03, 10489.10it/s]Preprocessing data:  69%|█████████████████████████████████████████████████████████████▉                            | 68830/100000 [00:06<00:02, 10509.86it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▉                           | 69882/100000 [00:06<00:02, 10511.21it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▊                          | 70937/100000 [00:06<00:02, 10522.48it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▊                         | 71990/100000 [00:06<00:02, 10444.18it/s]Preprocessing data:  73%|██████████████████████████████████████████████████████████████████▍                        | 73035/100000 [00:07<00:02, 9975.32it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▎                       | 74037/100000 [00:07<00:02, 9275.54it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▏                      | 74976/100000 [00:07<00:02, 8801.26it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████                      | 75867/100000 [00:07<00:02, 8508.61it/s]Preprocessing data:  77%|█████████████████████████████████████████████████████████████████████▊                     | 76725/100000 [00:07<00:02, 8334.59it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▌                    | 77563/100000 [00:07<00:02, 8210.87it/s]Preprocessing data:  78%|███████████████████████████████████████████████████████████████████████▎                   | 78387/100000 [00:07<00:02, 8119.50it/s]Preprocessing data:  79%|████████████████████████████████████████████████████████████████████████                   | 79201/100000 [00:07<00:02, 8039.52it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▊                  | 80006/100000 [00:07<00:02, 7914.17it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▌                 | 80798/100000 [00:08<00:02, 7692.81it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▍                | 81824/100000 [00:08<00:02, 8420.65it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▏               | 82671/100000 [00:08<00:02, 8316.20it/s]Preprocessing data:  84%|███████████████████████████████████████████████████████████████████████████▉               | 83506/100000 [00:08<00:01, 8297.55it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▊              | 84343/100000 [00:08<00:01, 8317.36it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▋             | 85385/100000 [00:08<00:01, 8933.59it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▌            | 86281/100000 [00:08<00:01, 8877.01it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▎           | 87171/100000 [00:08<00:01, 8596.70it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████▏          | 88149/100000 [00:08<00:01, 8939.32it/s]Preprocessing data:  89%|█████████████████████████████████████████████████████████████████████████████████▏         | 89154/100000 [00:08<00:01, 9263.83it/s]Preprocessing data:  90%|██████████████████████████████████████████████████████████████████████████████████         | 90126/100000 [00:09<00:01, 9396.46it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▊        | 91068/100000 [00:09<00:01, 8623.30it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▋       | 91944/100000 [00:09<00:01, 7968.38it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▍      | 92758/100000 [00:09<00:00, 7635.86it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████      | 93534/100000 [00:09<00:00, 7569.75it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▉     | 94396/100000 [00:09<00:00, 7856.52it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▊    | 95390/100000 [00:09<00:00, 8442.46it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▋   | 96321/100000 [00:09<00:00, 8688.06it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▍  | 97198/100000 [00:09<00:00, 8667.43it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▎ | 98099/100000 [00:10<00:00, 8766.72it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████▏| 99063/100000 [00:10<00:00, 9023.23it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9747.09it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1813])
attention_mask shape: torch.Size([4, 1813])
reward: tensor([-0.4941, -1.1250,  0.1348, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.9609,  0.1484, -0.5156, -0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1091])
attention_mask shape: torch.Size([4, 1091])
reward: tensor([-0.7617, -0.8398, -0.9062, -0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 620])
attention_mask shape: torch.Size([4, 620])
reward: tensor([ 0.6133, -1.7266,  0.1465,  1.6797], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1414])
attention_mask shape: torch.Size([4, 1414])
reward: tensor([ 0.0864,  0.1079, -0.7070, -0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1235])
attention_mask shape: torch.Size([4, 1235])
reward: tensor([ 2.0312, -0.9141, -0.8125,  0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1686])
attention_mask shape: torch.Size([4, 1686])
reward: tensor([ 0.4785, -0.5938,  0.9375, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([ 0.2773, -1.7578,  0.6211,  0.4395], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1266])
attention_mask shape: torch.Size([4, 1266])
reward: tensor([ 0.2471, -0.8438, -0.9961, -0.2734], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1096])
attention_mask shape: torch.Size([4, 1096])
reward: tensor([ 0.3164,  1.4375, -1.0547, -1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 814])
attention_mask shape: torch.Size([4, 814])
reward: tensor([ 0.1787,  0.1167, -0.1777,  0.5234], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1090])
attention_mask shape: torch.Size([4, 1090])
reward: tensor([ 0.0311, -0.8789, -0.1826, -0.2402], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1250])
attention_mask shape: torch.Size([4, 1250])
reward: tensor([-1.3750, -0.5820, -0.0933,  0.2158], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1403])
attention_mask shape: torch.Size([4, 1403])
reward: tensor([-0.6641,  0.1807,  1.7500,  0.0378], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 476])
attention_mask shape: torch.Size([4, 476])
reward: tensor([-0.1689,  0.0400,  0.6562, -0.7539], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1448])
attention_mask shape: torch.Size([4, 1448])
reward: tensor([ 0.5234, -0.3457, -0.2354, -0.2676], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1530])
attention_mask shape: torch.Size([4, 1530])
reward: tensor([-0.5547, -0.5625, -0.9062,  1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1829])
attention_mask shape: torch.Size([4, 1829])
reward: tensor([-0.8789, -0.5508,  0.5352, -0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1272])
attention_mask shape: torch.Size([4, 1272])
reward: tensor([-1.0859, -0.4141, -0.7773, -0.7305], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 894])
attention_mask shape: torch.Size([4, 894])
reward: tensor([ 2.0781,  0.9961, -1.0781,  0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 975])
attention_mask shape: torch.Size([4, 975])
reward: tensor([-1.0781, -1.9141, -0.1064, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1377])
attention_mask shape: torch.Size([4, 1377])
reward: tensor([-1.0078,  0.9609,  0.6758,  1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 997])
attention_mask shape: torch.Size([4, 997])
reward: tensor([ 0.9375,  1.0547,  0.4102, -2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1737])
attention_mask shape: torch.Size([4, 1737])
reward: tensor([-1.4766, -0.3242,  1.1875,  0.9648], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1566])
attention_mask shape: torch.Size([4, 1566])
reward: tensor([ 1.6328, -0.0757, -1.9297, -0.6719], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1526])
attention_mask shape: torch.Size([4, 1526])
reward: tensor([ 0.5117, -0.5078,  0.2246, -0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1709])
attention_mask shape: torch.Size([4, 1709])
reward: tensor([-0.7500, -0.5508,  0.0557, -0.2178], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 341])
attention_mask shape: torch.Size([4, 341])
reward: tensor([ 0.3750, -1.6016, -0.5234, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 757])
attention_mask shape: torch.Size([4, 757])
reward: tensor([ 0.3066, -0.5742, -0.1621,  0.2617], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1743])
attention_mask shape: torch.Size([4, 1743])
reward: tensor([-0.4492,  0.7812, -0.7344, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1409])
attention_mask shape: torch.Size([4, 1409])
reward: tensor([-0.6641,  0.5391,  0.0488, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 832])
attention_mask shape: torch.Size([4, 832])
reward: tensor([-0.9414,  0.2598, -0.1113, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1570])
attention_mask shape: torch.Size([4, 1570])
reward: tensor([-0.5273, -0.4219, -0.5078, -1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1523])
attention_mask shape: torch.Size([4, 1523])
reward: tensor([ 0.2021,  0.4805, -0.4629, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 829])
attention_mask shape: torch.Size([4, 829])
reward: tensor([ 0.4668, -0.1553, -0.4492, -0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.9141, -0.6367,  0.7773, -1.4141], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1519])
attention_mask shape: torch.Size([4, 1519])
reward: tensor([ 0.0244, -0.6836, -0.2539,  0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1422])
attention_mask shape: torch.Size([4, 1422])
reward: tensor([-0.1089,  0.7539,  0.6055, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([-0.8125,  0.5938,  0.4238, -2.0156], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1685])
attention_mask shape: torch.Size([4, 1685])
reward: tensor([ 1.9609, -0.3691,  0.3281,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1449])
attention_mask shape: torch.Size([4, 1449])
reward: tensor([-0.0933,  0.5234,  1.9375,  0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([ 1.1562, -0.3594, -0.6836,  0.4863], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 438])
attention_mask shape: torch.Size([4, 438])
reward: tensor([-0.2969, -0.9609, -0.5859, -1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 712])
attention_mask shape: torch.Size([4, 712])
reward: tensor([-0.0776, -0.7734, -1.1797, -0.4395], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1655])
attention_mask shape: torch.Size([4, 1655])
reward: tensor([-0.2178,  1.7891,  0.8672,  1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 943])
attention_mask shape: torch.Size([4, 943])
reward: tensor([ 1.0078, -0.6797,  0.0579,  0.1099], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 758])
attention_mask shape: torch.Size([4, 758])
reward: tensor([ 0.8281,  0.6328, -0.0378, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1629])
attention_mask shape: torch.Size([4, 1629])
reward: tensor([ 0.0845, -0.4883, -0.2334,  0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1433])
attention_mask shape: torch.Size([4, 1433])
reward: tensor([-0.5547, -0.3691,  1.0000, -0.8320], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([ 0.0266, -1.5391,  0.5469,  0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1550])
attention_mask shape: torch.Size([4, 1550])
reward: tensor([-0.9609, -0.7383, -0.1729, -0.1602], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1279])
attention_mask shape: torch.Size([4, 1279])
reward: tensor([-0.5039,  1.6328,  0.2285,  0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1323])
attention_mask shape: torch.Size([4, 1323])
reward: tensor([-0.6758, -1.4609,  0.7734, -0.2402], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1181])
attention_mask shape: torch.Size([4, 1181])
reward: tensor([-2.0000, -0.9609, -0.2617,  0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1478])
attention_mask shape: torch.Size([4, 1478])
reward: tensor([-0.7070,  0.1592,  0.9609, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-1.0781, -1.4141,  0.9766, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1148])
attention_mask shape: torch.Size([4, 1148])
reward: tensor([-0.6914,  0.6836, -1.4844, -0.5898], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 850])
attention_mask shape: torch.Size([4, 850])
reward: tensor([-2.0781,  0.2090,  0.1709, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([ 0.4316,  0.6680,  0.9453, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1560])
attention_mask shape: torch.Size([4, 1560])
reward: tensor([-0.6406,  1.4297, -0.7539, -0.0845], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 993])
attention_mask shape: torch.Size([4, 993])
reward: tensor([ 0.5586, -0.3418, -1.1406, -1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1707])
attention_mask shape: torch.Size([4, 1707])
reward: tensor([-1.1094,  0.2676,  1.5000,  0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1560])
attention_mask shape: torch.Size([4, 1560])
reward: tensor([ 0.1245, -0.7930, -0.6914,  0.3828], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 829])
attention_mask shape: torch.Size([4, 829])
reward: tensor([ 0.7812,  0.5703, -0.3867, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1457])
attention_mask shape: torch.Size([4, 1457])
reward: tensor([ 0.3613, -0.8984,  1.7656, -1.6875], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 849])
attention_mask shape: torch.Size([4, 849])
reward: tensor([ 0.0033,  1.4141, -0.4668,  0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 906])
attention_mask shape: torch.Size([4, 906])
reward: tensor([-0.3828, -0.2930, -0.6484, -0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1496])
attention_mask shape: torch.Size([4, 1496])
reward: tensor([-0.7500, -1.2266,  0.6680,  1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1036])
attention_mask shape: torch.Size([4, 1036])
reward: tensor([1.6094, 0.1099, 0.7227, 0.3320], device='cuda:0', dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1294])
attention_mask shape: torch.Size([4, 1294])
reward: tensor([-0.5469,  0.2695, -0.1309,  0.3906], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1709])
attention_mask shape: torch.Size([4, 1709])
reward: tensor([-0.4883, -0.4492, -0.4746,  0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 829])
attention_mask shape: torch.Size([4, 829])
reward: tensor([ 0.8164, -1.2422, -0.5508, -1.6250], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1455])
attention_mask shape: torch.Size([4, 1455])
reward: tensor([-1.3984,  0.5117, -0.5586, -0.2402], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1295])
attention_mask shape: torch.Size([4, 1295])
reward: tensor([-0.4531, -0.5820, -0.3691, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 735])
attention_mask shape: torch.Size([4, 735])
reward: tensor([ 0.6484, -0.1934, -0.7539, -0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 926])
attention_mask shape: torch.Size([4, 926])
reward: tensor([ 0.8516, -0.1133,  0.5547, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1539])
attention_mask shape: torch.Size([4, 1539])
reward: tensor([ 1.8750,  0.0713, -0.5039, -1.6641], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2042])
attention_mask shape: torch.Size([4, 2042])
reward: tensor([ 0.8828,  0.4688, -0.6172, -1.8125], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1348])
attention_mask shape: torch.Size([4, 1348])
reward: tensor([-0.9766,  1.4219,  1.7578,  0.3184], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 663])
attention_mask shape: torch.Size([4, 663])
reward: tensor([-0.2373, -0.3281, -0.4746, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1143])
attention_mask shape: torch.Size([4, 1143])
reward: tensor([-0.3828, -0.0601,  1.7031, -1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1131])
attention_mask shape: torch.Size([4, 1131])
reward: tensor([ 1.9531, -0.2266,  0.2168, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1377])
attention_mask shape: torch.Size([4, 1377])
reward: tensor([-0.3906,  0.1689,  0.8281, -0.0713], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 485])
attention_mask shape: torch.Size([4, 485])
reward: tensor([-0.8086,  0.4746,  1.2891, -0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1581])
attention_mask shape: torch.Size([4, 1581])
reward: tensor([-0.5781, -0.7539, -0.3594,  0.3164], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2656, -1.0469,  0.0757, -0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1212])
attention_mask shape: torch.Size([4, 1212])
reward: tensor([-0.9766,  0.0776,  0.5938,  0.0791], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1955])
attention_mask shape: torch.Size([4, 1955])
reward: tensor([-0.6133,  0.8594, -0.0422, -0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1910])
attention_mask shape: torch.Size([4, 1910])
reward: tensor([ 0.1445, -0.9336,  0.7930, -0.1729], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 809])
attention_mask shape: torch.Size([4, 809])
reward: tensor([ 1.7422, -0.4258, -0.3652, -0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 842])
attention_mask shape: torch.Size([4, 842])
reward: tensor([-0.4746,  1.4766,  0.0967,  0.7773], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1662])
attention_mask shape: torch.Size([4, 1662])
reward: tensor([ 1.6328,  0.6133, -1.0703, -1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 766])
attention_mask shape: torch.Size([4, 766])
reward: tensor([ 0.9766, -1.2109,  1.1484, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1731])
attention_mask shape: torch.Size([4, 1731])
reward: tensor([ 1.8047,  0.7227, -0.1758,  0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1561])
attention_mask shape: torch.Size([4, 1561])
reward: tensor([ 0.2451, -0.2002,  0.2949,  0.3750], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 808])
attention_mask shape: torch.Size([4, 808])
reward: tensor([ 0.3398,  0.2158, -0.6719, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6094, -1.3047,  0.2598, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-0.3066,  0.2812,  1.8906, -0.2021], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1474])
attention_mask shape: torch.Size([4, 1474])
reward: tensor([ 0.0133, -0.4531,  0.0977, -0.2373], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1362])
attention_mask shape: torch.Size([4, 1362])
reward: tensor([-0.3457, -1.0469,  0.8086, -1.7500], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1204])
attention_mask shape: torch.Size([4, 1204])
reward: tensor([-1.1797, -0.3770, -0.1641,  0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1053])
attention_mask shape: torch.Size([4, 1053])
reward: tensor([-1.2031, -0.9688, -1.1250,  1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1276])
attention_mask shape: torch.Size([4, 1276])
reward: tensor([-0.1001, -0.6758, -0.2637,  0.3359], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1495])
attention_mask shape: torch.Size([4, 1495])
reward: tensor([ 0.9922, -0.0713,  0.0045,  1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([-0.7422, -0.9492, -1.1797, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1523])
attention_mask shape: torch.Size([4, 1523])
reward: tensor([-0.7734, -0.1045, -0.8516,  1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-1.3828,  0.3398, -0.4707,  0.5742], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1224])
attention_mask shape: torch.Size([4, 1224])
reward: tensor([ 0.2539, -0.4082, -0.7031, -1.6250], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([-0.1045, -2.0312,  0.2412, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1761])
attention_mask shape: torch.Size([4, 1761])
reward: tensor([-0.4707, -0.7070,  1.7422,  0.8320], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1615])
attention_mask shape: torch.Size([4, 1615])
reward: tensor([-0.1797,  0.2256,  0.8633,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 301])
attention_mask shape: torch.Size([4, 301])
reward: tensor([1.0312, 0.1221, 1.1719, 0.0732], device='cuda:0', dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1492])
attention_mask shape: torch.Size([4, 1492])
reward: tensor([-0.3418,  0.8711,  0.5430,  1.9219], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 777])
attention_mask shape: torch.Size([4, 777])
reward: tensor([-0.4570,  0.7930, -0.7656,  0.2559], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1178])
attention_mask shape: torch.Size([4, 1178])
reward: tensor([-1.1094, -0.8203, -0.1729,  0.1660], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1021])
attention_mask shape: torch.Size([4, 1021])
reward: tensor([-0.6641, -0.3281,  1.1719, -1.4141], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1321])
attention_mask shape: torch.Size([4, 1321])
reward: tensor([-0.3516,  0.6758,  1.0859, -0.0532], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1171])
attention_mask shape: torch.Size([4, 1171])
reward: tensor([ 0.3672, -0.0889,  0.7109, -0.2227], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 847])
attention_mask shape: torch.Size([4, 847])
reward: tensor([-1.1172, -0.2715, -0.3730, -0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1212])
attention_mask shape: torch.Size([4, 1212])
reward: tensor([-2.1719, -0.0601,  1.5625, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1244])
attention_mask shape: torch.Size([4, 1244])
reward: tensor([ 1.2969, -0.0801, -0.9062,  0.0742], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1996])
attention_mask shape: torch.Size([4, 1996])
reward: tensor([ 0.0588, -0.3340, -0.7930, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1456])
attention_mask shape: torch.Size([4, 1456])
reward: tensor([-0.8789,  0.9492,  1.7891, -1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1212])
attention_mask shape: torch.Size([4, 1212])
reward: tensor([-0.1396,  0.8281,  0.2266, -1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 805])
attention_mask shape: torch.Size([4, 805])
reward: tensor([-0.8633, -1.1484, -1.0469, -0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 775])
attention_mask shape: torch.Size([4, 775])
reward: tensor([-1.2812,  0.8828,  1.0312, -0.0200], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3555, -0.4043, -0.9766, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1266])
attention_mask shape: torch.Size([4, 1266])
reward: tensor([-0.4531,  0.0200,  0.1465, -0.1777], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1439])
attention_mask shape: torch.Size([4, 1439])
reward: tensor([-0.6562, -0.0466, -0.6211,  0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1553])
attention_mask shape: torch.Size([4, 1553])
reward: tensor([ 0.6836,  0.8320, -0.2129, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1811])
attention_mask shape: torch.Size([4, 1811])
reward: tensor([-0.4941,  0.1670, -0.2334, -0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1007])
attention_mask shape: torch.Size([4, 1007])
reward: tensor([-0.6172, -1.3516,  0.4707, -1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1313])
attention_mask shape: torch.Size([4, 1313])
reward: tensor([-0.0089, -0.6875, -1.0469, -1.9141], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 715])
attention_mask shape: torch.Size([4, 715])
reward: tensor([-0.0713, -1.5547, -0.4492,  0.4805], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 614])
attention_mask shape: torch.Size([4, 614])
reward: tensor([0.6680, 0.9297, 0.8828, 0.9297], device='cuda:0', dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1427])
attention_mask shape: torch.Size([4, 1427])
reward: tensor([-0.4043, -1.5469, -0.4043,  0.1553], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1367])
attention_mask shape: torch.Size([4, 1367])
reward: tensor([-0.8906, -0.2871, -0.6328, -1.3281], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1856])
attention_mask shape: torch.Size([4, 1856])
reward: tensor([ 1.1172, -2.0312, -0.3027,  0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 776])
attention_mask shape: torch.Size([4, 776])
reward: tensor([1.2109, 0.2480, 0.0757, 0.1758], device='cuda:0', dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1314])
attention_mask shape: torch.Size([4, 1314])
reward: tensor([ 0.5078, -0.6055,  0.8359, -0.4492], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-0.5039, -0.5156,  0.3086, -0.1289], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1214])
attention_mask shape: torch.Size([4, 1214])
reward: tensor([-0.4082, -0.0422,  1.0156, -0.1641], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1439])
attention_mask shape: torch.Size([4, 1439])
reward: tensor([-1.0703, -0.4570, -0.3203,  0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 961])
attention_mask shape: torch.Size([4, 961])
reward: tensor([-0.8633, -0.4004, -0.7930, -0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 732])
attention_mask shape: torch.Size([4, 732])
reward: tensor([-0.1445, -0.9141, -0.8398,  0.1777], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1317])
attention_mask shape: torch.Size([4, 1317])
reward: tensor([ 0.3203, -0.6367, -0.6211, -0.1289], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1303])
attention_mask shape: torch.Size([4, 1303])
reward: tensor([-0.2812,  0.2090,  0.0654, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 845])
attention_mask shape: torch.Size([4, 845])
reward: tensor([ 0.2637,  0.7930, -0.6797, -0.1221], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1102])
attention_mask shape: torch.Size([4, 1102])
reward: tensor([ 1.2344,  0.0045, -0.0820, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1168])
attention_mask shape: torch.Size([4, 1168])
reward: tensor([-0.4004,  0.3613, -0.5508,  0.0422], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 971])
attention_mask shape: torch.Size([4, 971])
reward: tensor([ 0.4746, -0.6992,  0.4648,  1.3281], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-0.6562,  0.1670,  0.0178, -1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-1.4141,  0.0723, -1.2266, -1.7656], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 943])
attention_mask shape: torch.Size([4, 943])
reward: tensor([-0.9766, -1.2109, -1.0234,  0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1101])
attention_mask shape: torch.Size([4, 1101])
reward: tensor([ 0.4980, -1.2266, -0.0933,  0.5664], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 599])
attention_mask shape: torch.Size([4, 599])
reward: tensor([-1.2969, -0.4531, -0.5820,  0.2266], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1297])
attention_mask shape: torch.Size([4, 1297])
reward: tensor([-0.3965,  0.0255, -0.7617,  0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1352])
attention_mask shape: torch.Size([4, 1352])
reward: tensor([-1.4141, -0.6719,  0.6055,  1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1767])
attention_mask shape: torch.Size([4, 1767])
reward: tensor([ 0.6562,  0.1055, -1.4609, -0.2266], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1275])
attention_mask shape: torch.Size([4, 1275])
reward: tensor([-0.1445,  1.6562,  0.5547,  0.2324], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 862])
attention_mask shape: torch.Size([4, 862])
reward: tensor([-0.4941,  1.1172,  0.5195, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1091])
attention_mask shape: torch.Size([4, 1091])
reward: tensor([ 0.7734, -0.5469,  0.6328,  0.8672], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1042])
attention_mask shape: torch.Size([4, 1042])
reward: tensor([-0.3555,  0.3047,  1.7891,  1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1221])
attention_mask shape: torch.Size([4, 1221])
reward: tensor([-1.0781, -0.3105, -0.5273,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1239])
attention_mask shape: torch.Size([4, 1239])
reward: tensor([-0.9336,  0.0457, -0.4980, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.6445, -0.9492,  1.0234, -1.3750], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1255])
attention_mask shape: torch.Size([4, 1255])
reward: tensor([ 1.2266, -0.5820,  0.1865, -0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 693])
attention_mask shape: torch.Size([4, 693])
reward: tensor([ 0.4258, -0.5273, -0.4004, -0.2285], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1205])
attention_mask shape: torch.Size([4, 1205])
reward: tensor([ 2.3750,  0.7500,  0.5078, -0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1568])
attention_mask shape: torch.Size([4, 1568])
reward: tensor([ 1.4062,  0.5469, -0.6367,  0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1857])
attention_mask shape: torch.Size([4, 1857])
reward: tensor([-0.0889, -0.6016, -1.5312,  1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1446])
attention_mask shape: torch.Size([4, 1446])
reward: tensor([ 1.4297,  0.0679,  1.6719, -0.4629], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1120])
attention_mask shape: torch.Size([4, 1120])
reward: tensor([-1.3281, -1.6875, -1.9609,  1.5703], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1263])
attention_mask shape: torch.Size([4, 1263])
reward: tensor([-0.7109,  0.0522, -1.9766,  0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 971])
attention_mask shape: torch.Size([4, 971])
reward: tensor([-0.7734,  0.2910,  0.3730,  0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1848])
attention_mask shape: torch.Size([4, 1848])
reward: tensor([-0.5039, -0.9062,  0.7930, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1337])
attention_mask shape: torch.Size([4, 1337])
reward: tensor([-0.7305,  0.2559,  0.3730, -0.4355], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1549])
attention_mask shape: torch.Size([4, 1549])
reward: tensor([-0.1064,  1.0469,  0.8008, -0.0334], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1611])
attention_mask shape: torch.Size([4, 1611])
reward: tensor([-0.1582,  1.6797,  0.5938, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1543])
attention_mask shape: torch.Size([4, 1543])
reward: tensor([ 0.1055, -1.1641, -0.2266,  0.2471], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 913])
attention_mask shape: torch.Size([4, 913])
reward: tensor([ 0.1143,  0.4316, -0.7188,  0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 791])
attention_mask shape: torch.Size([4, 791])
reward: tensor([-0.1553, -1.3359, -0.8984, -0.0645], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1537])
attention_mask shape: torch.Size([4, 1537])
reward: tensor([-0.4219, -0.6641, -1.3594,  1.7188], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 790])
attention_mask shape: torch.Size([4, 790])
reward: tensor([ 2.0781, -1.4766, -1.2969, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1796])
attention_mask shape: torch.Size([4, 1796])
reward: tensor([-1.1250, -0.4141,  1.1719, -0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1763])
attention_mask shape: torch.Size([4, 1763])
reward: tensor([-0.4883, -0.3203,  0.5898, -0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1250])
attention_mask shape: torch.Size([4, 1250])
reward: tensor([ 0.2471, -0.3457,  0.5742, -0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1731])
attention_mask shape: torch.Size([4, 1731])
reward: tensor([ 0.6133,  0.8828, -1.5938, -0.5234], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6523, -1.1172,  0.8750,  0.0923], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1152])
attention_mask shape: torch.Size([4, 1152])
reward: tensor([-0.0623, -0.8047,  0.2598,  0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 848])
attention_mask shape: torch.Size([4, 848])
reward: tensor([-0.4355, -0.5078,  0.8711,  0.2480], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1753])
attention_mask shape: torch.Size([4, 1753])
reward: tensor([-0.8789, -0.3652,  1.7422, -0.0757], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1291])
attention_mask shape: torch.Size([4, 1291])
reward: tensor([-0.5156, -0.5781, -0.0011, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-1.5078, -0.9258, -0.8164, -1.3281], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1501])
attention_mask shape: torch.Size([4, 1501])
reward: tensor([ 0.6641, -0.0669,  0.6484,  0.3574], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 770])
attention_mask shape: torch.Size([4, 770])
reward: tensor([-0.4941, -1.7500,  1.0938,  0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1221])
attention_mask shape: torch.Size([4, 1221])
reward: tensor([-0.8398, -0.9766,  1.3594, -0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 735])
attention_mask shape: torch.Size([4, 735])
reward: tensor([-0.2246, -0.6523, -0.7188, -0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1610])
attention_mask shape: torch.Size([4, 1610])
reward: tensor([ 0.2773,  1.5391, -0.0623, -0.6445], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1819])
attention_mask shape: torch.Size([4, 1819])
reward: tensor([-1.5547,  0.3262, -0.2793, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1124])
attention_mask shape: torch.Size([4, 1124])
reward: tensor([-0.4941, -0.4180, -0.1582,  1.2031], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 807])
attention_mask shape: torch.Size([4, 807])
reward: tensor([-0.3691, -0.5156,  1.0312,  0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 714])
attention_mask shape: torch.Size([4, 714])
reward: tensor([-0.4316, -0.2676, -0.7227,  0.4648], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 782])
attention_mask shape: torch.Size([4, 782])
reward: tensor([-0.6250, -2.2188, -0.0356, -0.0713], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 960])
attention_mask shape: torch.Size([4, 960])
reward: tensor([ 0.5625,  0.2793, -0.1582,  0.1475], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 517])
attention_mask shape: torch.Size([4, 517])
reward: tensor([-0.3770,  0.5430, -0.4141, -0.0045], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 638])
attention_mask shape: torch.Size([4, 638])
reward: tensor([-0.2285, -0.4570,  0.0244, -0.1670], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1320])
attention_mask shape: torch.Size([4, 1320])
reward: tensor([-0.2197, -0.4043,  0.1055,  0.2363], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1434])
attention_mask shape: torch.Size([4, 1434])
reward: tensor([ 0.9688,  1.2422, -1.2266,  0.3848], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1609])
attention_mask shape: torch.Size([4, 1609])
reward: tensor([-1.2500, -0.5508, -0.2695,  0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 776])
attention_mask shape: torch.Size([4, 776])
reward: tensor([-0.7969, -0.4492, -1.5391, -0.8320], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1141])
attention_mask shape: torch.Size([4, 1141])
reward: tensor([-0.7188, -0.3027, -1.4141, -1.4844], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-0.6406,  1.2969,  1.2969, -0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 951])
attention_mask shape: torch.Size([4, 951])
reward: tensor([-0.3516, -1.2656, -0.7461,  0.1201], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.0089,  0.9727, -0.0444,  0.1299], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1462])
attention_mask shape: torch.Size([4, 1462])
reward: tensor([ 0.4824,  0.6484, -0.6914, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 829])
attention_mask shape: torch.Size([4, 829])
reward: tensor([ 0.6562, -0.6523,  1.6875, -1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 983])
attention_mask shape: torch.Size([4, 983])
reward: tensor([-1.0859,  0.7188,  0.8359, -0.4883], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1346])
attention_mask shape: torch.Size([4, 1346])
reward: tensor([ 0.8438,  0.4707, -0.5273,  0.4492], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1078])
attention_mask shape: torch.Size([4, 1078])
reward: tensor([-0.1289, -0.8594,  0.4258, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 897])
attention_mask shape: torch.Size([4, 897])
reward: tensor([ 0.7109, -1.0859, -0.2520,  1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1292])
attention_mask shape: torch.Size([4, 1292])
reward: tensor([-0.5625, -0.1133, -1.4922, -1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1702])
attention_mask shape: torch.Size([4, 1702])
reward: tensor([ 0.0522, -0.1021, -0.3770,  1.8594], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1235])
attention_mask shape: torch.Size([4, 1235])
reward: tensor([ 0.1001, -1.8047, -1.3750, -0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1026])
attention_mask shape: torch.Size([4, 1026])
reward: tensor([ 0.1035, -0.7930, -1.3359,  0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1315])
attention_mask shape: torch.Size([4, 1315])
reward: tensor([ 1.2812, -0.6367, -0.4316,  0.2471], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1722])
attention_mask shape: torch.Size([4, 1722])
reward: tensor([-2.0000, -0.7031, -0.7930,  0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 532])
attention_mask shape: torch.Size([4, 532])
reward: tensor([-0.2109, -1.1719,  0.0457,  0.5820], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-0.9609,  0.7305, -0.2246,  0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 845])
attention_mask shape: torch.Size([4, 845])
reward: tensor([-1.1016, -1.5234, -1.6250, -0.0601], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1480])
attention_mask shape: torch.Size([4, 1480])
reward: tensor([ 0.9883, -0.0688,  0.2285, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1307])
attention_mask shape: torch.Size([4, 1307])
reward: tensor([-0.0244, -0.3652, -0.9414,  0.1299], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1382])
attention_mask shape: torch.Size([4, 1382])
reward: tensor([ 0.6562, -1.0078,  2.2656,  0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1577])
attention_mask shape: torch.Size([4, 1577])
reward: tensor([ 2.0312, -0.5352, -0.9258,  1.6016], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1138])
attention_mask shape: torch.Size([4, 1138])
reward: tensor([ 0.1245, -0.3105,  1.5078,  1.8750], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1643])
attention_mask shape: torch.Size([4, 1643])
reward: tensor([ 0.5430, -0.3770, -0.8633,  0.2832], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 527])
attention_mask shape: torch.Size([4, 527])
reward: tensor([-1.4219, -1.2266, -0.0178,  0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1526])
attention_mask shape: torch.Size([4, 1526])
reward: tensor([-0.6172,  0.1670, -0.6836, -1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1866])
attention_mask shape: torch.Size([4, 1866])
reward: tensor([ 1.1562, -0.4219, -1.2422, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 924])
attention_mask shape: torch.Size([4, 924])
reward: tensor([-1.0391, -1.8672,  0.6680, -0.1113], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1114])
attention_mask shape: torch.Size([4, 1114])
reward: tensor([ 0.5430,  1.8203,  1.1484, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.1245,  0.0045, -0.1045,  0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([-0.4219, -0.6055,  0.9453, -0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1615])
attention_mask shape: torch.Size([4, 1615])
reward: tensor([ 2.0781,  0.9141, -1.0312,  0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1617])
attention_mask shape: torch.Size([4, 1617])
reward: tensor([-0.5352, -0.9492,  2.0938, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 983])
attention_mask shape: torch.Size([4, 983])
reward: tensor([-0.4746, -0.0623, -1.1875,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 975])
attention_mask shape: torch.Size([4, 975])
reward: tensor([0.2490, 0.4570, 0.8789, 0.3555], device='cuda:0', dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1276])
attention_mask shape: torch.Size([4, 1276])
reward: tensor([ 0.1621, -0.6406, -0.5859, -0.1729], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1292])
attention_mask shape: torch.Size([4, 1292])
reward: tensor([-1.1484, -1.7031, -0.4043,  0.0835], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1279])
attention_mask shape: torch.Size([4, 1279])
reward: tensor([ 0.5703, -0.8984, -0.6836,  0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 589])
attention_mask shape: torch.Size([4, 589])
reward: tensor([-0.9258,  0.8594,  0.1079,  0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1462])
attention_mask shape: torch.Size([4, 1462])
reward: tensor([-0.6211,  1.3281, -0.4082, -1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
[2024-10-28 00:32:41,311] [INFO] [launch.py:351:main] Process 861489 exits successfully.
sequences shape: torch.Size([4, 873])
attention_mask shape: torch.Size([4, 873])
reward: tensor([-1.0078, -0.5898,  1.1094, -1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
[2024-10-28 00:32:56,327] [INFO] [launch.py:351:main] Process 861490 exits successfully.
sequences shape: torch.Size([4, 834])
attention_mask shape: torch.Size([4, 834])
reward: tensor([ 0.5469, -1.0078,  1.2656, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1729])
attention_mask shape: torch.Size([4, 1729])
reward: tensor([ 1.2031, -0.0334,  1.3125, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1301])
attention_mask shape: torch.Size([4, 1301])
reward: tensor([ 0.5469, -1.1172,  0.5781, -0.2578], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-28 00:33:46,378] [INFO] [launch.py:351:main] Process 861488 exits successfully.
[2024-10-28 00:35:23,477] [INFO] [launch.py:351:main] Process 861491 exits successfully.
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# [K(base) root@autodl-container-ec234bbd2e-925c6d34:~# bash run_eval_reward_openrlhf.sh
[?2004l+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-12th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-12th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_12 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-28 11:13:34,961] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-28 11:13:37,118] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-28 11:13:37,119] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-12th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_12 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-28 11:13:38,468] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-28 11:13:40,324] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-28 11:13:40,324] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-28 11:13:40,324] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-28 11:13:40,324] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-28 11:13:40,324] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-28 11:13:40,325] [INFO] [launch.py:256:main] process 879744 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-12th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_12', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-28 11:13:40,325] [INFO] [launch.py:256:main] process 879745 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-12th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_12', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-28 11:13:40,325] [INFO] [launch.py:256:main] process 879746 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-12th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_12', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-28 11:13:40,325] [INFO] [launch.py:256:main] process 879747 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_NashRS_512prompt_trivial-12th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_NashRS_12', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-28 11:13:43,201] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-28 11:13:43,203] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-28 11:13:43,214] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-28 11:13:43,217] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-28 11:13:45,667] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-28 11:13:45,667] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-28 11:13:46,019] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-28 11:13:46,063] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-28 11:13:46,069] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.76it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.52it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.79it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.55it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.81it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.55it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.08it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.97it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.81it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.70it/s]
[2024-10-28 11:13:48,454] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.37it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.46it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.58it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.51it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.62it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.78it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.66it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.66it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.91it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.80it/s]
[2024-10-28 11:13:58,138] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-28 11:14:08,011] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-28 11:14:08,192] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-28 11:14:08,192] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-28 11:14:08,192] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-28 11:14:08,667] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-28 11:14:08,668] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-28 11:14:08,668] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-28 11:14:08,668] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-28 11:14:08,669] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-28 11:14:08,797] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-28 11:14:08,798] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-28 11:14:08,798] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.07 GB, percent = 2.9%
[2024-10-28 11:14:08,930] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-28 11:14:08,931] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-28 11:14:08,931] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.07 GB, percent = 2.9%
[2024-10-28 11:14:08,932] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8a68481f30>
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-28 11:14:08,933] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-28 11:14:08,934] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-28 11:14:08,935] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-28 11:14:08,935] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-28 11:14:08,935] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-28 11:14:08,935] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-28 11:14:08,935] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-28 11:14:11,883] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-28 11:14:11,885] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-28 11:14:12,008] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-28 11:14:12,009] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-28 11:14:12,010] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.1 GB, percent = 2.9%
[2024-10-28 11:14:12,129] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-28 11:14:12,130] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-28 11:14:12,130] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.09 GB, percent = 2.9%
[2024-10-28 11:14:12,131] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-28 11:14:12,131] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-28 11:14:12,131] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-28 11:14:12,131] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-28 11:14:12,131] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8a595f2800>
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-28 11:14:12,132] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-28 11:14:12,133] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-28 11:14:12,133] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 655/100000 [00:00<00:15, 6548.09it/s]Preprocessing data:   2%|█▌                                                                                          | 1671/100000 [00:00<00:11, 8667.76it/s]Preprocessing data:   3%|██▌                                                                                         | 2720/100000 [00:00<00:10, 9497.43it/s]Preprocessing data:   4%|███▍                                                                                        | 3752/100000 [00:00<00:09, 9819.46it/s]Preprocessing data:   5%|████▍                                                                                       | 4782/100000 [00:00<00:09, 9990.47it/s]Preprocessing data:   6%|█████▎                                                                                     | 5810/100000 [00:00<00:09, 10087.16it/s]Preprocessing data:   7%|██████▏                                                                                    | 6836/100000 [00:00<00:09, 10141.55it/s]Preprocessing data:   8%|███████▏                                                                                   | 7866/100000 [00:00<00:09, 10191.22it/s]Preprocessing data:   9%|████████                                                                                   | 8890/100000 [00:00<00:08, 10206.14it/s]Preprocessing data:  10%|█████████                                                                                  | 9919/100000 [00:01<00:08, 10229.46it/s]Preprocessing data:  11%|█████████▉                                                                                | 11006/100000 [00:01<00:08, 10423.77it/s]Preprocessing data:  12%|██████████▉                                                                               | 12097/100000 [00:01<00:08, 10568.63it/s]Preprocessing data:  13%|███████████▊                                                                              | 13184/100000 [00:01<00:08, 10657.53it/s]Preprocessing data:  14%|████████████▊                                                                             | 14267/100000 [00:01<00:08, 10708.15it/s]Preprocessing data:  15%|█████████████▊                                                                            | 15350/100000 [00:01<00:07, 10742.08it/s]Preprocessing data:  16%|██████████████▊                                                                           | 16430/100000 [00:01<00:07, 10757.74it/s]Preprocessing data:  18%|███████████████▊                                                                          | 17506/100000 [00:01<00:07, 10691.15it/s]Preprocessing data:  19%|████████████████▋                                                                         | 18580/100000 [00:01<00:07, 10702.89it/s]Preprocessing data:  20%|█████████████████▋                                                                        | 19651/100000 [00:01<00:07, 10479.87it/s]Preprocessing data:  21%|██████████████████▋                                                                       | 20759/100000 [00:02<00:07, 10654.14it/s]Preprocessing data:  22%|███████████████████▋                                                                      | 21867/100000 [00:02<00:07, 10779.74it/s]Preprocessing data:  23%|████████████████████▋                                                                     | 22947/100000 [00:02<00:07, 10783.56it/s]Preprocessing data:  24%|█████████████████████▌                                                                    | 24026/100000 [00:02<00:07, 10771.74it/s]Preprocessing data:  25%|██████████████████████▌                                                                   | 25104/100000 [00:02<00:06, 10755.50it/s]Preprocessing data:  26%|███████████████████████▌                                                                  | 26180/100000 [00:02<00:06, 10733.67it/s]Preprocessing data:  27%|████████████████████████▌                                                                 | 27254/100000 [00:02<00:06, 10721.97it/s]Preprocessing data:  28%|█████████████████████████▍                                                                | 28327/100000 [00:02<00:06, 10707.14it/s]Preprocessing data:  29%|██████████████████████████▍                                                               | 29398/100000 [00:02<00:06, 10679.87it/s]Preprocessing data:  30%|███████████████████████████▍                                                              | 30467/100000 [00:02<00:06, 10667.57it/s]Preprocessing data:  32%|████████████████████████████▍                                                             | 31534/100000 [00:03<00:06, 10639.32it/s]Preprocessing data:  33%|█████████████████████████████▎                                                            | 32598/100000 [00:03<00:06, 10623.89it/s]Preprocessing data:  34%|██████████████████████████████▎                                                           | 33661/100000 [00:03<00:06, 10592.82it/s]Preprocessing data:  35%|███████████████████████████████▏                                                          | 34721/100000 [00:03<00:06, 10577.59it/s]Preprocessing data:  36%|████████████████████████████████▏                                                         | 35779/100000 [00:03<00:06, 10556.13it/s]Preprocessing data:  37%|█████████████████████████████████▏                                                        | 36837/100000 [00:03<00:05, 10562.21it/s]Preprocessing data:  38%|██████████████████████████████████                                                        | 37894/100000 [00:03<00:05, 10557.52it/s]Preprocessing data:  39%|███████████████████████████████████                                                       | 38950/100000 [00:03<00:05, 10532.52it/s]Preprocessing data:  40%|████████████████████████████████████                                                      | 40004/100000 [00:03<00:05, 10517.75it/s]Preprocessing data:  41%|████████████████████████████████████▉                                                     | 41061/100000 [00:03<00:05, 10532.35it/s]Preprocessing data:  42%|█████████████████████████████████████▉                                                    | 42115/100000 [00:04<00:05, 10529.28it/s]Preprocessing data:  43%|██████████████████████████████████████▊                                                   | 43178/100000 [00:04<00:05, 10558.41it/s]Preprocessing data:  44%|███████████████████████████████████████▊                                                  | 44258/100000 [00:04<00:05, 10630.15it/s]Preprocessing data:  45%|████████████████████████████████████████▊                                                 | 45340/100000 [00:04<00:05, 10684.35it/s]Preprocessing data:  46%|█████████████████████████████████████████▊                                                | 46426/100000 [00:04<00:04, 10736.05it/s]Preprocessing data:  48%|██████████████████████████████████████████▊                                               | 47511/100000 [00:04<00:04, 10769.79it/s]Preprocessing data:  49%|███████████████████████████████████████████▋                                              | 48588/100000 [00:04<00:04, 10748.32it/s]Preprocessing data:  50%|████████████████████████████████████████████▋                                             | 49663/100000 [00:04<00:04, 10748.12it/s]Preprocessing data:  51%|█████████████████████████████████████████████▋                                            | 50744/100000 [00:04<00:04, 10765.50it/s]Preprocessing data:  52%|██████████████████████████████████████████████▋                                           | 51821/100000 [00:04<00:04, 10765.73it/s]Preprocessing data:  53%|███████████████████████████████████████████████▌                                          | 52904/100000 [00:05<00:04, 10783.14it/s]Preprocessing data:  54%|████████████████████████████████████████████████▌                                         | 53988/100000 [00:05<00:04, 10798.81it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▌                                        | 55070/100000 [00:05<00:04, 10802.43it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▌                                       | 56154/100000 [00:05<00:04, 10812.29it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▌                                      | 57236/100000 [00:05<00:03, 10738.99it/s]Preprocessing data:  58%|████████████████████████████████████████████████████▍                                     | 58312/100000 [00:05<00:03, 10743.27it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████▍                                    | 59392/100000 [00:05<00:03, 10756.85it/s]Preprocessing data:  60%|██████████████████████████████████████████████████████▍                                   | 60468/100000 [00:05<00:03, 10754.99it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▍                                  | 61544/100000 [00:05<00:03, 10726.97it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▎                                 | 62635/100000 [00:05<00:03, 10781.35it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▎                                | 63714/100000 [00:06<00:03, 10782.25it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▎                               | 64793/100000 [00:06<00:03, 10753.55it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▎                              | 65869/100000 [00:06<00:03, 10747.05it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████▏                             | 66944/100000 [00:06<00:03, 10745.14it/s]Preprocessing data:  68%|█████████████████████████████████████████████████████████████▏                            | 68019/100000 [00:06<00:02, 10713.42it/s]Preprocessing data:  69%|██████████████████████████████████████████████████████████████▏                           | 69093/100000 [00:06<00:02, 10718.57it/s]Preprocessing data:  70%|███████████████████████████████████████████████████████████████▏                          | 70168/100000 [00:06<00:02, 10726.77it/s]Preprocessing data:  71%|████████████████████████████████████████████████████████████████                          | 71241/100000 [00:06<00:02, 10718.18it/s]Preprocessing data:  72%|█████████████████████████████████████████████████████████████████                         | 72313/100000 [00:06<00:02, 10705.44it/s]Preprocessing data:  73%|██████████████████████████████████████████████████████████████████▊                        | 73384/100000 [00:06<00:02, 9868.72it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▋                       | 74384/100000 [00:07<00:02, 9235.67it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▌                      | 75323/100000 [00:07<00:02, 8873.45it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████▎                     | 76222/100000 [00:07<00:02, 8623.33it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▏                    | 77092/100000 [00:07<00:02, 8476.60it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▉                    | 77945/100000 [00:07<00:02, 8364.35it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▋                   | 78785/100000 [00:07<00:02, 8264.56it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▍                  | 79613/100000 [00:07<00:02, 8207.61it/s]Preprocessing data:  80%|█████████████████████████████████████████████████████████████████████████▏                 | 80435/100000 [00:07<00:02, 7979.94it/s]Preprocessing data:  81%|██████████████████████████████████████████████████████████████████████████                 | 81338/100000 [00:07<00:02, 8276.64it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▊                | 82229/100000 [00:08<00:02, 8459.36it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▌               | 83101/100000 [00:08<00:01, 8533.63it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▍              | 83957/100000 [00:08<00:01, 8198.54it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▎             | 85011/100000 [00:08<00:01, 8869.63it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▎            | 86002/100000 [00:08<00:01, 9171.51it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████            | 86924/100000 [00:08<00:01, 8654.14it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████           | 87923/100000 [00:08<00:01, 9031.16it/s]Preprocessing data:  89%|████████████████████████████████████████████████████████████████████████████████▉          | 88935/100000 [00:08<00:01, 9343.53it/s]Preprocessing data:  90%|█████████████████████████████████████████████████████████████████████████████████▊         | 89944/100000 [00:08<00:01, 9560.99it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▋        | 90906/100000 [00:09<00:01, 8896.35it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▌       | 91809/100000 [00:09<00:00, 8351.81it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▎      | 92658/100000 [00:09<00:00, 7981.27it/s]Preprocessing data:  93%|█████████████████████████████████████████████████████████████████████████████████████      | 93467/100000 [00:09<00:00, 7841.28it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▊     | 94318/100000 [00:09<00:00, 8023.38it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▊    | 95338/100000 [00:09<00:00, 8634.37it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▋   | 96293/100000 [00:09<00:00, 8896.01it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▍  | 97190/100000 [00:09<00:00, 8850.38it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▎ | 98094/100000 [00:09<00:00, 8904.58it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████▏| 99102/100000 [00:09<00:00, 9250.23it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9955.33it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.4707, -1.2109,  0.3848, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.9375, -1.6562,  1.2266, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1138])
attention_mask shape: torch.Size([4, 1138])
reward: tensor([-1.3281, -0.6250, -0.6133, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1229])
attention_mask shape: torch.Size([4, 1229])
reward: tensor([-0.9961, -2.2031,  0.4355,  0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1443])
attention_mask shape: torch.Size([4, 1443])
reward: tensor([-0.0056,  0.2236, -0.8164, -0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1222])
attention_mask shape: torch.Size([4, 1222])
reward: tensor([ 1.9141, -0.5039, -0.3281,  0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1954])
attention_mask shape: torch.Size([4, 1954])
reward: tensor([-0.4141, -0.4316,  0.7109, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([-0.5195, -2.0781,  0.6328,  0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1082])
attention_mask shape: torch.Size([4, 1082])
reward: tensor([-0.8984, -0.1357, -0.1377, -0.2676], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1063])
attention_mask shape: torch.Size([4, 1063])
reward: tensor([ 0.2949,  1.1562, -0.2354, -0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1619])
attention_mask shape: torch.Size([4, 1619])
reward: tensor([-0.0334,  0.0791, -1.1641, -0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 989])
attention_mask shape: torch.Size([4, 989])
reward: tensor([ 0.4883, -0.2021,  0.1245,  0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 803])
attention_mask shape: torch.Size([4, 803])
reward: tensor([-1.2969, -0.8516, -0.5938, -0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1905])
attention_mask shape: torch.Size([4, 1905])
reward: tensor([-0.4980,  0.1079,  2.1562,  0.0991], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 779])
attention_mask shape: torch.Size([4, 779])
reward: tensor([ 0.1299, -0.7539,  1.0156, -0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1544])
attention_mask shape: torch.Size([4, 1544])
reward: tensor([ 0.8594,  0.9297, -0.3164, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1918])
attention_mask shape: torch.Size([4, 1918])
reward: tensor([-0.1001, -0.9688, -0.4707,  0.0334], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1779])
attention_mask shape: torch.Size([4, 1779])
reward: tensor([ 0.6758, -0.0801, -1.3438, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1379])
attention_mask shape: torch.Size([4, 1379])
reward: tensor([-1.0703, -0.1670, -1.7656, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 993])
attention_mask shape: torch.Size([4, 993])
reward: tensor([ 1.9219,  0.9297, -1.2656, -0.2598], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1678])
attention_mask shape: torch.Size([4, 1678])
reward: tensor([-1.7188, -0.3516, -1.5469, -0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1232])
attention_mask shape: torch.Size([4, 1232])
reward: tensor([ 0.2773,  0.7109, -1.1406,  0.8125], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1006])
attention_mask shape: torch.Size([4, 1006])
reward: tensor([-0.7656,  0.6328, -0.2734, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1347])
attention_mask shape: torch.Size([4, 1347])
reward: tensor([-1.1016, -0.2676,  1.0859,  0.2256], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7070, -0.9688, -0.4805, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1348])
attention_mask shape: torch.Size([4, 1348])
reward: tensor([ 0.8945, -1.4141,  0.0457, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1641])
attention_mask shape: torch.Size([4, 1641])
reward: tensor([-0.8008, -0.5508,  0.4707, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 450])
attention_mask shape: torch.Size([4, 450])
reward: tensor([ 0.9805, -0.7383, -0.0111,  0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1484])
attention_mask shape: torch.Size([4, 1484])
reward: tensor([-0.4316, -0.7852, -0.2930, -0.1953], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2024])
attention_mask shape: torch.Size([4, 2024])
reward: tensor([-0.8438, -0.8320, -0.2539, -1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1507])
attention_mask shape: torch.Size([4, 1507])
reward: tensor([-0.4668,  0.5781,  0.0522, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 746])
attention_mask shape: torch.Size([4, 746])
reward: tensor([-1.5391, -0.3105, -0.3281, -0.2969], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1804])
attention_mask shape: torch.Size([4, 1804])
reward: tensor([-0.4043, -0.4453,  1.6719, -1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1694])
attention_mask shape: torch.Size([4, 1694])
reward: tensor([ 0.6328, -0.4805, -1.0391,  0.3770], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 863])
attention_mask shape: torch.Size([4, 863])
reward: tensor([-0.3281,  0.7461, -0.9609, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1899])
attention_mask shape: torch.Size([4, 1899])
reward: tensor([ 1.4141,  1.2812,  1.1016, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1308])
attention_mask shape: torch.Size([4, 1308])
reward: tensor([ 1.2266, -1.3750, -1.0469,  0.1035], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1437])
attention_mask shape: torch.Size([4, 1437])
reward: tensor([ 0.0510, -1.9297,  0.0811, -0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([-1.4453,  0.1465, -0.4082, -2.2188], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1790])
attention_mask shape: torch.Size([4, 1790])
reward: tensor([ 1.9297,  0.5273, -0.5352,  1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1777])
attention_mask shape: torch.Size([4, 1777])
reward: tensor([ 0.8906, -0.9766,  2.1094,  1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([ 0.7109, -1.2422, -0.5547,  0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 385])
attention_mask shape: torch.Size([4, 385])
reward: tensor([-0.1221, -0.9961, -1.4766, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 919])
attention_mask shape: torch.Size([4, 919])
reward: tensor([-0.6875, -0.2070, -0.7070, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1557])
attention_mask shape: torch.Size([4, 1557])
reward: tensor([-0.6562,  1.7500,  0.9961,  1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1120])
attention_mask shape: torch.Size([4, 1120])
reward: tensor([ 1.4297, -1.2188,  0.0022, -0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 748])
attention_mask shape: torch.Size([4, 748])
reward: tensor([-0.0757,  0.1621,  0.7773, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.4219, -1.2656, -0.2070,  0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1596])
attention_mask shape: torch.Size([4, 1596])
reward: tensor([-0.1621, -0.1641, -0.0623, -0.5508], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 703])
attention_mask shape: torch.Size([4, 703])
reward: tensor([-0.8633,  0.0698,  0.1533,  1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1424])
attention_mask shape: torch.Size([4, 1424])
reward: tensor([-0.6562, -0.8984,  0.3340, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1830])
attention_mask shape: torch.Size([4, 1830])
reward: tensor([-0.5859,  1.0156, -0.7969, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1338])
attention_mask shape: torch.Size([4, 1338])
reward: tensor([-0.4492, -0.1245,  0.3730, -0.7773], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1197])
attention_mask shape: torch.Size([4, 1197])
reward: tensor([ 0.1045,  0.4316, -0.6172,  0.5156], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1440])
attention_mask shape: torch.Size([4, 1440])
reward: tensor([-0.9141, -0.0623, -0.3652, -0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-1.6875, -0.4492,  0.5508, -1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1723])
attention_mask shape: torch.Size([4, 1723])
reward: tensor([-0.4805,  0.6680,  0.3496, -1.1797], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 785])
attention_mask shape: torch.Size([4, 785])
reward: tensor([-1.2656,  0.7969,  0.1748, -1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1123])
attention_mask shape: torch.Size([4, 1123])
reward: tensor([ 0.5547, -0.3906,  1.0859,  0.1235], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6562,  1.4219, -2.0156, -0.0713], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1108])
attention_mask shape: torch.Size([4, 1108])
reward: tensor([ 0.5273, -1.0391, -0.7383, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1590])
attention_mask shape: torch.Size([4, 1590])
reward: tensor([-0.2559, -0.1484,  1.1484,  0.2324], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1581])
attention_mask shape: torch.Size([4, 1581])
reward: tensor([-0.0801, -0.0820, -1.2188,  0.1279], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 871])
attention_mask shape: torch.Size([4, 871])
reward: tensor([ 0.6680,  0.9141, -1.5391, -1.5234], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1144])
attention_mask shape: torch.Size([4, 1144])
reward: tensor([ 1.0234,  0.0688,  0.0388, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 809])
attention_mask shape: torch.Size([4, 809])
reward: tensor([-0.1533,  0.8477, -0.7812,  0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1030])
attention_mask shape: torch.Size([4, 1030])
reward: tensor([-0.3105, -0.8203, -0.0732, -0.3828], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1561])
attention_mask shape: torch.Size([4, 1561])
reward: tensor([ 0.1621, -0.0557,  0.6562,  1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1177])
attention_mask shape: torch.Size([4, 1177])
reward: tensor([1.5938, 0.3164, 0.7188, 0.3320], device='cuda:0', dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.8828,  0.0845, -0.7031,  0.5234], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1662])
attention_mask shape: torch.Size([4, 1662])
reward: tensor([ 0.5273,  0.1270, -0.3379,  0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1364])
attention_mask shape: torch.Size([4, 1364])
reward: tensor([-1.3125, -0.8789, -0.3457, -1.8125], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1114])
attention_mask shape: torch.Size([4, 1114])
reward: tensor([-0.1602,  0.7031,  0.0122,  0.2656], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1481])
attention_mask shape: torch.Size([4, 1481])
reward: tensor([-0.8203,  0.2080,  0.4746,  1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1196])
attention_mask shape: torch.Size([4, 1196])
reward: tensor([-1.4062, -1.8984, -0.4141, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1484])
attention_mask shape: torch.Size([4, 1484])
reward: tensor([-0.1953,  0.3320, -1.8047, -1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1419])
attention_mask shape: torch.Size([4, 1419])
reward: tensor([ 1.9219, -0.9688,  0.8945, -0.4883], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1403])
attention_mask shape: torch.Size([4, 1403])
reward: tensor([ 0.2070,  0.0864, -0.8086, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1389])
attention_mask shape: torch.Size([4, 1389])
reward: tensor([-0.5508, -0.2109, -0.6211,  1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 764])
attention_mask shape: torch.Size([4, 764])
reward: tensor([-1.5000,  0.3086, -0.8984, -0.8320], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 967])
attention_mask shape: torch.Size([4, 967])
reward: tensor([-0.4180,  0.3711,  1.1172, -1.7969], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1199])
attention_mask shape: torch.Size([4, 1199])
reward: tensor([ 2.3125, -0.1396, -2.1562, -0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1512])
attention_mask shape: torch.Size([4, 1512])
reward: tensor([-0.0222,  0.0457, -0.0178, -0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 633])
attention_mask shape: torch.Size([4, 633])
reward: tensor([ 0.2695,  0.4004, -0.5273, -0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1402])
attention_mask shape: torch.Size([4, 1402])
reward: tensor([-1.0469, -0.2158,  0.5234,  0.1582], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1547])
attention_mask shape: torch.Size([4, 1547])
reward: tensor([-0.6758, -0.6367, -0.6523,  0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1269])
attention_mask shape: torch.Size([4, 1269])
reward: tensor([-0.6719,  0.1133,  1.2656,  0.2412], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.9336, -1.7188, -0.4453, -0.2871], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1434])
attention_mask shape: torch.Size([4, 1434])
reward: tensor([-0.8984, -1.2109,  0.8164,  0.2344], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1203])
attention_mask shape: torch.Size([4, 1203])
reward: tensor([ 0.7969,  0.0713,  1.3516, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 851])
attention_mask shape: torch.Size([4, 851])
reward: tensor([-0.6445,  1.1562, -0.9258,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1624])
attention_mask shape: torch.Size([4, 1624])
reward: tensor([ 1.1172,  0.5781, -1.1875, -1.2344], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 632])
attention_mask shape: torch.Size([4, 632])
reward: tensor([ 0.3809, -0.2227, -0.0669, -0.7773], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1935])
attention_mask shape: torch.Size([4, 1935])
reward: tensor([ 0.8945,  1.6406, -0.1826, -0.2246], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1279])
attention_mask shape: torch.Size([4, 1279])
reward: tensor([-1.4219, -0.2539,  0.5586, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1015])
attention_mask shape: torch.Size([4, 1015])
reward: tensor([ 0.0356,  1.3359, -1.2344,  0.1641], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1342])
attention_mask shape: torch.Size([4, 1342])
reward: tensor([ 0.2773,  0.0244,  1.2656, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-0.7930, -0.7773,  0.0957, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1474])
attention_mask shape: torch.Size([4, 1474])
reward: tensor([-0.5156, -1.7812,  0.3008,  0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1198])
attention_mask shape: torch.Size([4, 1198])
reward: tensor([ 0.1045, -1.3828,  0.5195, -0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1186])
attention_mask shape: torch.Size([4, 1186])
reward: tensor([-1.4453, -0.1846,  0.1943,  0.3574], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 714])
attention_mask shape: torch.Size([4, 714])
reward: tensor([ 0.1318, -1.1484, -0.3105, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1792])
attention_mask shape: torch.Size([4, 1792])
reward: tensor([ 0.0045, -0.5156,  0.8594, -1.7188], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1473])
attention_mask shape: torch.Size([4, 1473])
reward: tensor([ 0.6016,  0.7695, -0.3770,  1.2031], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([ 0.2734, -0.7656,  2.5000, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6406, -1.0938,  0.1602,  1.7266], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1224])
attention_mask shape: torch.Size([4, 1224])
reward: tensor([-0.8008,  0.2422, -0.5742,  0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1051])
attention_mask shape: torch.Size([4, 1051])
reward: tensor([ 0.3555,  0.4863,  0.1797, -0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 738])
attention_mask shape: torch.Size([4, 738])
reward: tensor([-0.8125, -0.9062, -0.7070, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6992, -0.6172,  0.4668, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1670])
attention_mask shape: torch.Size([4, 1670])
reward: tensor([-0.0688,  0.2812,  1.4141,  0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 357])
attention_mask shape: torch.Size([4, 357])
reward: tensor([ 1.1406, -0.6914, -0.5898, -0.6172], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1580])
attention_mask shape: torch.Size([4, 1580])
reward: tensor([-0.7852,  0.7070, -0.6211,  1.6641], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1476])
attention_mask shape: torch.Size([4, 1476])
reward: tensor([ 0.2637, -0.1533, -1.7344, -1.3594], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1095])
attention_mask shape: torch.Size([4, 1095])
reward: tensor([-0.1201,  0.7383, -0.0400,  1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1273])
attention_mask shape: torch.Size([4, 1273])
reward: tensor([-0.4219, -0.8203, -0.9688, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1536])
attention_mask shape: torch.Size([4, 1536])
reward: tensor([-1.3750,  0.3262,  1.2812, -0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1013])
attention_mask shape: torch.Size([4, 1013])
reward: tensor([-0.5898,  0.1768,  0.6680, -0.0913], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 534])
attention_mask shape: torch.Size([4, 534])
reward: tensor([ 0.1348, -0.7969, -0.4453, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1559])
attention_mask shape: torch.Size([4, 1559])
reward: tensor([-1.1094, -0.1797,  1.1406,  0.2539], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1611])
attention_mask shape: torch.Size([4, 1611])
reward: tensor([-1.2344, -0.3281,  0.3516, -0.7227], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1444])
attention_mask shape: torch.Size([4, 1444])
reward: tensor([-0.0244, -0.3340, -0.7930,  0.5508], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1388])
attention_mask shape: torch.Size([4, 1388])
reward: tensor([-0.5117,  0.7539,  0.9102, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1090])
attention_mask shape: torch.Size([4, 1090])
reward: tensor([-0.5352,  0.1348,  0.1484, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([ 0.9727, -2.0469, -0.7109,  0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 682])
attention_mask shape: torch.Size([4, 682])
reward: tensor([-1.2500,  0.1445, -1.7188,  0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1598])
attention_mask shape: torch.Size([4, 1598])
reward: tensor([-0.2373, -0.8711, -0.2930, -0.1113], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1486])
attention_mask shape: torch.Size([4, 1486])
reward: tensor([-0.0466, -0.4043, -0.2695, -0.1934], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1371])
attention_mask shape: torch.Size([4, 1371])
reward: tensor([-0.7656,  0.0078, -1.2266,  0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.5664, -0.8633, -0.4043, -0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1691])
attention_mask shape: torch.Size([4, 1691])
reward: tensor([-0.9336,  0.1943,  0.3203, -0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1128])
attention_mask shape: torch.Size([4, 1128])
reward: tensor([-0.1133, -1.4141,  0.1885, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1379])
attention_mask shape: torch.Size([4, 1379])
reward: tensor([ 0.0432,  0.2412, -1.2344, -1.7500], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 653])
attention_mask shape: torch.Size([4, 653])
reward: tensor([-1.9531, -1.0859, -0.2734,  0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 781])
attention_mask shape: torch.Size([4, 781])
reward: tensor([ 1.0859,  0.4688, -0.1270,  0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.4141, -2.1719, -1.6016,  1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([-1.6719,  0.2715,  0.1221, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1856])
attention_mask shape: torch.Size([4, 1856])
reward: tensor([-1.3281, -0.5234,  0.1348,  0.0156], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 911])
attention_mask shape: torch.Size([4, 911])
reward: tensor([-0.2598,  0.3672,  1.0234, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2012])
attention_mask shape: torch.Size([4, 2012])
reward: tensor([-0.1934, -0.4043, -0.9414, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 649])
attention_mask shape: torch.Size([4, 649])
reward: tensor([-0.6445, -0.4707,  0.6172,  0.3184], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 963])
attention_mask shape: torch.Size([4, 963])
reward: tensor([ 0.6406, -0.1689,  1.4609,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1525])
attention_mask shape: torch.Size([4, 1525])
reward: tensor([ 1.4609, -0.4570, -1.0703, -0.0178], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1067])
attention_mask shape: torch.Size([4, 1067])
reward: tensor([-0.1582, -0.6562, -0.1484,  0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 958])
attention_mask shape: torch.Size([4, 958])
reward: tensor([ 0.1914, -0.1689,  0.2490,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1481])
attention_mask shape: torch.Size([4, 1481])
reward: tensor([ 0.1689, -1.4766, -0.7031, -1.8906], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1178])
attention_mask shape: torch.Size([4, 1178])
reward: tensor([ 1.0547,  0.9375, -0.5469,  0.0255], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1008])
attention_mask shape: torch.Size([4, 1008])
reward: tensor([ 1.3672, -0.1514, -1.3281, -0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([ 1.7969,  0.5859, -1.0391, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1468])
attention_mask shape: torch.Size([4, 1468])
reward: tensor([-1.1094, -0.5898, -0.8047,  0.0167], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1361])
attention_mask shape: torch.Size([4, 1361])
reward: tensor([ 0.7539, -1.7031,  0.2734,  1.5859], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1025])
attention_mask shape: torch.Size([4, 1025])
reward: tensor([-0.6172, -0.8320,  0.2246,  0.2334], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1444])
attention_mask shape: torch.Size([4, 1444])
reward: tensor([-0.7969,  1.9453, -0.5859,  1.3516], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1098])
attention_mask shape: torch.Size([4, 1098])
reward: tensor([-1.2891, -0.5039, -1.8125,  0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1345])
attention_mask shape: torch.Size([4, 1345])
reward: tensor([ 0.0255, -0.5898, -2.0156,  0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 707])
attention_mask shape: torch.Size([4, 707])
reward: tensor([-0.0378, -0.3730, -0.1777,  0.4160], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1088])
attention_mask shape: torch.Size([4, 1088])
reward: tensor([-0.8906, -0.4844, -1.7969,  1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1667])
attention_mask shape: torch.Size([4, 1667])
reward: tensor([-0.1641, -0.5859, -0.0820,  1.4453], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1559])
attention_mask shape: torch.Size([4, 1559])
reward: tensor([ 0.5547, -1.1406,  0.1436, -0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1293])
attention_mask shape: torch.Size([4, 1293])
reward: tensor([ 0.0942,  1.1016, -0.0133, -0.2090], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1194])
attention_mask shape: torch.Size([4, 1194])
reward: tensor([-1.0469,  0.8398, -1.9844, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1110])
attention_mask shape: torch.Size([4, 1110])
reward: tensor([-0.3027, -1.0391,  0.8203,  0.8672], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 814])
attention_mask shape: torch.Size([4, 814])
reward: tensor([ 0.2676,  1.0000,  0.0811, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1536])
attention_mask shape: torch.Size([4, 1536])
reward: tensor([-0.5625, -0.4180, -1.1016, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1340])
attention_mask shape: torch.Size([4, 1340])
reward: tensor([-0.0623, -1.5625, -0.7930,  0.1455], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.6680, -1.6562,  0.5898, -1.4297], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1387])
attention_mask shape: torch.Size([4, 1387])
reward: tensor([ 1.2891, -0.3105, -0.4746, -0.0601], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 886])
attention_mask shape: torch.Size([4, 886])
reward: tensor([ 1.3047, -0.4453, -0.8008, -1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1163])
attention_mask shape: torch.Size([4, 1163])
reward: tensor([ 2.0000, -0.7227,  0.3652, -0.8398], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1568])
attention_mask shape: torch.Size([4, 1568])
reward: tensor([ 1.5469,  0.9727, -0.1533, -0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1308])
attention_mask shape: torch.Size([4, 1308])
reward: tensor([-0.8477, -0.0669, -0.9414,  0.1426], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1550])
attention_mask shape: torch.Size([4, 1550])
reward: tensor([ 1.3438, -0.8359,  1.6953, -0.7773], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 730])
attention_mask shape: torch.Size([4, 730])
reward: tensor([-0.4395, -0.4531, -0.4980,  1.8047], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1680])
attention_mask shape: torch.Size([4, 1680])
reward: tensor([ 0.3359, -0.2969, -1.5938,  0.3535], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1181])
attention_mask shape: torch.Size([4, 1181])
reward: tensor([-0.9688,  0.6680,  0.6406,  0.8047], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1884])
attention_mask shape: torch.Size([4, 1884])
reward: tensor([-0.3906, -1.0469, -0.5938, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.5547,  0.6445, -0.0645, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1474])
attention_mask shape: torch.Size([4, 1474])
reward: tensor([-0.6133, -2.1406,  0.3535, -0.1465], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1330])
attention_mask shape: torch.Size([4, 1330])
reward: tensor([-0.3066,  0.2422,  0.8633,  0.2051], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1258])
attention_mask shape: torch.Size([4, 1258])
reward: tensor([-0.8398, -1.1250, -0.3770, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 731])
attention_mask shape: torch.Size([4, 731])
reward: tensor([-0.2041, -1.8281, -0.7383, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 910])
attention_mask shape: torch.Size([4, 910])
reward: tensor([-0.5547, -1.1406, -1.0312, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1489])
attention_mask shape: torch.Size([4, 1489])
reward: tensor([-0.5625, -0.3867, -1.8750,  1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([ 1.3672, -0.7617, -0.1641, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.1562, -0.4570,  0.2949, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3730, -1.1562,  0.5898,  0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1333])
attention_mask shape: torch.Size([4, 1333])
reward: tensor([-0.3555, -0.2021,  0.4883, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.0601,  0.9219, -0.3379,  1.6250], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1855])
attention_mask shape: torch.Size([4, 1855])
reward: tensor([-0.0334,  0.8359,  0.0400, -0.9766], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1209])
attention_mask shape: torch.Size([4, 1209])
reward: tensor([ 0.2471, -0.2021,  0.3320, -0.5234], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 667])
attention_mask shape: torch.Size([4, 667])
reward: tensor([-0.4355, -0.8086,  1.0938, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1653])
attention_mask shape: torch.Size([4, 1653])
reward: tensor([-1.8984, -0.2373,  1.7891, -0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1226])
attention_mask shape: torch.Size([4, 1226])
reward: tensor([-0.5469, -1.8125, -0.0422, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-2.1562, -0.5430, -1.3125, -0.6094], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1413])
attention_mask shape: torch.Size([4, 1413])
reward: tensor([-1.3984,  0.1064,  0.6680,  0.1465], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 742])
attention_mask shape: torch.Size([4, 742])
reward: tensor([-0.5039, -0.2773, -0.8281,  0.7969], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1184])
attention_mask shape: torch.Size([4, 1184])
reward: tensor([-0.3965, -0.9609,  1.3359, -0.4453], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 592])
attention_mask shape: torch.Size([4, 592])
reward: tensor([-0.7344, -0.0776, -1.6719, -0.0669], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1852])
attention_mask shape: torch.Size([4, 1852])
reward: tensor([-0.1953, -0.9062,  0.3184, -1.8984], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1368])
attention_mask shape: torch.Size([4, 1368])
reward: tensor([-0.9688,  0.3809, -0.4980, -0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1129])
attention_mask shape: torch.Size([4, 1129])
reward: tensor([-0.6758, -0.6445, -1.0469, -0.1001], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1187])
attention_mask shape: torch.Size([4, 1187])
reward: tensor([-0.7852, -0.2578,  1.5547,  0.6367], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 740])
attention_mask shape: torch.Size([4, 740])
reward: tensor([-0.1338, -0.9492, -0.6016,  0.2256], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1115])
attention_mask shape: torch.Size([4, 1115])
reward: tensor([-0.3242, -2.2031, -0.8164, -1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1575])
attention_mask shape: torch.Size([4, 1575])
reward: tensor([ 0.4492,  0.4746, -1.1719,  0.2422], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 526])
attention_mask shape: torch.Size([4, 526])
reward: tensor([-0.8906, -0.1797, -0.9141, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1249])
attention_mask shape: torch.Size([4, 1249])
reward: tensor([-0.5547, -0.7969, -0.0022, -0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1378])
attention_mask shape: torch.Size([4, 1378])
reward: tensor([-0.2910, -0.4453,  0.2441,  0.2363], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.0859, -0.6250, -1.8125, -1.7344], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1550])
attention_mask shape: torch.Size([4, 1550])
reward: tensor([-0.6484, -1.3359,  1.2812,  0.9102], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 678])
attention_mask shape: torch.Size([4, 678])
reward: tensor([-0.7539, -0.7695, -1.8594, -1.2422], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.7969, -0.2754, -1.2422, -1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1572])
attention_mask shape: torch.Size([4, 1572])
reward: tensor([-0.9414,  1.1641,  0.2344, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1137])
attention_mask shape: torch.Size([4, 1137])
reward: tensor([-1.5078, -0.6367, -0.2617, -1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.7656,  0.7930,  0.7734,  1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1627])
attention_mask shape: torch.Size([4, 1627])
reward: tensor([ 0.7188,  0.7656,  1.2656, -0.1445], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([-0.3164, -1.5078,  2.1094, -1.9141], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1134])
attention_mask shape: torch.Size([4, 1134])
reward: tensor([-1.7188,  1.4219, -0.2793, -0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1358])
attention_mask shape: torch.Size([4, 1358])
reward: tensor([ 0.9336,  0.2373, -0.7109,  0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 908])
attention_mask shape: torch.Size([4, 908])
reward: tensor([ 0.0532, -0.7422,  0.3730, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1175])
attention_mask shape: torch.Size([4, 1175])
reward: tensor([ 1.2734, -1.3828, -1.4844,  1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([-0.6562, -0.3867, -2.1094, -0.1396], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1780])
attention_mask shape: torch.Size([4, 1780])
reward: tensor([-0.2227, -0.1064, -0.5625,  1.8281], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1235])
attention_mask shape: torch.Size([4, 1235])
reward: tensor([ 0.0211, -1.0312, -0.9336, -0.1777], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 734])
attention_mask shape: torch.Size([4, 734])
reward: tensor([-0.0688, -0.2930, -1.1172, -1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1320])
attention_mask shape: torch.Size([4, 1320])
reward: tensor([ 1.0469, -0.7148, -0.8320, -0.0400], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1477])
attention_mask shape: torch.Size([4, 1477])
reward: tensor([ 0.4746, -0.3164, -0.8438,  0.3906], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 784])
attention_mask shape: torch.Size([4, 784])
reward: tensor([-0.0713, -0.8633,  0.0669,  0.8945], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1204])
attention_mask shape: torch.Size([4, 1204])
reward: tensor([-1.0156,  0.6133, -0.5234,  0.5156], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 868])
attention_mask shape: torch.Size([4, 868])
reward: tensor([ 0.1709, -2.1719, -0.5820, -0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1127])
attention_mask shape: torch.Size([4, 1127])
reward: tensor([-0.8984, -1.0391,  0.7188, -0.3652], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([-0.3906, -1.3672, -0.1885,  0.0222], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.0781, -0.8906,  1.8828, -0.1689], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 2.2031, -0.1221, -1.5938,  1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1018])
attention_mask shape: torch.Size([4, 1018])
reward: tensor([-0.1777, -0.4180,  1.4141,  1.0625], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.1250e+00, -3.9648e-01, -1.1139e-03, -3.0664e-01], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 639])
attention_mask shape: torch.Size([4, 639])
reward: tensor([-1.0859, -1.2188, -0.9414,  1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6758,  0.0933, -0.4805, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1336])
attention_mask shape: torch.Size([4, 1336])
reward: tensor([ 0.8008,  0.4102, -0.1729,  0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1171])
attention_mask shape: torch.Size([4, 1171])
reward: tensor([ 0.8516, -1.7812,  0.4883,  0.4668], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 996])
attention_mask shape: torch.Size([4, 996])
reward: tensor([ 0.5273,  1.7891,  0.6758, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.9297, -1.0391, -0.1426, -1.9844], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1303])
attention_mask shape: torch.Size([4, 1303])
reward: tensor([ 0.0723, -0.7734,  1.1406, -0.2676], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1486])
attention_mask shape: torch.Size([4, 1486])
reward: tensor([ 0.8750,  0.0457, -1.1094,  0.9375], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1676])
attention_mask shape: torch.Size([4, 1676])
reward: tensor([ 0.0601,  0.4238,  1.9453, -1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1023])
attention_mask shape: torch.Size([4, 1023])
reward: tensor([-0.2246,  0.0300, -0.9883,  0.2002], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1260])
attention_mask shape: torch.Size([4, 1260])
reward: tensor([ 1.3281,  0.4648,  1.6328, -0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1615])
attention_mask shape: torch.Size([4, 1615])
reward: tensor([-0.5742, -0.3828, -0.4805, -1.4453], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1453])
attention_mask shape: torch.Size([4, 1453])
reward: tensor([ 0.6367, -0.6914, -0.8711,  0.0835], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1566])
attention_mask shape: torch.Size([4, 1566])
reward: tensor([ 0.0422, -0.2891, -0.6719, -0.1885], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1109])
attention_mask shape: torch.Size([4, 1109])
reward: tensor([ 0.6484,  0.4980, -0.1797,  0.9648], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1487])
attention_mask shape: torch.Size([4, 1487])
reward: tensor([ 0.3027,  1.2266, -0.3281, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 642])
attention_mask shape: torch.Size([4, 642])
reward: tensor([-0.6328, -0.3164,  0.5430, -1.3594], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 843])
attention_mask shape: torch.Size([4, 843])
reward: tensor([ 0.5312,  0.2773,  1.7500, -1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3027,  1.1250, -0.5156,  0.5195], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-1.7500, -1.4219, -0.9336, -0.2314], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-28 12:13:14,578] [INFO] [launch.py:351:main] Process 879746 exits successfully.
[2024-10-28 12:13:15,579] [INFO] [launch.py:351:main] Process 879744 exits successfully.
[2024-10-28 12:15:27,713] [INFO] [launch.py:351:main] Process 879745 exits successfully.
[2024-10-28 12:16:17,763] [INFO] [launch.py:351:main] Process 879747 exits successfully.
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# [K(base) root@autodl-container-ec234bbd2e-925c6d34:~# bash run_eval_reward_openrlhf.sh
[?2004l+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-2th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-2th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_2 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-30 17:38:16,441] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-30 17:38:18,378] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-30 17:38:18,378] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-2th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_2 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-30 17:38:19,799] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-30 17:38:22,092] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-30 17:38:22,092] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-30 17:38:22,092] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-30 17:38:22,092] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-30 17:38:22,092] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-30 17:38:22,093] [INFO] [launch.py:256:main] process 901927 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-2th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_2', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-30 17:38:22,093] [INFO] [launch.py:256:main] process 901928 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-2th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_2', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-30 17:38:22,093] [INFO] [launch.py:256:main] process 901929 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-2th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_2', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-30 17:38:22,094] [INFO] [launch.py:256:main] process 901930 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-2th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_2', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-30 17:38:23,743] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-30 17:38:23,775] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-30 17:38:23,791] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-30 17:38:23,795] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-30 17:38:26,260] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-30 17:38:26,261] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-30 17:38:26,677] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-30 17:38:26,696] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-10-30 17:38:27,810] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.31it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.24it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.38it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.71it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.22it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.39it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.97it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.42it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.35it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  6.15it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.39it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.38it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.23it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.53it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.48it/s]
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.26it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.27it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.29it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.51it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.42it/s]
[2024-10-30 17:38:49,582] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-30 17:38:49,876] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-30 17:38:49,925] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-30 17:38:50,957] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-30 17:38:50,957] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-30 17:38:50,957] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-30 17:38:52,250] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-30 17:38:52,251] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-30 17:38:52,251] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-30 17:38:52,252] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-30 17:38:52,253] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-30 17:38:52,440] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-30 17:38:52,440] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-30 17:38:52,441] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 39.61 GB, percent = 3.9%
[2024-10-30 17:38:52,630] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-30 17:38:52,631] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-30 17:38:52,631] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 39.62 GB, percent = 3.9%
[2024-10-30 17:38:52,632] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-30 17:38:52,632] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-30 17:38:52,632] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-30 17:38:52,632] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-30 17:38:52,632] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7a30595cf0>
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-30 17:38:52,633] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-30 17:38:52,634] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-30 17:38:52,634] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-30 17:38:52,635] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-30 17:38:52,635] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-30 17:38:52,635] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-30 17:38:57,049] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-30 17:38:57,051] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-30 17:38:57,197] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-30 17:38:57,197] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-30 17:38:57,198] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 32.98 GB, percent = 3.3%
[2024-10-30 17:38:57,322] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-30 17:38:57,322] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-30 17:38:57,323] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 32.93 GB, percent = 3.3%
[2024-10-30 17:38:57,324] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-30 17:38:57,324] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-30 17:38:57,324] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-30 17:38:57,324] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-30 17:38:57,324] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-30 17:38:57,324] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-30 17:38:57,324] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-30 17:38:57,324] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7a301b27d0>
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-30 17:38:57,325] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-30 17:38:57,326] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-30 17:38:57,326] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 632/100000 [00:00<00:15, 6318.50it/s]Preprocessing data:   2%|█▍                                                                                          | 1628/100000 [00:00<00:11, 8459.67it/s]Preprocessing data:   3%|██▍                                                                                         | 2593/100000 [00:00<00:10, 8999.85it/s]Preprocessing data:   4%|███▎                                                                                        | 3605/100000 [00:00<00:10, 9441.68it/s]Preprocessing data:   5%|████▎                                                                                       | 4638/100000 [00:00<00:09, 9760.75it/s]Preprocessing data:   6%|█████▏                                                                                      | 5661/100000 [00:00<00:09, 9918.79it/s]Preprocessing data:   7%|██████                                                                                     | 6686/100000 [00:00<00:09, 10024.83it/s]Preprocessing data:   8%|███████                                                                                    | 7722/100000 [00:00<00:09, 10128.97it/s]Preprocessing data:   9%|███████▉                                                                                   | 8750/100000 [00:00<00:08, 10173.28it/s]Preprocessing data:  10%|████████▉                                                                                  | 9770/100000 [00:01<00:08, 10179.16it/s]Preprocessing data:  11%|█████████▊                                                                                | 10837/100000 [00:01<00:08, 10326.48it/s]Preprocessing data:  12%|██████████▋                                                                               | 11900/100000 [00:01<00:08, 10418.10it/s]Preprocessing data:  13%|███████████▋                                                                              | 12949/100000 [00:01<00:08, 10437.93it/s]Preprocessing data:  14%|████████████▌                                                                             | 13994/100000 [00:01<00:08, 10440.78it/s]Preprocessing data:  15%|█████████████▌                                                                            | 15051/100000 [00:01<00:08, 10477.22it/s]Preprocessing data:  16%|██████████████▌                                                                           | 16127/100000 [00:01<00:07, 10559.60it/s]Preprocessing data:  17%|███████████████▍                                                                          | 17206/100000 [00:01<00:07, 10627.87it/s]Preprocessing data:  18%|████████████████▍                                                                         | 18275/100000 [00:01<00:07, 10644.80it/s]Preprocessing data:  19%|█████████████████▍                                                                        | 19344/100000 [00:01<00:07, 10657.28it/s]Preprocessing data:  20%|██████████████████▍                                                                       | 20428/100000 [00:02<00:07, 10710.54it/s]Preprocessing data:  22%|███████████████████▎                                                                      | 21500/100000 [00:02<00:07, 10696.60it/s]Preprocessing data:  23%|████████████████████▎                                                                     | 22570/100000 [00:02<00:07, 10612.85it/s]Preprocessing data:  24%|█████████████████████▎                                                                    | 23632/100000 [00:02<00:07, 10605.18it/s]Preprocessing data:  25%|██████████████████████▏                                                                   | 24693/100000 [00:02<00:07, 10580.79it/s]Preprocessing data:  26%|███████████████████████▏                                                                  | 25752/100000 [00:02<00:07, 10556.83it/s]Preprocessing data:  27%|████████████████████████▏                                                                 | 26808/100000 [00:02<00:06, 10538.43it/s]Preprocessing data:  28%|█████████████████████████                                                                 | 27862/100000 [00:02<00:06, 10483.48it/s]Preprocessing data:  29%|██████████████████████████                                                                | 28911/100000 [00:02<00:06, 10439.26it/s]Preprocessing data:  30%|██████████████████████████▉                                                               | 29955/100000 [00:02<00:06, 10347.20it/s]Preprocessing data:  31%|███████████████████████████▉                                                              | 30990/100000 [00:03<00:06, 10330.59it/s]Preprocessing data:  32%|████████████████████████████▊                                                             | 32024/100000 [00:03<00:06, 10284.61it/s]Preprocessing data:  33%|█████████████████████████████▋                                                            | 33053/100000 [00:03<00:06, 10264.51it/s]Preprocessing data:  34%|██████████████████████████████▋                                                           | 34080/100000 [00:03<00:06, 10209.12it/s]Preprocessing data:  35%|███████████████████████████████▌                                                          | 35101/100000 [00:03<00:06, 10168.80it/s]Preprocessing data:  36%|████████████████████████████████▌                                                         | 36118/100000 [00:03<00:06, 10145.46it/s]Preprocessing data:  37%|█████████████████████████████████▍                                                        | 37140/100000 [00:03<00:06, 10167.46it/s]Preprocessing data:  38%|██████████████████████████████████▎                                                       | 38157/100000 [00:03<00:06, 10090.03it/s]Preprocessing data:  39%|███████████████████████████████████▎                                                      | 39174/100000 [00:03<00:06, 10110.84it/s]Preprocessing data:  40%|████████████████████████████████████▏                                                     | 40199/100000 [00:03<00:05, 10150.59it/s]Preprocessing data:  41%|█████████████████████████████████████                                                     | 41220/100000 [00:04<00:05, 10168.02it/s]Preprocessing data:  42%|██████████████████████████████████████                                                    | 42237/100000 [00:04<00:05, 10078.77it/s]Preprocessing data:  43%|██████████████████████████████████████▉                                                   | 43264/100000 [00:04<00:05, 10134.69it/s]Preprocessing data:  44%|███████████████████████████████████████▉                                                  | 44316/100000 [00:04<00:05, 10248.94it/s]Preprocessing data:  45%|████████████████████████████████████████▊                                                 | 45383/100000 [00:04<00:05, 10374.22it/s]Preprocessing data:  46%|█████████████████████████████████████████▊                                                | 46449/100000 [00:04<00:05, 10456.56it/s]Preprocessing data:  48%|██████████████████████████████████████████▊                                               | 47505/100000 [00:04<00:05, 10487.29it/s]Preprocessing data:  49%|███████████████████████████████████████████▋                                              | 48554/100000 [00:04<00:04, 10477.48it/s]Preprocessing data:  50%|████████████████████████████████████████████▋                                             | 49602/100000 [00:04<00:04, 10431.18it/s]Preprocessing data:  51%|█████████████████████████████████████████████▌                                            | 50665/100000 [00:04<00:04, 10489.33it/s]Preprocessing data:  52%|██████████████████████████████████████████████▌                                           | 51715/100000 [00:05<00:04, 10396.13it/s]Preprocessing data:  53%|███████████████████████████████████████████████▍                                          | 52775/100000 [00:05<00:04, 10455.07it/s]Preprocessing data:  54%|████████████████████████████████████████████████▍                                         | 53821/100000 [00:05<00:04, 10423.64it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▍                                        | 54872/100000 [00:05<00:04, 10447.02it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▎                                       | 55917/100000 [00:05<00:04, 10400.81it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▎                                      | 56976/100000 [00:05<00:04, 10457.04it/s]Preprocessing data:  58%|████████████████████████████████████████████████████▏                                     | 58040/100000 [00:05<00:03, 10510.73it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████▏                                    | 59098/100000 [00:05<00:03, 10530.46it/s]Preprocessing data:  60%|██████████████████████████████████████████████████████▏                                   | 60156/100000 [00:05<00:03, 10543.70it/s]Preprocessing data:  61%|███████████████████████████████████████████████████████                                   | 61219/100000 [00:05<00:03, 10568.65it/s]Preprocessing data:  62%|████████████████████████████████████████████████████████                                  | 62287/100000 [00:06<00:03, 10600.29it/s]Preprocessing data:  63%|█████████████████████████████████████████████████████████                                 | 63355/100000 [00:06<00:03, 10623.34it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▉                                | 64418/100000 [00:06<00:03, 10608.48it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▉                               | 65479/100000 [00:06<00:03, 10533.70it/s]Preprocessing data:  67%|███████████████████████████████████████████████████████████▉                              | 66534/100000 [00:06<00:03, 10536.08it/s]Preprocessing data:  68%|████████████████████████████████████████████████████████████▊                             | 67588/100000 [00:06<00:03, 10529.83it/s]Preprocessing data:  69%|█████████████████████████████████████████████████████████████▊                            | 68644/100000 [00:06<00:02, 10536.78it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▋                           | 69698/100000 [00:06<00:02, 10530.12it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▋                          | 70754/100000 [00:06<00:02, 10537.87it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▋                         | 71808/100000 [00:06<00:02, 10537.32it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▌                        | 72862/100000 [00:07<00:02, 10183.59it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▏                       | 73883/100000 [00:07<00:02, 9336.00it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████                       | 74831/100000 [00:07<00:02, 8873.88it/s]Preprocessing data:  76%|████████████████████████████████████████████████████████████████████▉                      | 75731/100000 [00:07<00:02, 8583.89it/s]Preprocessing data:  77%|█████████████████████████████████████████████████████████████████████▋                     | 76598/100000 [00:07<00:02, 8394.94it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████▍                    | 77443/100000 [00:07<00:02, 8234.26it/s]Preprocessing data:  78%|███████████████████████████████████████████████████████████████████████▏                   | 78270/100000 [00:07<00:02, 8085.08it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▉                   | 79081/100000 [00:07<00:02, 7965.79it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▋                  | 79879/100000 [00:07<00:02, 7806.46it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▍                 | 80661/100000 [00:08<00:02, 7661.43it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▎                | 81624/100000 [00:08<00:02, 8219.18it/s]Preprocessing data:  82%|███████████████████████████████████████████████████████████████████████████                | 82458/100000 [00:08<00:02, 8253.27it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▊               | 83286/100000 [00:08<00:02, 8192.41it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▌              | 84107/100000 [00:08<00:01, 8052.05it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▍             | 85147/100000 [00:08<00:01, 8733.35it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▎            | 86088/100000 [00:08<00:01, 8931.47it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▏           | 86984/100000 [00:08<00:01, 8387.62it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████           | 87961/100000 [00:08<00:01, 8777.69it/s]Preprocessing data:  89%|████████████████████████████████████████████████████████████████████████████████▉          | 88947/100000 [00:08<00:01, 9088.20it/s]Preprocessing data:  90%|█████████████████████████████████████████████████████████████████████████████████▊         | 89919/100000 [00:09<00:01, 9272.24it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▋        | 90852/100000 [00:09<00:01, 8658.95it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▍       | 91730/100000 [00:09<00:01, 8165.98it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▏      | 92559/100000 [00:09<00:00, 7813.34it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▉      | 93350/100000 [00:09<00:00, 7650.86it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▋     | 94136/100000 [00:09<00:00, 7706.52it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▌    | 95130/100000 [00:09<00:00, 8333.90it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▍   | 96094/100000 [00:09<00:00, 8705.85it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▏  | 96972/100000 [00:09<00:00, 8607.99it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████  | 97865/100000 [00:10<00:00, 8700.31it/s]Preprocessing data:  99%|█████████████████████████████████████████████████████████████████████████████████████████▉ | 98830/100000 [00:10<00:00, 8978.33it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████▊| 99829/100000 [00:10<00:00, 9275.05it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9719.98it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1176])
attention_mask shape: torch.Size([4, 1176])
reward: tensor([-0.1289, -1.8828, -0.5039, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1757])
attention_mask shape: torch.Size([4, 1757])
reward: tensor([ 1.9453, -1.2188,  0.4395, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1082])
attention_mask shape: torch.Size([4, 1082])
reward: tensor([-0.7695, -1.3047, -1.1406, -0.4395], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 424])
attention_mask shape: torch.Size([4, 424])
reward: tensor([-0.1777, -1.4922, -0.6250, -0.2070], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1393])
attention_mask shape: torch.Size([4, 1393])
reward: tensor([ 0.6172,  0.1099,  0.3613, -0.4629], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1101])
attention_mask shape: torch.Size([4, 1101])
reward: tensor([ 1.9922, -1.0859, -1.8281, -1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1657])
attention_mask shape: torch.Size([4, 1657])
reward: tensor([-1.7578, -1.1797,  0.1670, -1.6953], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 814])
attention_mask shape: torch.Size([4, 814])
reward: tensor([-0.5273, -0.3516,  0.6758, -0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 846])
attention_mask shape: torch.Size([4, 846])
reward: tensor([-1.1094, -0.7734,  0.0522, -0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1053])
attention_mask shape: torch.Size([4, 1053])
reward: tensor([ 0.3027,  1.1562, -0.1357, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 723])
attention_mask shape: torch.Size([4, 723])
reward: tensor([-0.5352, -0.4004, -0.0532, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 873])
attention_mask shape: torch.Size([4, 873])
reward: tensor([-1.3359, -0.1846, -1.9453,  0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 537])
attention_mask shape: torch.Size([4, 537])
reward: tensor([-1.5703, -0.9336, -0.8125, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1318])
attention_mask shape: torch.Size([4, 1318])
reward: tensor([-0.6328,  0.0200, -0.8398,  0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 665])
attention_mask shape: torch.Size([4, 665])
reward: tensor([ 0.3828, -0.4941,  1.6406, -0.4629], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1394])
attention_mask shape: torch.Size([4, 1394])
reward: tensor([ 0.5234, -0.6406, -0.2334, -1.4766], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-0.8398, -2.0469,  0.0566,  0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1142])
attention_mask shape: torch.Size([4, 1142])
reward: tensor([ 0.1533,  0.1885, -0.1602, -0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 642])
attention_mask shape: torch.Size([4, 642])
reward: tensor([-0.4395, -0.5742, -0.7656, -0.0422], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 759])
attention_mask shape: torch.Size([4, 759])
reward: tensor([ 0.8086,  0.8281, -1.1016, -0.2598], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1022])
attention_mask shape: torch.Size([4, 1022])
reward: tensor([-1.1172, -1.9141, -0.0801, -1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1108])
attention_mask shape: torch.Size([4, 1108])
reward: tensor([-0.1426,  0.6758, -0.3027,  1.0625], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 708])
attention_mask shape: torch.Size([4, 708])
reward: tensor([-1.5000,  1.1875, -0.7461, -1.7188], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 970])
attention_mask shape: torch.Size([4, 970])
reward: tensor([-2.1719, -0.3555,  0.0000,  1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1255])
attention_mask shape: torch.Size([4, 1255])
reward: tensor([ 0.4570, -0.6719, -0.0977, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1271])
attention_mask shape: torch.Size([4, 1271])
reward: tensor([ 0.2910, -0.9688, -0.1670, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1528])
attention_mask shape: torch.Size([4, 1528])
reward: tensor([-0.4258,  0.3418,  0.1045, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 287])
attention_mask shape: torch.Size([4, 287])
reward: tensor([ 1.1139e-03, -9.1406e-01, -1.5469e+00, -3.3789e-01], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 747])
attention_mask shape: torch.Size([4, 747])
reward: tensor([ 0.6211, -1.4844, -0.5430, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1303])
attention_mask shape: torch.Size([4, 1303])
reward: tensor([ 0.0344, -0.0334,  0.0835, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1718])
attention_mask shape: torch.Size([4, 1718])
reward: tensor([-0.9414,  0.4707, -1.0469, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 913])
attention_mask shape: torch.Size([4, 913])
reward: tensor([-1.1797,  0.2559, -1.4375, -0.3418], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1430])
attention_mask shape: torch.Size([4, 1430])
reward: tensor([-0.6445, -0.0713,  1.2656, -1.4844], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1320])
attention_mask shape: torch.Size([4, 1320])
reward: tensor([ 0.3438, -0.5508, -1.3281, -0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 759])
attention_mask shape: torch.Size([4, 759])
reward: tensor([-0.4941, -1.2422, -0.8203, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1591])
attention_mask shape: torch.Size([4, 1591])
reward: tensor([ 0.3379,  0.8945,  0.6797, -1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1135])
attention_mask shape: torch.Size([4, 1135])
reward: tensor([ 0.8359, -1.1641, -0.8203, -0.0289], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1071])
attention_mask shape: torch.Size([4, 1071])
reward: tensor([ 0.0378, -1.5938, -0.3242, -0.8359], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 484])
attention_mask shape: torch.Size([4, 484])
reward: tensor([-0.2773,  0.7734,  0.1621, -2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1474])
attention_mask shape: torch.Size([4, 1474])
reward: tensor([ 0.2617, -0.4844, -1.8203,  1.1719], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1189])
attention_mask shape: torch.Size([4, 1189])
reward: tensor([ 0.3398, -0.6406,  1.6797,  1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1263])
attention_mask shape: torch.Size([4, 1263])
reward: tensor([ 0.9727, -0.0933, -1.3281,  0.2158], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 256])
attention_mask shape: torch.Size([4, 256])
reward: tensor([-1.2500, -1.2109, -0.6523, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 583])
attention_mask shape: torch.Size([4, 583])
reward: tensor([-0.6914, -0.7930, -0.6562, -0.4883], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1559])
attention_mask shape: torch.Size([4, 1559])
reward: tensor([-0.5430,  1.8438, -0.4082,  1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 828])
attention_mask shape: torch.Size([4, 828])
reward: tensor([-0.4316, -1.0391, -0.9961, -0.6328], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 615])
attention_mask shape: torch.Size([4, 615])
reward: tensor([-0.2334, -1.0859,  1.0625, -0.6172], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1585])
attention_mask shape: torch.Size([4, 1585])
reward: tensor([-0.1953, -0.1826, -0.2334,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1133])
attention_mask shape: torch.Size([4, 1133])
reward: tensor([-1.2812, -0.6406,  0.5508, -1.0234], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 694])
attention_mask shape: torch.Size([4, 694])
reward: tensor([-1.6172, -0.2539,  0.7812, -0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1641])
attention_mask shape: torch.Size([4, 1641])
reward: tensor([-0.6328,  0.4043, -0.5156, -0.5781], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1124])
attention_mask shape: torch.Size([4, 1124])
reward: tensor([-0.5781,  0.9688, -0.7695, -0.1641], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1208])
attention_mask shape: torch.Size([4, 1208])
reward: tensor([-0.7461, -0.3340,  0.6211, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1045])
attention_mask shape: torch.Size([4, 1045])
reward: tensor([-1.8516, -1.0156, -0.7344, -0.0713], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1429])
attention_mask shape: torch.Size([4, 1429])
reward: tensor([-0.0378, -0.9062,  1.0547, -0.0067], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 544])
attention_mask shape: torch.Size([4, 544])
reward: tensor([-0.6406, -0.7148,  0.6328, -1.7969], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 859])
attention_mask shape: torch.Size([4, 859])
reward: tensor([-0.9961, -0.8906, -0.9766, -0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 475])
attention_mask shape: torch.Size([4, 475])
reward: tensor([-1.3281,  0.4395, -1.3672, -1.4141], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([ 0.3672, -0.2715, -1.0312, -1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1463])
attention_mask shape: torch.Size([4, 1463])
reward: tensor([-1.9141,  1.5000, -0.8438, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 968])
attention_mask shape: torch.Size([4, 968])
reward: tensor([-1.3125, -1.0156, -1.7266, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1244])
attention_mask shape: torch.Size([4, 1244])
reward: tensor([-1.4219, -0.0688,  1.3438, -0.7070], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1437])
attention_mask shape: torch.Size([4, 1437])
reward: tensor([-0.5859, -0.4219, -1.1719,  0.1992], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 555])
attention_mask shape: torch.Size([4, 555])
reward: tensor([-1.1250, -0.1001, -0.3828, -0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1233])
attention_mask shape: torch.Size([4, 1233])
reward: tensor([ 1.0312, -0.7695,  1.2188, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 752])
attention_mask shape: torch.Size([4, 752])
reward: tensor([ 0.1455,  1.0781, -1.2734,  0.4902], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1010])
attention_mask shape: torch.Size([4, 1010])
reward: tensor([-0.5703, -0.1201, -0.2354, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1093])
attention_mask shape: torch.Size([4, 1093])
reward: tensor([-1.1562, -1.3828, -0.7539,  1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 959])
attention_mask shape: torch.Size([4, 959])
reward: tensor([ 1.1719, -0.0889,  0.0033, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1136])
attention_mask shape: torch.Size([4, 1136])
reward: tensor([-0.2578, -1.1484, -1.2500,  0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1498])
attention_mask shape: torch.Size([4, 1498])
reward: tensor([-1.0938, -0.4629, -0.4043,  0.3672], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 622])
attention_mask shape: torch.Size([4, 622])
reward: tensor([-0.0889, -1.0391, -0.5820, -1.8984], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1145])
attention_mask shape: torch.Size([4, 1145])
reward: tensor([-1.4375,  0.1885,  1.2656,  0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1232])
attention_mask shape: torch.Size([4, 1232])
reward: tensor([ 0.2578,  0.3730, -0.2041,  0.1934], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 266])
attention_mask shape: torch.Size([4, 266])
reward: tensor([-1.5938, -0.2227, -0.7734, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 691])
attention_mask shape: torch.Size([4, 691])
reward: tensor([ 0.8320, -1.2734, -1.4375, -1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1390])
attention_mask shape: torch.Size([4, 1390])
reward: tensor([ 1.8594, -0.3906, -0.2090, -0.4492], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1163])
attention_mask shape: torch.Size([4, 1163])
reward: tensor([ 0.0255, -1.9844, -1.2109, -0.7305], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1113])
attention_mask shape: torch.Size([4, 1113])
reward: tensor([-1.3828, -0.0977,  1.1562,  0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 523])
attention_mask shape: torch.Size([4, 523])
reward: tensor([-1.5625, -1.4297,  0.1885,  0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 806])
attention_mask shape: torch.Size([4, 806])
reward: tensor([-0.3828,  0.0811,  0.5625, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 720])
attention_mask shape: torch.Size([4, 720])
reward: tensor([ 1.3672,  0.1631,  0.6211, -1.3750], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1274])
attention_mask shape: torch.Size([4, 1274])
reward: tensor([-0.7148,  0.9375,  0.9609, -0.0532], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 514])
attention_mask shape: torch.Size([4, 514])
reward: tensor([-1.6094, -0.2734,  0.5117, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1149])
attention_mask shape: torch.Size([4, 1149])
reward: tensor([-0.9258, -1.7422, -0.0067,  0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1352])
attention_mask shape: torch.Size([4, 1352])
reward: tensor([-0.1309, -0.6328, -0.0977,  0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1077])
attention_mask shape: torch.Size([4, 1077])
reward: tensor([-1.2734,  0.1133,  0.6562, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1464])
attention_mask shape: torch.Size([4, 1464])
reward: tensor([-0.8984, -0.3242, -0.1982, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1077])
attention_mask shape: torch.Size([4, 1077])
reward: tensor([-1.2031, -0.9766,  0.8008, -1.6875], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 589])
attention_mask shape: torch.Size([4, 589])
reward: tensor([ 1.0078, -0.6680, -0.6211,  0.0767], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 854])
attention_mask shape: torch.Size([4, 854])
reward: tensor([-0.7344,  1.2109, -0.6367,  1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1254])
attention_mask shape: torch.Size([4, 1254])
reward: tensor([ 0.7109,  0.5039, -0.5547, -1.5938], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 416])
attention_mask shape: torch.Size([4, 416])
reward: tensor([-0.6211, -0.7070,  0.2812,  0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1460])
attention_mask shape: torch.Size([4, 1460])
reward: tensor([ 0.5508,  1.0000, -0.2021, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1219])
attention_mask shape: torch.Size([4, 1219])
reward: tensor([-0.6211, -0.2002,  1.8125, -1.4688], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 633])
attention_mask shape: torch.Size([4, 633])
reward: tensor([-0.2871, -0.2090, -1.6328, -0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1252])
attention_mask shape: torch.Size([4, 1252])
reward: tensor([-0.6094,  0.6094, -0.0488, -0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 982])
attention_mask shape: torch.Size([4, 982])
reward: tensor([-0.4668, -0.9492,  0.1826, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1002])
attention_mask shape: torch.Size([4, 1002])
reward: tensor([ 0.2402, -0.2422,  0.3418,  0.1143], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1043])
attention_mask shape: torch.Size([4, 1043])
reward: tensor([-0.1914, -1.2812,  0.9648, -1.5859], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1068])
attention_mask shape: torch.Size([4, 1068])
reward: tensor([-1.3828, -0.8164, -0.0244, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 627])
attention_mask shape: torch.Size([4, 627])
reward: tensor([-0.1709, -1.3125,  0.3809, -0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1104])
attention_mask shape: torch.Size([4, 1104])
reward: tensor([ 0.1621, -1.0547, -0.6641, -0.6367], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1200])
attention_mask shape: torch.Size([4, 1200])
reward: tensor([ 0.3535, -0.0200,  0.1079,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 901])
attention_mask shape: torch.Size([4, 901])
reward: tensor([-0.6758, -0.9062,  0.8086, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1396])
attention_mask shape: torch.Size([4, 1396])
reward: tensor([-0.9414, -0.5625,  0.1465,  1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1094])
attention_mask shape: torch.Size([4, 1094])
reward: tensor([-1.6016,  0.8086, -0.9688,  1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 821])
attention_mask shape: torch.Size([4, 821])
reward: tensor([ 0.4355, -1.9219, -0.2197, -0.8125], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 826])
attention_mask shape: torch.Size([4, 826])
reward: tensor([-0.7070, -0.6797,  0.1406, -0.2246], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1812])
attention_mask shape: torch.Size([4, 1812])
reward: tensor([-0.6992, -0.7461,  1.9453,  0.3906], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-0.9609, -0.1001,  0.1865,  0.2080], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 201])
attention_mask shape: torch.Size([4, 201])
reward: tensor([-0.8047, -0.0488, -2.1250, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1368])
attention_mask shape: torch.Size([4, 1368])
reward: tensor([-0.5859,  1.4688, -2.1094,  0.8438], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 615])
attention_mask shape: torch.Size([4, 615])
reward: tensor([-1.6016, -0.5781, -1.8750,  0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 934])
attention_mask shape: torch.Size([4, 934])
reward: tensor([-1.7188,  0.3086, -0.1729, -0.2334], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 681])
attention_mask shape: torch.Size([4, 681])
reward: tensor([-1.0156, -0.3418,  0.2373,  0.2715], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([-0.9336,  0.1602,  0.6328, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 949])
attention_mask shape: torch.Size([4, 949])
reward: tensor([-0.9414, -0.3867, -0.3867, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 619])
attention_mask shape: torch.Size([4, 619])
reward: tensor([ 0.1377, -2.1562,  0.1133,  0.0366], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1280])
attention_mask shape: torch.Size([4, 1280])
reward: tensor([-1.5000, -0.1982,  1.6719, -0.7227], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 799])
attention_mask shape: torch.Size([4, 799])
reward: tensor([ 0.0122, -0.9336, -0.6016, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1092])
attention_mask shape: torch.Size([4, 1092])
reward: tensor([-1.2031, -0.9062, -1.1641, -0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1234])
attention_mask shape: torch.Size([4, 1234])
reward: tensor([-1.0078,  0.4883,  0.6836, -0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 742])
attention_mask shape: torch.Size([4, 742])
reward: tensor([-0.8984,  0.1123, -0.0977, -0.5742], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 564])
attention_mask shape: torch.Size([4, 564])
reward: tensor([ 0.2412, -1.0781, -1.2812, -0.4629], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 485])
attention_mask shape: torch.Size([4, 485])
reward: tensor([-0.6992,  0.8633,  0.7109, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1440])
attention_mask shape: torch.Size([4, 1440])
reward: tensor([-0.2715, -0.4043, -1.5234, -0.3164], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1177])
attention_mask shape: torch.Size([4, 1177])
reward: tensor([-0.8711, -0.4844,  0.1953, -0.6328], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1319])
attention_mask shape: torch.Size([4, 1319])
reward: tensor([-1.0469, -0.0669, -0.0669,  1.2188], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1308])
attention_mask shape: torch.Size([4, 1308])
reward: tensor([-0.1357,  0.6172,  0.1309, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1675])
attention_mask shape: torch.Size([4, 1675])
reward: tensor([-0.8516,  0.2305, -0.4453, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 937])
attention_mask shape: torch.Size([4, 937])
reward: tensor([ 0.2158, -1.3359, -0.7109, -1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1086])
attention_mask shape: torch.Size([4, 1086])
reward: tensor([ 0.0222, -1.4609, -1.6406, -1.5469], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 637])
attention_mask shape: torch.Size([4, 637])
reward: tensor([ 1.1250, -1.0781,  0.8828, -0.2578], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 499])
attention_mask shape: torch.Size([4, 499])
reward: tensor([0.6328, 0.1709, 0.1865, 0.3496], device='cuda:0', dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1456])
attention_mask shape: torch.Size([4, 1456])
reward: tensor([-0.4043, -2.1719, -0.9766,  1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1070])
attention_mask shape: torch.Size([4, 1070])
reward: tensor([-1.4375, -1.2812, -0.3828, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1211])
attention_mask shape: torch.Size([4, 1211])
reward: tensor([-0.0957, -0.8789,  0.6406, -0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 656])
attention_mask shape: torch.Size([4, 656])
reward: tensor([ 0.9688, -0.3828, -0.4395, -0.5898], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1172])
attention_mask shape: torch.Size([4, 1172])
reward: tensor([ 0.2910, -1.2734, -0.5742, -0.5508], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.8008, -1.6328, -0.7461, -0.2021], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 837])
attention_mask shape: torch.Size([4, 837])
reward: tensor([-0.6562, -0.2227,  0.8906,  0.0623], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1177])
attention_mask shape: torch.Size([4, 1177])
reward: tensor([ 0.6680,  0.1953, -0.6680,  0.0579], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 803])
attention_mask shape: torch.Size([4, 803])
reward: tensor([ 0.5117, -0.6484, -1.2031, -0.3770], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 713])
attention_mask shape: torch.Size([4, 713])
reward: tensor([-1.7422, -0.2070, -1.5469,  0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1324])
attention_mask shape: torch.Size([4, 1324])
reward: tensor([ 0.1631, -0.2002, -1.3750, -0.2617], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 671])
attention_mask shape: torch.Size([4, 671])
reward: tensor([-1.8438, -0.1484, -0.7461, -0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 750])
attention_mask shape: torch.Size([4, 750])
reward: tensor([ 0.4043, -0.4844, -0.7969, -0.1426], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 563])
attention_mask shape: torch.Size([4, 563])
reward: tensor([ 0.6758, -0.5586, -0.4492,  0.0801], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1073])
attention_mask shape: torch.Size([4, 1073])
reward: tensor([-0.3730,  1.1797,  0.3906, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 617])
attention_mask shape: torch.Size([4, 617])
reward: tensor([-0.2354, -1.3672, -0.3105,  0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 725])
attention_mask shape: torch.Size([4, 725])
reward: tensor([-0.8164, -0.3340,  0.2480, -0.2266], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1046])
attention_mask shape: torch.Size([4, 1046])
reward: tensor([-1.0547,  0.4219, -0.5508,  0.1201], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 869])
attention_mask shape: torch.Size([4, 869])
reward: tensor([-1.1484, -0.0422, -1.4844,  1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 737])
attention_mask shape: torch.Size([4, 737])
reward: tensor([-1.0469, -1.0078,  0.2070, -0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 555])
attention_mask shape: torch.Size([4, 555])
reward: tensor([-0.2930,  0.2188, -0.6250, -0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([-0.2334, -0.2930, -1.5000,  1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1266])
attention_mask shape: torch.Size([4, 1266])
reward: tensor([-0.7461, -0.5273,  0.7148, -0.1133], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([-0.5195, -0.2314, -0.9141,  0.0522], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 858])
attention_mask shape: torch.Size([4, 858])
reward: tensor([-1.0859, -0.4570, -1.0547, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 554])
attention_mask shape: torch.Size([4, 554])
reward: tensor([-0.2812,  0.1245,  0.5273, -1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 857])
attention_mask shape: torch.Size([4, 857])
reward: tensor([ 0.8438, -0.8906, -0.6992,  1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 717])
attention_mask shape: torch.Size([4, 717])
reward: tensor([ 0.0557,  0.0669,  0.0566, -0.1621], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1082])
attention_mask shape: torch.Size([4, 1082])
reward: tensor([-0.8125,  0.0189, -0.7461,  0.2373], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1139])
attention_mask shape: torch.Size([4, 1139])
reward: tensor([-0.0244, -0.2109, -1.8125, -0.2373], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1335])
attention_mask shape: torch.Size([4, 1335])
reward: tensor([ 0.5859, -0.7148,  0.1719, -2.0000], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1123])
attention_mask shape: torch.Size([4, 1123])
reward: tensor([ 1.2109, -1.2188, -0.8906, -0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 575])
attention_mask shape: torch.Size([4, 575])
reward: tensor([ 0.6914, -0.6328, -0.7539, -0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1177])
attention_mask shape: torch.Size([4, 1177])
reward: tensor([ 2.1875, -1.9609,  1.2109, -1.6406], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1065])
attention_mask shape: torch.Size([4, 1065])
reward: tensor([ 0.9688,  0.6367, -0.1177,  0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1219])
attention_mask shape: torch.Size([4, 1219])
reward: tensor([-0.3340,  0.8633, -1.2422,  0.1211], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1449])
attention_mask shape: torch.Size([4, 1449])
reward: tensor([ 1.2656,  0.0854,  1.3516, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 756])
attention_mask shape: torch.Size([4, 756])
reward: tensor([-1.3281, -0.9062, -0.9492,  2.0312], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1244])
attention_mask shape: torch.Size([4, 1244])
reward: tensor([ 0.5781, -0.1309, -1.9844,  0.2393], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 957])
attention_mask shape: torch.Size([4, 957])
reward: tensor([-1.0938, -0.0601,  0.6250,  0.2119], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1213])
attention_mask shape: torch.Size([4, 1213])
reward: tensor([-0.3691, -1.1562, -0.1357, -0.6797], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1814])
attention_mask shape: torch.Size([4, 1814])
reward: tensor([ 0.3262, -0.1133,  0.1953, -0.3770], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1497])
attention_mask shape: torch.Size([4, 1497])
reward: tensor([-1.3594,  0.9688,  0.6133,  0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1120])
attention_mask shape: torch.Size([4, 1120])
reward: tensor([-0.4316, -0.2520, -0.8281, -0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1400])
attention_mask shape: torch.Size([4, 1400])
reward: tensor([-0.0466, -1.2734, -0.2578, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1245])
attention_mask shape: torch.Size([4, 1245])
reward: tensor([-0.2109, -0.7109, -0.5820, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 623])
attention_mask shape: torch.Size([4, 623])
reward: tensor([-0.8281, -1.0391, -0.7734, -0.4805], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1151])
attention_mask shape: torch.Size([4, 1151])
reward: tensor([-0.4629, -0.4707, -1.2969, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 572])
attention_mask shape: torch.Size([4, 572])
reward: tensor([ 1.1719, -0.9609,  0.2949, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1696])
attention_mask shape: torch.Size([4, 1696])
reward: tensor([-0.7734, -0.5195, -0.3066, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1741])
attention_mask shape: torch.Size([4, 1741])
reward: tensor([-0.7422,  0.5898,  0.5898,  0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1070])
attention_mask shape: torch.Size([4, 1070])
reward: tensor([ 0.3320, -0.2598,  0.0967, -0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1681])
attention_mask shape: torch.Size([4, 1681])
reward: tensor([ 0.3457,  0.9258, -1.0312,  0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1506])
attention_mask shape: torch.Size([4, 1506])
reward: tensor([-0.1826, -1.2422,  0.9102, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 674])
attention_mask shape: torch.Size([4, 674])
reward: tensor([ 0.8984,  0.1709, -0.5391, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 520])
attention_mask shape: torch.Size([4, 520])
reward: tensor([-0.4883, -0.1533,  0.7305, -0.2158], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1598])
attention_mask shape: torch.Size([4, 1598])
reward: tensor([-0.5469,  0.3887,  1.5469, -0.1553], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1141])
attention_mask shape: torch.Size([4, 1141])
reward: tensor([-0.4668, -0.8477, -0.1777, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-1.6094, -0.7227,  0.1455, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1251])
attention_mask shape: torch.Size([4, 1251])
reward: tensor([-1.5625, -0.1826, -0.1641, -0.2910], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 590])
attention_mask shape: torch.Size([4, 590])
reward: tensor([-1.5234, -0.3555, -0.4082,  0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1007])
attention_mask shape: torch.Size([4, 1007])
reward: tensor([-0.3516, -1.0234,  1.2344,  0.1865], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 609])
attention_mask shape: torch.Size([4, 609])
reward: tensor([-0.2021, -0.3652, -0.8633, -0.1465], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1393])
attention_mask shape: torch.Size([4, 1393])
reward: tensor([-0.3457,  1.6172, -1.4141, -0.2178], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1203])
attention_mask shape: torch.Size([4, 1203])
reward: tensor([-0.9688,  0.1914, -0.6094, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1078])
attention_mask shape: torch.Size([4, 1078])
reward: tensor([-0.9883, -0.4453, -0.1089, -0.2891], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 836])
attention_mask shape: torch.Size([4, 836])
reward: tensor([-0.1377, -1.2734,  0.0544,  0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 859])
attention_mask shape: torch.Size([4, 859])
reward: tensor([-1.1172, -0.0045,  0.2275,  0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 339])
attention_mask shape: torch.Size([4, 339])
reward: tensor([ 0.8359, -1.5859, -0.7383, -0.8398], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1130])
attention_mask shape: torch.Size([4, 1130])
reward: tensor([-0.0510,  0.4238,  0.2119, -0.1445], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 417])
attention_mask shape: torch.Size([4, 417])
reward: tensor([ 0.1562, -0.2969, -1.1250, -0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 818])
attention_mask shape: torch.Size([4, 818])
reward: tensor([-0.4004, -0.6094,  0.1943, -0.2891], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1083])
attention_mask shape: torch.Size([4, 1083])
reward: tensor([-0.4316, -0.9414, -0.3691, -0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1392])
attention_mask shape: torch.Size([4, 1392])
reward: tensor([ 1.1016,  0.8984, -2.1719,  0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1269])
attention_mask shape: torch.Size([4, 1269])
reward: tensor([-0.6250, -1.8750,  1.0703,  0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 343])
attention_mask shape: torch.Size([4, 343])
reward: tensor([-1.1406, -0.9141, -1.6250, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1168])
attention_mask shape: torch.Size([4, 1168])
reward: tensor([-0.6836, -0.2520, -1.4453, -1.6953], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-0.9258,  1.4062,  0.8672, -0.2266], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 185])
attention_mask shape: torch.Size([4, 185])
reward: tensor([-1.0547, -1.7031, -1.0391, -1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1680])
attention_mask shape: torch.Size([4, 1680])
reward: tensor([-0.0078,  0.7539, -0.1445,  0.0635], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1506])
attention_mask shape: torch.Size([4, 1506])
reward: tensor([ 1.1016, -0.8789,  0.6055, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 594])
attention_mask shape: torch.Size([4, 594])
reward: tensor([-0.9414, -0.7617,  1.0156, -0.3594], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 875])
attention_mask shape: torch.Size([4, 875])
reward: tensor([ 0.3848,  1.3828, -0.2676, -0.7539], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1223])
attention_mask shape: torch.Size([4, 1223])
reward: tensor([-0.3652, -0.8789, -0.2227,  0.6055], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 763])
attention_mask shape: torch.Size([4, 763])
reward: tensor([-0.2578, -0.9258, -0.0510, -0.9141], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 552])
attention_mask shape: torch.Size([4, 552])
reward: tensor([ 1.1406, -1.4297, -0.9062,  0.8633], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1207])
attention_mask shape: torch.Size([4, 1207])
reward: tensor([-0.1270, -0.9258, -1.3281,  0.2178], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1652])
attention_mask shape: torch.Size([4, 1652])
reward: tensor([-0.5430,  0.1514, -0.3965,  1.7344], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 505])
attention_mask shape: torch.Size([4, 505])
reward: tensor([-0.3379,  0.6680, -1.1797, -1.9297], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 494])
attention_mask shape: torch.Size([4, 494])
reward: tensor([-1.1562, -1.2969, -1.2031, -1.1719], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1006])
attention_mask shape: torch.Size([4, 1006])
reward: tensor([ 1.0469, -0.9258, -0.0557, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 994])
attention_mask shape: torch.Size([4, 994])
reward: tensor([ 0.8398, -0.6406, -0.6055, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 500])
attention_mask shape: torch.Size([4, 500])
reward: tensor([-0.5391, -0.5781, -0.2695, -0.2715], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([-1.3125,  0.4883,  0.3008,  0.8750], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 667])
attention_mask shape: torch.Size([4, 667])
reward: tensor([ 0.1709, -1.4219, -0.9766,  0.1641], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1150])
attention_mask shape: torch.Size([4, 1150])
reward: tensor([ 0.7695, -0.7695, -1.6719, -0.3828], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1100])
attention_mask shape: torch.Size([4, 1100])
reward: tensor([ 0.4883, -1.2109, -0.5586, -0.3418], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1313])
attention_mask shape: torch.Size([4, 1313])
reward: tensor([ 0.5703, -1.5547,  2.2500, -0.8398], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1394])
attention_mask shape: torch.Size([4, 1394])
reward: tensor([ 2.2656, -0.7109, -0.4180,  1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 637])
attention_mask shape: torch.Size([4, 637])
reward: tensor([-0.1133, -1.1250, -0.3516, -0.3164], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1597])
attention_mask shape: torch.Size([4, 1597])
reward: tensor([-0.2178, -0.4219, -0.7812,  1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 522])
attention_mask shape: torch.Size([4, 522])
reward: tensor([-0.7539, -1.2969, -0.0178, -0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1140])
attention_mask shape: torch.Size([4, 1140])
reward: tensor([-0.6250, -0.8906,  0.9141, -2.1719], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1179])
attention_mask shape: torch.Size([4, 1179])
reward: tensor([ 1.2422, -1.0391, -1.8438, -0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 880])
attention_mask shape: torch.Size([4, 880])
reward: tensor([ 0.7031, -2.0781,  0.7656, -0.6094], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 931])
attention_mask shape: torch.Size([4, 931])
reward: tensor([ 0.2500,  1.7656, -0.4453, -1.3438], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1550])
attention_mask shape: torch.Size([4, 1550])
reward: tensor([-0.6016, -1.1641, -0.8047, -1.8594], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1117])
attention_mask shape: torch.Size([4, 1117])
reward: tensor([ 0.2441, -0.5391,  0.8203, -0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([ 1.4062,  0.6133, -0.6016,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1614])
attention_mask shape: torch.Size([4, 1614])
reward: tensor([-0.3867, -0.8594,  1.7500, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 715])
attention_mask shape: torch.Size([4, 715])
reward: tensor([-1.1016, -0.6797, -1.6797,  0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 735])
attention_mask shape: torch.Size([4, 735])
reward: tensor([ 0.2285,  0.2236,  1.3281, -0.1934], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1269])
attention_mask shape: torch.Size([4, 1269])
reward: tensor([-0.2178, -0.0820, -0.3340, -1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1375])
attention_mask shape: torch.Size([4, 1375])
reward: tensor([-0.8086, -2.0312,  0.7070, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1378])
attention_mask shape: torch.Size([4, 1378])
reward: tensor([ 0.0300, -1.3438, -0.6719,  0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 314])
attention_mask shape: torch.Size([4, 314])
reward: tensor([-0.4082, -0.8516, -0.0156, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1127])
attention_mask shape: torch.Size([4, 1127])
reward: tensor([-0.9492,  0.3184, -0.5391, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 556])
attention_mask shape: torch.Size([4, 556])
reward: tensor([-1.2266, -0.2715,  1.4844, -1.7422], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1014])
attention_mask shape: torch.Size([4, 1014])
reward: tensor([ 0.0111, -1.7578,  1.0859,  0.0457], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1421])
attention_mask shape: torch.Size([4, 1421])
reward: tensor([ 1.4688, -1.7578,  0.2891,  0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 820])
attention_mask shape: torch.Size([4, 820])
reward: tensor([ 0.3066, -0.6211,  0.4102, -0.1309], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-30 18:12:29,112] [INFO] [launch.py:351:main] Process 901927 exits successfully.
[2024-10-30 18:13:35,177] [INFO] [launch.py:351:main] Process 901928 exits successfully.
[2024-10-30 18:15:14,272] [INFO] [launch.py:351:main] Process 901930 exits successfully.
[2024-10-30 18:15:52,309] [INFO] [launch.py:351:main] Process 901929 exits successfully.
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# [K(base) root@autodl-container-ec234bbd2e-925c6d34:~# bash run_eval_reward_openrlhf.sh
[?2004l+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-3th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-3th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_3 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-31 09:17:01,665] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-31 09:17:04,480] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-31 09:17:04,480] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-3th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_3 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-31 09:17:05,855] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-31 09:17:08,759] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-31 09:17:08,759] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-31 09:17:08,759] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-31 09:17:08,759] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-31 09:17:08,759] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-31 09:17:08,760] [INFO] [launch.py:256:main] process 949400 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-3th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_3', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-31 09:17:08,760] [INFO] [launch.py:256:main] process 949401 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-3th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_3', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-31 09:17:08,760] [INFO] [launch.py:256:main] process 949402 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-3th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_3', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-31 09:17:08,761] [INFO] [launch.py:256:main] process 949403 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-3th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_3', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-31 09:17:10,345] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-31 09:17:10,449] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-31 09:17:10,450] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-31 09:17:10,451] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-31 09:17:12,923] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-31 09:17:12,923] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-31 09:17:12,931] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-10-31 09:17:13,249] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-31 09:17:13,250] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.53it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.46it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.41it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.32it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.47it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.39it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.58it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.70it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.61it/s]
Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.34it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.39it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.59it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.65it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.55it/s]
Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.40it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.67it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.58it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.50it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.87it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.77it/s]
[2024-10-31 09:17:36,011] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 09:17:36,185] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 09:17:36,461] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-31 09:17:36,503] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-31 09:17:36,503] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-31 09:17:36,503] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 09:17:36,968] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-31 09:17:36,969] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-31 09:17:36,969] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 09:17:36,970] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 09:17:36,970] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 09:17:37,135] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-31 09:17:37,136] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-31 09:17:37,136] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.65 GB, percent = 2.7%
[2024-10-31 09:17:37,280] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-31 09:17:37,281] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-31 09:17:37,281] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.66 GB, percent = 2.7%
[2024-10-31 09:17:37,282] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-31 09:17:37,282] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-31 09:17:37,282] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-31 09:17:37,282] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-31 09:17:37,282] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe0b42d2b90>
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-31 09:17:37,283] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-31 09:17:37,284] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-31 09:17:37,284] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-31 09:17:37,284] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-31 09:17:37,284] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-31 09:17:37,285] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 09:17:42,188] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-31 09:17:42,190] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-31 09:17:42,384] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-31 09:17:42,385] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-31 09:17:42,385] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.67 GB, percent = 2.7%
[2024-10-31 09:17:42,514] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-31 09:17:42,514] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-31 09:17:42,514] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.67 GB, percent = 2.7%
[2024-10-31 09:17:42,516] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-31 09:17:42,516] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-31 09:17:42,516] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-31 09:17:42,516] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-31 09:17:42,516] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-31 09:17:42,516] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-31 09:17:42,516] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-31 09:17:42,516] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-31 09:17:42,516] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-31 09:17:42,516] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-31 09:17:42,516] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe0a5d127d0>
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-31 09:17:42,517] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-31 09:17:42,518] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-31 09:17:42,518] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 622/100000 [00:00<00:15, 6216.05it/s]Preprocessing data:   2%|█▌                                                                                          | 1635/100000 [00:00<00:11, 8515.82it/s]Preprocessing data:   3%|██▍                                                                                         | 2649/100000 [00:00<00:10, 9253.48it/s]Preprocessing data:   4%|███▎                                                                                        | 3665/100000 [00:00<00:10, 9610.66it/s]Preprocessing data:   5%|████▎                                                                                       | 4653/100000 [00:00<00:09, 9707.47it/s]Preprocessing data:   6%|█████▏                                                                                      | 5641/100000 [00:00<00:09, 9763.52it/s]Preprocessing data:   7%|██████                                                                                      | 6656/100000 [00:00<00:09, 9888.23it/s]Preprocessing data:   8%|███████                                                                                     | 7675/100000 [00:00<00:09, 9983.15it/s]Preprocessing data:   9%|███████▉                                                                                    | 8674/100000 [00:00<00:09, 9897.41it/s]Preprocessing data:  10%|████████▉                                                                                   | 9664/100000 [00:01<00:10, 8851.98it/s]Preprocessing data:  11%|█████████▋                                                                                 | 10714/100000 [00:01<00:09, 9315.64it/s]Preprocessing data:  12%|██████████▋                                                                                | 11783/100000 [00:01<00:09, 9707.95it/s]Preprocessing data:  13%|███████████▋                                                                               | 12833/100000 [00:01<00:08, 9937.70it/s]Preprocessing data:  14%|████████████▌                                                                             | 13894/100000 [00:01<00:08, 10133.42it/s]Preprocessing data:  15%|█████████████▍                                                                            | 14956/100000 [00:01<00:08, 10276.68it/s]Preprocessing data:  16%|██████████████▍                                                                           | 16007/100000 [00:01<00:08, 10343.20it/s]Preprocessing data:  17%|███████████████▎                                                                          | 17046/100000 [00:01<00:08, 10300.70it/s]Preprocessing data:  18%|████████████████▎                                                                         | 18088/100000 [00:01<00:07, 10334.20it/s]Preprocessing data:  19%|█████████████████▏                                                                        | 19127/100000 [00:01<00:07, 10348.23it/s]Preprocessing data:  20%|██████████████████▏                                                                       | 20164/100000 [00:02<00:07, 10301.64it/s]Preprocessing data:  21%|███████████████████                                                                       | 21212/100000 [00:02<00:07, 10353.95it/s]Preprocessing data:  22%|████████████████████                                                                      | 22282/100000 [00:02<00:07, 10456.79it/s]Preprocessing data:  23%|█████████████████████                                                                     | 23334/100000 [00:02<00:07, 10473.49it/s]Preprocessing data:  24%|█████████████████████▉                                                                    | 24388/100000 [00:02<00:07, 10491.36it/s]Preprocessing data:  25%|██████████████████████▉                                                                   | 25438/100000 [00:02<00:07, 10483.77it/s]Preprocessing data:  26%|███████████████████████▊                                                                  | 26487/100000 [00:02<00:07, 10427.56it/s]Preprocessing data:  28%|████████████████████████▊                                                                 | 27530/100000 [00:02<00:06, 10413.45it/s]Preprocessing data:  29%|█████████████████████████▋                                                                | 28576/100000 [00:02<00:06, 10426.35it/s]Preprocessing data:  30%|██████████████████████████▋                                                               | 29619/100000 [00:02<00:06, 10272.72it/s]Preprocessing data:  31%|███████████████████████████▌                                                              | 30658/100000 [00:03<00:06, 10306.39it/s]Preprocessing data:  32%|████████████████████████████▌                                                             | 31700/100000 [00:03<00:06, 10338.25it/s]Preprocessing data:  33%|█████████████████████████████▍                                                            | 32744/100000 [00:03<00:06, 10368.07it/s]Preprocessing data:  34%|██████████████████████████████▍                                                           | 33784/100000 [00:03<00:06, 10376.83it/s]Preprocessing data:  35%|███████████████████████████████▎                                                          | 34822/100000 [00:03<00:06, 10361.01it/s]Preprocessing data:  36%|████████████████████████████████▎                                                         | 35859/100000 [00:03<00:06, 10336.16it/s]Preprocessing data:  37%|█████████████████████████████████▏                                                        | 36893/100000 [00:03<00:06, 10336.23it/s]Preprocessing data:  38%|██████████████████████████████████▏                                                       | 37935/100000 [00:03<00:05, 10360.85it/s]Preprocessing data:  39%|███████████████████████████████████                                                       | 38973/100000 [00:03<00:05, 10365.86it/s]Preprocessing data:  40%|████████████████████████████████████                                                      | 40017/100000 [00:03<00:05, 10385.65it/s]Preprocessing data:  41%|████████████████████████████████████▉                                                     | 41057/100000 [00:04<00:05, 10387.75it/s]Preprocessing data:  42%|█████████████████████████████████████▉                                                    | 42096/100000 [00:04<00:05, 10375.81it/s]Preprocessing data:  43%|██████████████████████████████████████▊                                                   | 43145/100000 [00:04<00:05, 10409.29it/s]Preprocessing data:  44%|███████████████████████████████████████▊                                                  | 44208/100000 [00:04<00:05, 10474.69it/s]Preprocessing data:  45%|████████████████████████████████████████▋                                                 | 45270/100000 [00:04<00:05, 10516.79it/s]Preprocessing data:  46%|█████████████████████████████████████████▋                                                | 46331/100000 [00:04<00:05, 10544.22it/s]Preprocessing data:  47%|██████████████████████████████████████████▋                                               | 47386/100000 [00:04<00:04, 10527.34it/s]Preprocessing data:  48%|███████████████████████████████████████████▌                                              | 48444/100000 [00:04<00:04, 10539.98it/s]Preprocessing data:  50%|████████████████████████████████████████████▌                                             | 49504/100000 [00:04<00:04, 10556.98it/s]Preprocessing data:  51%|█████████████████████████████████████████████▌                                            | 50560/100000 [00:04<00:04, 10555.75it/s]Preprocessing data:  52%|██████████████████████████████████████████████▍                                           | 51616/100000 [00:05<00:04, 10516.87it/s]Preprocessing data:  53%|███████████████████████████████████████████████▍                                          | 52669/100000 [00:05<00:04, 10520.49it/s]Preprocessing data:  54%|████████████████████████████████████████████████▎                                         | 53722/100000 [00:05<00:04, 10521.52it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▎                                        | 54775/100000 [00:05<00:04, 10486.90it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▎                                       | 55845/100000 [00:05<00:04, 10548.94it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▏                                      | 56918/100000 [00:05<00:04, 10602.63it/s]Preprocessing data:  58%|████████████████████████████████████████████████████▏                                     | 57990/100000 [00:05<00:03, 10635.53it/s]Preprocessing data:  59%|█████████████████████████████████████████████████████▏                                    | 59057/100000 [00:05<00:03, 10645.66it/s]Preprocessing data:  60%|██████████████████████████████████████████████████████                                    | 60123/100000 [00:05<00:03, 10648.81it/s]Preprocessing data:  61%|███████████████████████████████████████████████████████                                   | 61188/100000 [00:05<00:03, 10647.23it/s]Preprocessing data:  62%|████████████████████████████████████████████████████████                                  | 62262/100000 [00:06<00:03, 10673.05it/s]Preprocessing data:  63%|█████████████████████████████████████████████████████████                                 | 63336/100000 [00:06<00:03, 10691.09it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▉                                | 64406/100000 [00:06<00:03, 10670.33it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▉                               | 65474/100000 [00:06<00:03, 10653.52it/s]Preprocessing data:  67%|███████████████████████████████████████████████████████████▉                              | 66540/100000 [00:06<00:03, 10638.35it/s]Preprocessing data:  68%|████████████████████████████████████████████████████████████▊                             | 67604/100000 [00:06<00:03, 10629.49it/s]Preprocessing data:  69%|█████████████████████████████████████████████████████████████▊                            | 68667/100000 [00:06<00:02, 10609.92it/s]Preprocessing data:  70%|██████████████████████████████████████████████████████████████▊                           | 69729/100000 [00:06<00:02, 10530.95it/s]Preprocessing data:  71%|███████████████████████████████████████████████████████████████▋                          | 70788/100000 [00:06<00:02, 10548.03it/s]Preprocessing data:  72%|████████████████████████████████████████████████████████████████▋                         | 71843/100000 [00:06<00:02, 10543.45it/s]Preprocessing data:  73%|█████████████████████████████████████████████████████████████████▌                        | 72898/100000 [00:07<00:02, 10189.45it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▎                       | 73920/100000 [00:07<00:02, 9452.28it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▏                      | 74877/100000 [00:07<00:02, 8967.20it/s]Preprocessing data:  76%|████████████████████████████████████████████████████████████████████▉                      | 75785/100000 [00:07<00:02, 8666.05it/s]Preprocessing data:  77%|█████████████████████████████████████████████████████████████████████▊                     | 76660/100000 [00:07<00:02, 8474.22it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▌                    | 77513/100000 [00:07<00:02, 8329.29it/s]Preprocessing data:  78%|███████████████████████████████████████████████████████████████████████▎                   | 78349/100000 [00:07<00:02, 8211.50it/s]Preprocessing data:  79%|████████████████████████████████████████████████████████████████████████                   | 79172/100000 [00:07<00:02, 8038.77it/s]Preprocessing data:  80%|████████████████████████████████████████████████████████████████████████▊                  | 79977/100000 [00:07<00:02, 7924.76it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▌                 | 80770/100000 [00:08<00:02, 7744.60it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▍                | 81782/100000 [00:08<00:02, 8417.69it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▏               | 82628/100000 [00:08<00:02, 8311.17it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▉               | 83462/100000 [00:08<00:01, 8285.98it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▋              | 84293/100000 [00:08<00:01, 8286.95it/s]Preprocessing data:  85%|█████████████████████████████████████████████████████████████████████████████▋             | 85326/100000 [00:08<00:01, 8885.25it/s]Preprocessing data:  86%|██████████████████████████████████████████████████████████████████████████████▍            | 86221/100000 [00:08<00:01, 8900.85it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▎           | 87113/100000 [00:08<00:01, 8550.89it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████▏          | 88113/100000 [00:08<00:01, 8969.00it/s]Preprocessing data:  89%|█████████████████████████████████████████████████████████████████████████████████          | 89018/100000 [00:09<00:01, 8991.14it/s]Preprocessing data:  90%|█████████████████████████████████████████████████████████████████████████████████▊         | 89964/100000 [00:09<00:01, 9129.06it/s]Preprocessing data:  91%|██████████████████████████████████████████████████████████████████████████████████▋        | 90880/100000 [00:09<00:01, 8291.83it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▍       | 91726/100000 [00:09<00:01, 7900.47it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▏      | 92530/100000 [00:09<00:00, 7529.68it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▉      | 93294/100000 [00:09<00:00, 7448.26it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▌     | 94046/100000 [00:09<00:00, 7419.18it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████▍    | 95033/100000 [00:09<00:00, 8106.80it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▎   | 96004/100000 [00:09<00:00, 8564.49it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▏  | 96869/100000 [00:09<00:00, 8478.56it/s]Preprocessing data:  98%|████████████████████████████████████████████████████████████████████████████████████████▉  | 97723/100000 [00:10<00:00, 8487.98it/s]Preprocessing data:  99%|█████████████████████████████████████████████████████████████████████████████████████████▊ | 98667/100000 [00:10<00:00, 8766.26it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████▋| 99670/100000 [00:10<00:00, 9136.57it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9686.42it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1469])
attention_mask shape: torch.Size([4, 1469])
reward: tensor([-0.1289, -1.1719,  0.5781, -0.2930], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1491])
attention_mask shape: torch.Size([4, 1491])
reward: tensor([ 1.8984, -0.6328, -0.4219, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1061])
attention_mask shape: torch.Size([4, 1061])
reward: tensor([-1.8984, -0.9336, -0.7109, -0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 543])
attention_mask shape: torch.Size([4, 543])
reward: tensor([-0.0801, -1.9297,  0.2256,  1.1172], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1530])
attention_mask shape: torch.Size([4, 1530])
reward: tensor([ 0.1187,  0.2393, -0.2910,  0.0811], device='cuda:0',
       dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 690])
attention_mask shape: torch.Size([4, 690])
reward: tensor([ 0.0801, -1.3359, -0.5039, -0.1045], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1562])
attention_mask shape: torch.Size([4, 1562])
reward: tensor([ 0.1445, -0.0957,  0.5586, -1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1206])
attention_mask shape: torch.Size([4, 1206])
reward: tensor([ 0.8359, -1.7812, -0.1021,  0.0635], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1143])
attention_mask shape: torch.Size([4, 1143])
reward: tensor([ 0.4629, -1.1406, -0.6211, -0.1602], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1076])
attention_mask shape: torch.Size([4, 1076])
reward: tensor([ 0.2451,  1.1562,  0.2002, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1619])
attention_mask shape: torch.Size([4, 1619])
reward: tensor([-0.6875, -0.9336, -1.3672,  0.0957], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 927])
attention_mask shape: torch.Size([4, 927])
reward: tensor([ 0.6328, -0.2695, -0.5938, -0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1055])
attention_mask shape: torch.Size([4, 1055])
reward: tensor([-1.4297, -0.3027, -0.2793, -0.1602], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1731])
attention_mask shape: torch.Size([4, 1731])
reward: tensor([-0.5625, -0.1064,  1.8750,  0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 726])
attention_mask shape: torch.Size([4, 726])
reward: tensor([ 0.3320, -0.2598,  1.3281, -0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1474])
attention_mask shape: torch.Size([4, 1474])
reward: tensor([ 0.5703, -0.5469, -0.2314, -0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1918])
attention_mask shape: torch.Size([4, 1918])
reward: tensor([-1.0781, -0.7461, -0.5508, -0.2578], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1821])
attention_mask shape: torch.Size([4, 1821])
reward: tensor([-0.2090, -0.2471, -0.4219,  0.3398], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 984])
attention_mask shape: torch.Size([4, 984])
reward: tensor([-0.7617, -0.8633, -1.5000, -0.4883], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 793])
attention_mask shape: torch.Size([4, 793])
reward: tensor([ 1.8750,  0.8164, -1.0312,  0.0344], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 917])
attention_mask shape: torch.Size([4, 917])
reward: tensor([ 0.0200, -1.4766,  0.1836, -0.9258], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1092])
attention_mask shape: torch.Size([4, 1092])
reward: tensor([ 0.1318,  0.4180, -0.2471,  1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 826])
attention_mask shape: torch.Size([4, 826])
reward: tensor([ 0.4219,  0.6172,  0.1953, -2.1250], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1483])
attention_mask shape: torch.Size([4, 1483])
reward: tensor([ 1.2109, -0.0977,  0.0713,  1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.1719,  0.0178, -0.0864, -0.9492], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1421])
attention_mask shape: torch.Size([4, 1421])
reward: tensor([ 0.2305, -0.1670,  0.4570, -1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1612])
attention_mask shape: torch.Size([4, 1612])
reward: tensor([-0.6094, -0.4531,  0.5039,  0.2949], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 360])
attention_mask shape: torch.Size([4, 360])
reward: tensor([ 0.1865, -1.0781, -0.6992, -0.1865], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 830])
attention_mask shape: torch.Size([4, 830])
reward: tensor([-0.9492, -0.8438,  0.4453,  0.2090], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1697])
attention_mask shape: torch.Size([4, 1697])
reward: tensor([0.0791, 0.2949, 0.3203, 0.2275], device='cuda:0', dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1531])
attention_mask shape: torch.Size([4, 1531])
reward: tensor([-0.5547, -1.4219, -0.6250, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1017])
attention_mask shape: torch.Size([4, 1017])
reward: tensor([-1.9531,  0.3555,  0.2637, -0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1299])
attention_mask shape: torch.Size([4, 1299])
reward: tensor([-0.5742, -0.3340,  0.7070, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1684])
attention_mask shape: torch.Size([4, 1684])
reward: tensor([ 0.5039,  0.4844, -1.3672,  0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1030])
attention_mask shape: torch.Size([4, 1030])
reward: tensor([ 0.5352,  0.6055, -1.1797, -0.7461], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.2891, -0.6055,  1.1484, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1342])
attention_mask shape: torch.Size([4, 1342])
reward: tensor([ 1.0234, -0.2129, -0.3652,  0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1076])
attention_mask shape: torch.Size([4, 1076])
reward: tensor([ 0.4316, -1.1094, -0.1982, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([ 0.0623,  0.5430,  1.2422, -1.9922], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1771])
attention_mask shape: torch.Size([4, 1771])
reward: tensor([ 1.2812, -0.4980, -0.6914,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1324])
attention_mask shape: torch.Size([4, 1324])
reward: tensor([0.9570, 0.5195, 1.2109, 1.5078], device='cuda:0', dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1534])
attention_mask shape: torch.Size([4, 1534])
reward: tensor([ 1.2422,  0.8398, -1.2422,  0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 327])
attention_mask shape: torch.Size([4, 327])
reward: tensor([-0.6836, -0.3379, -0.1621, -1.9922], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 972])
attention_mask shape: torch.Size([4, 972])
reward: tensor([-1.3750, -0.2422, -0.9492, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1505])
attention_mask shape: torch.Size([4, 1505])
reward: tensor([-0.6836,  0.7734,  0.7188,  0.9727], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1197])
attention_mask shape: torch.Size([4, 1197])
reward: tensor([ 1.1875, -0.8516,  0.1367,  0.2393], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 756])
attention_mask shape: torch.Size([4, 756])
reward: tensor([-0.1934,  0.2422,  1.5234, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.9844, -1.2422, -1.4922,  0.4297], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1721])
attention_mask shape: torch.Size([4, 1721])
reward: tensor([-0.4395, -0.1377,  0.6562, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 842])
attention_mask shape: torch.Size([4, 842])
reward: tensor([-1.4297, -0.8906, -0.2090,  0.7930], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1440])
attention_mask shape: torch.Size([4, 1440])
reward: tensor([-0.4844, -0.7969, -0.1709, -0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1292])
attention_mask shape: torch.Size([4, 1292])
reward: tensor([-0.7812,  1.6406,  0.6211,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1982])
attention_mask shape: torch.Size([4, 1982])
reward: tensor([-2.1406, -0.6875,  1.0156, -0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1435])
attention_mask shape: torch.Size([4, 1435])
reward: tensor([0.0289, 0.0167, 0.2559, 0.5156], device='cuda:0', dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1440])
attention_mask shape: torch.Size([4, 1440])
reward: tensor([-0.4141, -0.4180,  0.4668, -0.4883], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 705])
attention_mask shape: torch.Size([4, 705])
reward: tensor([-1.0312, -1.0156,  0.9141, -0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1133])
attention_mask shape: torch.Size([4, 1133])
reward: tensor([-0.8711,  0.1299,  0.8633, -0.0067], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 660])
attention_mask shape: torch.Size([4, 660])
reward: tensor([-1.2656,  0.3320,  0.2266, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1556])
attention_mask shape: torch.Size([4, 1556])
reward: tensor([ 0.3105, -0.2852,  0.7812, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1615])
attention_mask shape: torch.Size([4, 1615])
reward: tensor([-0.4180,  1.4609, -0.8789, -0.0820], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1030])
attention_mask shape: torch.Size([4, 1030])
reward: tensor([-0.3555, -1.3516, -0.5352, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1450])
attention_mask shape: torch.Size([4, 1450])
reward: tensor([-1.2109,  0.0864,  1.1094,  0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1928])
attention_mask shape: torch.Size([4, 1928])
reward: tensor([-0.1309, -0.2793, -1.0078,  0.3105], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 603])
attention_mask shape: torch.Size([4, 603])
reward: tensor([ 0.5469,  0.5664, -0.3867, -0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1424])
attention_mask shape: torch.Size([4, 1424])
reward: tensor([ 0.2051, -0.4004,  1.6641, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 874])
attention_mask shape: torch.Size([4, 874])
reward: tensor([-0.7617,  1.0234,  0.1729,  0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1145])
attention_mask shape: torch.Size([4, 1145])
reward: tensor([-0.6992, -0.1357, -0.6797, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1376])
attention_mask shape: torch.Size([4, 1376])
reward: tensor([ 0.0334, -0.3164,  0.8086,  1.7344], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 815])
attention_mask shape: torch.Size([4, 815])
reward: tensor([1.3594, 0.4375, 0.6836, 0.3320], device='cuda:0', dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1323])
attention_mask shape: torch.Size([4, 1323])
reward: tensor([ 0.0278, -1.1641,  1.1719,  1.1406], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1637])
attention_mask shape: torch.Size([4, 1637])
reward: tensor([ 0.0879,  0.3867, -0.3770,  1.1719], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 854])
attention_mask shape: torch.Size([4, 854])
reward: tensor([ 0.3867, -1.2031, -0.4004, -1.8281], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1065])
attention_mask shape: torch.Size([4, 1065])
reward: tensor([-0.0067, -0.2373,  0.9062, -0.1377], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1371])
attention_mask shape: torch.Size([4, 1371])
reward: tensor([-0.2285,  0.0654,  0.0791,  1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 241])
attention_mask shape: torch.Size([4, 241])
reward: tensor([-1.3594, -0.5195, -0.5938, -1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 787])
attention_mask shape: torch.Size([4, 787])
reward: tensor([ 0.9219, -0.2490, -0.2773, -0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1322])
attention_mask shape: torch.Size([4, 1322])
reward: tensor([ 1.8125,  0.0466, -0.9883, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1303])
attention_mask shape: torch.Size([4, 1303])
reward: tensor([-0.3594, -0.3906, -0.1846, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1247])
attention_mask shape: torch.Size([4, 1247])
reward: tensor([-0.3242,  1.1484, -1.0859,  0.0178], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 741])
attention_mask shape: torch.Size([4, 741])
reward: tensor([-0.5820,  0.2715,  0.3105, -1.3984], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1068])
attention_mask shape: torch.Size([4, 1068])
reward: tensor([-0.9336,  0.1367,  1.4062, -1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1056])
attention_mask shape: torch.Size([4, 1056])
reward: tensor([ 1.2891, -1.8281,  0.1079, -0.1064], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1422])
attention_mask shape: torch.Size([4, 1422])
reward: tensor([-0.0266,  0.4160,  1.3516, -0.0688], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 383])
attention_mask shape: torch.Size([4, 383])
reward: tensor([-0.8789, -0.8086, -0.0045, -0.0532], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1655])
attention_mask shape: torch.Size([4, 1655])
reward: tensor([-0.7070, -0.5234,  0.2520,  0.4316], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1478])
attention_mask shape: torch.Size([4, 1478])
reward: tensor([ 0.1934, -0.6250, -0.6914,  0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1397])
attention_mask shape: torch.Size([4, 1397])
reward: tensor([-1.0156,  0.0776, -0.6562,  1.4219], device='cuda:0',
       dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6562, -0.8086, -0.1982, -0.2715], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1523])
attention_mask shape: torch.Size([4, 1523])
reward: tensor([ 0.2676, -1.5469,  0.8594,  0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 785])
attention_mask shape: torch.Size([4, 785])
reward: tensor([-0.0244, -0.5859,  0.8359, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 770])
attention_mask shape: torch.Size([4, 770])
reward: tensor([-0.7812,  1.1328, -1.1250,  1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1553])
attention_mask shape: torch.Size([4, 1553])
reward: tensor([ 0.9570,  0.9141, -1.0391, -0.1865], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1245])
attention_mask shape: torch.Size([4, 1245])
reward: tensor([-0.0977, -0.3457, -0.4355,  0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1809])
attention_mask shape: torch.Size([4, 1809])
reward: tensor([ 1.6406,  1.3828, -0.2178, -0.1641], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1429])
attention_mask shape: torch.Size([4, 1429])
reward: tensor([ 0.3906, -0.2002,  0.8789, -1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1004])
attention_mask shape: torch.Size([4, 1004])
reward: tensor([ 0.1167,  1.4844, -1.4297,  0.0056], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1389])
attention_mask shape: torch.Size([4, 1389])
reward: tensor([-0.6094,  0.0654,  0.7148, -0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 926])
attention_mask shape: torch.Size([4, 926])
reward: tensor([-0.6367, -0.3418,  1.1562, -1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 926])
attention_mask shape: torch.Size([4, 926])
reward: tensor([-0.0133, -0.4355, -0.1641, -0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1183])
attention_mask shape: torch.Size([4, 1183])
reward: tensor([ 0.1797, -0.1621,  0.7930, -1.3047], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1677])
attention_mask shape: torch.Size([4, 1677])
reward: tensor([-1.3359, -0.6836, -1.4453, -0.0557], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 597])
attention_mask shape: torch.Size([4, 597])
reward: tensor([ 0.1235, -0.9492, -0.4980,  0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1229])
attention_mask shape: torch.Size([4, 1229])
reward: tensor([ 0.5039, -0.4570, -0.4746,  0.0334], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1397])
attention_mask shape: torch.Size([4, 1397])
reward: tensor([ 1.5000, -0.1709,  0.1201,  0.2197], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1072])
attention_mask shape: torch.Size([4, 1072])
reward: tensor([ 0.3047, -1.3594,  0.6758,  0.2041], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1445])
attention_mask shape: torch.Size([4, 1445])
reward: tensor([-0.6836,  0.1089, -0.3457,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1317])
attention_mask shape: torch.Size([4, 1317])
reward: tensor([-0.4453,  0.3926, -0.1982,  0.8008], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 940])
attention_mask shape: torch.Size([4, 940])
reward: tensor([ 1.1328,  0.2891, -0.4004, -0.0444], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 997])
attention_mask shape: torch.Size([4, 997])
reward: tensor([-1.8438, -1.0391, -0.9766, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1579])
attention_mask shape: torch.Size([4, 1579])
reward: tensor([ 0.0532, -0.8086,  1.4766,  0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1246])
attention_mask shape: torch.Size([4, 1246])
reward: tensor([-0.5078, -0.1514,  0.2227, -0.0557], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 223])
attention_mask shape: torch.Size([4, 223])
reward: tensor([-0.5352, -1.0312,  1.0156, -0.6367], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1552])
attention_mask shape: torch.Size([4, 1552])
reward: tensor([-0.0688,  0.6680, -1.1484,  1.5625], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 691])
attention_mask shape: torch.Size([4, 691])
reward: tensor([-1.0703,  0.5078, -1.1094, -0.1201], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1043])
attention_mask shape: torch.Size([4, 1043])
reward: tensor([-0.0244,  0.8125, -0.1729,  0.4082], device='cuda:0',
       dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 918])
attention_mask shape: torch.Size([4, 918])
reward: tensor([-0.6133,  0.0133,  0.1768, -1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-0.8086, -0.4082,  1.0703, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1006])
attention_mask shape: torch.Size([4, 1006])
reward: tensor([-0.8359, -0.5273,  0.7422,  0.0322], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1358])
attention_mask shape: torch.Size([4, 1358])
reward: tensor([-0.0444, -1.6406, -1.4062, -2.1406], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1275])
attention_mask shape: torch.Size([4, 1275])
reward: tensor([-0.0444, -0.3906,  1.3359, -1.5703], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1457])
attention_mask shape: torch.Size([4, 1457])
reward: tensor([-1.0547, -0.0820,  0.3594, -0.9961], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1677])
attention_mask shape: torch.Size([4, 1677])
reward: tensor([ 2.0312, -1.7266, -1.4297, -0.5039], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1350])
attention_mask shape: torch.Size([4, 1350])
reward: tensor([-1.1484,  1.0781,  0.6758, -0.6562], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1358])
attention_mask shape: torch.Size([4, 1358])
reward: tensor([-0.8984, -0.5078,  0.5195,  0.3066], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 778])
attention_mask shape: torch.Size([4, 778])
reward: tensor([ 0.8438, -0.7812, -0.8047, -0.2598], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 901])
attention_mask shape: torch.Size([4, 901])
reward: tensor([-0.5938,  1.0234, -0.8047, -0.1514], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1409])
attention_mask shape: torch.Size([4, 1409])
reward: tensor([-0.1689, -0.4980, -0.4043, -0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1372])
attention_mask shape: torch.Size([4, 1372])
reward: tensor([-0.2871,  0.2969,  1.4922, -0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1301])
attention_mask shape: torch.Size([4, 1301])
reward: tensor([-0.8984, -0.3105, -0.9766,  0.8164], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1436])
attention_mask shape: torch.Size([4, 1436])
reward: tensor([ 0.3418,  0.5859,  0.4980, -0.5703], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2022])
attention_mask shape: torch.Size([4, 2022])
reward: tensor([-1.0703,  0.2715,  0.7930, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1424])
attention_mask shape: torch.Size([4, 1424])
reward: tensor([-0.1533, -1.4844, -0.0133, -1.1094], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1241])
attention_mask shape: torch.Size([4, 1241])
reward: tensor([-0.3164,  0.1758, -1.0312, -1.4375], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 533])
attention_mask shape: torch.Size([4, 533])
reward: tensor([ 0.7305, -1.4609,  0.7383, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 656])
attention_mask shape: torch.Size([4, 656])
reward: tensor([ 0.3965, -1.0859,  0.8633,  0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1444])
attention_mask shape: torch.Size([4, 1444])
reward: tensor([-0.5156, -1.5312, -0.0532,  1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1209])
attention_mask shape: torch.Size([4, 1209])
reward: tensor([ 0.2812,  0.3809,  0.6484, -0.2246], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1856])
attention_mask shape: torch.Size([4, 1856])
reward: tensor([ 1.2266, -2.1719,  0.5195,  0.6875], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 928])
attention_mask shape: torch.Size([4, 928])
reward: tensor([-0.6797, -0.6484,  0.9062,  0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1554])
attention_mask shape: torch.Size([4, 1554])
reward: tensor([-0.1270, -0.3105, -1.0234, -0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([-0.3203, -0.4805, -0.7930, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1128])
attention_mask shape: torch.Size([4, 1128])
reward: tensor([-0.7383,  1.2266,  1.0234, -0.2559], device='cuda:0',
       dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1311])
attention_mask shape: torch.Size([4, 1311])
reward: tensor([ 1.1797,  0.1318, -0.8711,  0.5820], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 957])
attention_mask shape: torch.Size([4, 957])
reward: tensor([ 0.5117, -0.5039, -0.9961, -2.0938], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 802])
attention_mask shape: torch.Size([4, 802])
reward: tensor([-1.2734, -1.4219, -1.4922,  0.2871], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1734])
attention_mask shape: torch.Size([4, 1734])
reward: tensor([ 0.4316, -1.6875, -1.0781, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1005])
attention_mask shape: torch.Size([4, 1005])
reward: tensor([-1.2188,  0.0579,  0.3770,  0.0835], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 915])
attention_mask shape: torch.Size([4, 915])
reward: tensor([ 0.7734,  0.0544, -1.0156, -0.7734], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 666])
attention_mask shape: torch.Size([4, 666])
reward: tensor([ 1.2266, -0.7148, -0.4883, -0.5508], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1106])
attention_mask shape: torch.Size([4, 1106])
reward: tensor([0.7539, 0.3730, 0.4258, 0.0466], device='cuda:0', dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 856])
attention_mask shape: torch.Size([4, 856])
reward: tensor([-0.3828, -1.3438, -0.3965,  1.3672], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1049])
attention_mask shape: torch.Size([4, 1049])
reward: tensor([-0.4453,  0.3984,  0.6680,  0.4805], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-1.1641,  1.2266, -1.0391, -1.5859], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 821])
attention_mask shape: torch.Size([4, 821])
reward: tensor([-0.3164, -1.2656, -1.2734,  1.4297], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1345])
attention_mask shape: torch.Size([4, 1345])
reward: tensor([-0.1934, -2.0938,  0.6914,  0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 588])
attention_mask shape: torch.Size([4, 588])
reward: tensor([-0.8711, -0.8398, -0.5625,  0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1055])
attention_mask shape: torch.Size([4, 1055])
reward: tensor([-0.6016, -0.3965, -0.0334, -0.0579], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1328])
attention_mask shape: torch.Size([4, 1328])
reward: tensor([-0.6758, -0.5859, -0.0820,  1.3125], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1595])
attention_mask shape: torch.Size([4, 1595])
reward: tensor([ 0.4668, -0.1289, -0.7344, -0.1177], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1285])
attention_mask shape: torch.Size([4, 1285])
reward: tensor([ 0.0679,  0.5703,  0.3086, -0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1194])
attention_mask shape: torch.Size([4, 1194])
reward: tensor([-1.0859, -1.2500,  0.2949, -1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1002])
attention_mask shape: torch.Size([4, 1002])
reward: tensor([ 0.3594, -1.1094, -0.2314, -0.4805], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 910])
attention_mask shape: torch.Size([4, 910])
reward: tensor([0.1758, 0.6875, 1.2656, 0.2617], device='cuda:0', dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1079])
attention_mask shape: torch.Size([4, 1079])
reward: tensor([ 0.1982, -0.4004, -0.3379,  0.4414], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1086])
attention_mask shape: torch.Size([4, 1086])
reward: tensor([-0.0289, -1.2969,  0.0913, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1344])
attention_mask shape: torch.Size([4, 1344])
reward: tensor([ 0.6445,  0.0366,  0.2793, -0.8711], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1332])
attention_mask shape: torch.Size([4, 1332])
reward: tensor([ 1.0547, -0.7656, -0.0200, -0.1270], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 701])
attention_mask shape: torch.Size([4, 701])
reward: tensor([ 0.0255, -0.4180, -0.7695, -0.0864], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1255])
attention_mask shape: torch.Size([4, 1255])
reward: tensor([ 1.8984, -0.7383,  0.6328, -1.1016], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1176])
attention_mask shape: torch.Size([4, 1176])
reward: tensor([-0.1396,  0.0244, -1.1875, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1205])
attention_mask shape: torch.Size([4, 1205])
reward: tensor([-0.1982,  0.6836, -0.1465,  0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1543])
attention_mask shape: torch.Size([4, 1543])
reward: tensor([ 1.4141, -0.1177,  1.7812, -1.0859], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 747])
attention_mask shape: torch.Size([4, 747])
reward: tensor([-1.3281, -1.1719, -1.0078,  1.7344], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1173])
attention_mask shape: torch.Size([4, 1173])
reward: tensor([-0.5820, -0.2158, -2.1094,  0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1045])
attention_mask shape: torch.Size([4, 1045])
reward: tensor([-0.5156,  1.2812,  0.4844,  0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1591])
attention_mask shape: torch.Size([4, 1591])
reward: tensor([ 0.1157, -0.5938,  0.9297, -0.0400], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1859])
attention_mask shape: torch.Size([4, 1859])
reward: tensor([ 1.2891,  0.0432, -0.8203, -0.5547], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1528])
attention_mask shape: torch.Size([4, 1528])
reward: tensor([0.1846, 1.0234, 0.7109, 0.6484], device='cuda:0', dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1937])
attention_mask shape: torch.Size([4, 1937])
reward: tensor([-0.7344, -1.3672,  0.8672, -0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1504])
attention_mask shape: torch.Size([4, 1504])
reward: tensor([-0.7188, -1.1562, -0.2539, -0.2354], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 977])
attention_mask shape: torch.Size([4, 977])
reward: tensor([ 0.3457,  0.4316, -0.5938, -1.0547], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 713])
attention_mask shape: torch.Size([4, 713])
reward: tensor([-0.1445,  0.5078, -0.7461, -0.0444], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1579])
attention_mask shape: torch.Size([4, 1579])
reward: tensor([-0.2930, -0.6992, -1.3516,  0.8945], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 663])
attention_mask shape: torch.Size([4, 663])
reward: tensor([ 1.0703, -1.0859,  0.0133, -1.1641], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2029])
attention_mask shape: torch.Size([4, 2029])
reward: tensor([-1.3828, -0.4668,  1.5703, -1.7578], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1491])
attention_mask shape: torch.Size([4, 1491])
reward: tensor([-0.3965, -0.6094, -0.0820, -0.0289], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1108])
attention_mask shape: torch.Size([4, 1108])
reward: tensor([-0.2891,  0.2871,  0.0967, -0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.2109,  0.9688, -1.9922,  0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1791])
attention_mask shape: torch.Size([4, 1791])
reward: tensor([ 1.0547, -0.2178,  0.8828, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 855])
attention_mask shape: torch.Size([4, 855])
reward: tensor([ 1.1719e+00, -2.4414e-01, -1.1139e-03, -2.4219e-01], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 819])
attention_mask shape: torch.Size([4, 819])
reward: tensor([ 0.0967,  0.0579,  0.7539, -0.2070], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1624])
attention_mask shape: torch.Size([4, 1624])
reward: tensor([-2.1562, -0.2891,  0.7656,  0.7539], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1163])
attention_mask shape: torch.Size([4, 1163])
reward: tensor([-0.6719, -1.2109, -0.1982, -0.2637], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-2.2031, -0.5078, -0.5859, -0.2969], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1652])
attention_mask shape: torch.Size([4, 1652])
reward: tensor([ 0.9141,  0.1826,  0.9609, -0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 627])
attention_mask shape: torch.Size([4, 627])
reward: tensor([-0.8633, -1.4453, -0.1426,  0.3691], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1157])
attention_mask shape: torch.Size([4, 1157])
reward: tensor([-0.1426, -1.3125,  0.4082,  0.4395], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 700])
attention_mask shape: torch.Size([4, 700])
reward: tensor([-0.0111, -1.2266, -0.8906, -1.5312], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1398])
attention_mask shape: torch.Size([4, 1398])
reward: tensor([ 0.1689,  1.1016, -0.2158, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1667])
attention_mask shape: torch.Size([4, 1667])
reward: tensor([-0.9688,  0.2793,  0.1279, -1.6562], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1114])
attention_mask shape: torch.Size([4, 1114])
reward: tensor([-0.7461, -0.5117, -0.1582, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([-0.3066, -1.8672, -0.1514, -0.5625], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 871])
attention_mask shape: torch.Size([4, 871])
reward: tensor([-0.4043, -0.4316, -0.1641, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 603])
attention_mask shape: torch.Size([4, 603])
reward: tensor([ 0.7852, -2.2031, -1.0703, -0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1575])
attention_mask shape: torch.Size([4, 1575])
reward: tensor([ 0.2598,  1.2812, -1.5703,  0.0432], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 512])
attention_mask shape: torch.Size([4, 512])
reward: tensor([-0.6172,  0.5703, -1.5000, -0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1151])
attention_mask shape: torch.Size([4, 1151])
reward: tensor([-1.1641, -0.9414,  0.0889, -0.1357], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1879])
attention_mask shape: torch.Size([4, 1879])
reward: tensor([-0.1113, -0.3379, -1.8594, -0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1373])
attention_mask shape: torch.Size([4, 1373])
reward: tensor([ 1.0000,  1.0938, -1.1875,  0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.5312, -0.8398,  1.3438,  0.0488], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 478])
attention_mask shape: torch.Size([4, 478])
reward: tensor([-1.0547, -0.3418, -0.9883, -0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1199])
attention_mask shape: torch.Size([4, 1199])
reward: tensor([-0.6758, -0.2041, -0.2715, -1.9453], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-0.3379,  1.9844,  1.1406, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 787])
attention_mask shape: torch.Size([4, 787])
reward: tensor([ 0.6055, -1.1875, -0.9492,  0.0111], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.6250,  0.7109, -0.2471,  0.8828], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1363])
attention_mask shape: torch.Size([4, 1363])
reward: tensor([ 0.8008,  0.7227, -0.1758, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 995])
attention_mask shape: torch.Size([4, 995])
reward: tensor([-0.9609, -1.6016, -0.4941, -0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1134])
attention_mask shape: torch.Size([4, 1134])
reward: tensor([ 0.3691,  2.3125,  0.6719, -1.0469], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1269])
attention_mask shape: torch.Size([4, 1269])
reward: tensor([ 0.5703, -0.6133,  0.3164,  0.1709], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 609])
attention_mask shape: torch.Size([4, 609])
reward: tensor([ 0.2910, -1.1562, -0.1777, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 682])
attention_mask shape: torch.Size([4, 682])
reward: tensor([ 0.5820, -0.4453, -1.6719,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1274])
attention_mask shape: torch.Size([4, 1274])
reward: tensor([-0.6562, -0.0933, -2.1094, -1.3750], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1563])
attention_mask shape: torch.Size([4, 1563])
reward: tensor([ 0.3047,  0.0933, -0.3555,  1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1235])
attention_mask shape: torch.Size([4, 1235])
reward: tensor([-0.6641, -1.2734, -1.1406, -0.5352], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 700])
attention_mask shape: torch.Size([4, 700])
reward: tensor([ 0.5625, -0.5039, -1.4297, -1.1875], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([ 1.2422, -0.6133, -1.0234,  0.4746], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1042])
attention_mask shape: torch.Size([4, 1042])
reward: tensor([ 0.8516, -1.1797, -0.1221,  0.0322], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 579])
attention_mask shape: torch.Size([4, 579])
reward: tensor([-0.6523, -0.9883, -1.1172,  0.6836], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1628])
attention_mask shape: torch.Size([4, 1628])
reward: tensor([-0.7344,  0.7109, -0.7773,  0.2334], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 936])
attention_mask shape: torch.Size([4, 936])
reward: tensor([ 0.0889, -2.1250, -0.2871,  0.8750], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1021])
attention_mask shape: torch.Size([4, 1021])
reward: tensor([-0.4180, -0.0845, -0.6211, -1.0938], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.0200, -1.8203, -0.5586,  0.1001], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1446])
attention_mask shape: torch.Size([4, 1446])
reward: tensor([ 0.8203, -1.3594,  2.5938, -0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1435])
attention_mask shape: torch.Size([4, 1435])
reward: tensor([ 2.1562, -1.1016, -0.3281,  1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1028])
attention_mask shape: torch.Size([4, 1028])
reward: tensor([ 0.6250, -0.5898,  1.7812,  1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1971])
attention_mask shape: torch.Size([4, 1971])
reward: tensor([ 0.1582, -0.4219, -1.0391,  0.8516], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 808])
attention_mask shape: torch.Size([4, 808])
reward: tensor([-0.6250, -1.8047, -0.0156,  0.1533], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1384])
attention_mask shape: torch.Size([4, 1384])
reward: tensor([-0.7188, -0.7305,  0.6172, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1205])
attention_mask shape: torch.Size([4, 1205])
reward: tensor([ 1.1797,  0.0522, -0.6211, -0.2334], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 885])
attention_mask shape: torch.Size([4, 885])
reward: tensor([-0.9062, -1.9531,  0.7812, -0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1076])
attention_mask shape: torch.Size([4, 1076])
reward: tensor([ 0.4238,  2.1406,  0.4688, -1.0703], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1289])
attention_mask shape: torch.Size([4, 1289])
reward: tensor([-0.5586, -1.2734,  0.5078, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1040])
attention_mask shape: torch.Size([4, 1040])
reward: tensor([-0.4180, -0.2578, -1.5391,  0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1426])
attention_mask shape: torch.Size([4, 1426])
reward: tensor([-0.6367,  0.7109, -1.3594,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1685])
attention_mask shape: torch.Size([4, 1685])
reward: tensor([-0.4141,  0.5703,  1.6875, -0.8906], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 882])
attention_mask shape: torch.Size([4, 882])
reward: tensor([-0.5508, -0.0222, -1.5000,  0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1084])
attention_mask shape: torch.Size([4, 1084])
reward: tensor([ 0.9648, -1.1641,  1.2812,  0.0610], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1234])
attention_mask shape: torch.Size([4, 1234])
reward: tensor([ 0.1621, -0.4883, -1.2422, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1394])
attention_mask shape: torch.Size([4, 1394])
reward: tensor([ 0.1387, -0.5156,  0.5469, -0.1357], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1279])
attention_mask shape: torch.Size([4, 1279])
reward: tensor([-0.0532, -0.7227, -0.7031,  0.2041], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 569])
attention_mask shape: torch.Size([4, 569])
reward: tensor([ 0.1338,  0.3125, -0.4492,  0.3613], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1242])
attention_mask shape: torch.Size([4, 1242])
reward: tensor([-0.2158,  0.7422, -0.2266, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
[2024-10-31 10:08:10,416] [INFO] [launch.py:351:main] Process 949401 exits successfully.
sequences shape: torch.Size([4, 834])
attention_mask shape: torch.Size([4, 834])
reward: tensor([-1.2109, -0.3652, -1.1250, -1.4453], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 617])
attention_mask shape: torch.Size([4, 617])
reward: tensor([ 0.1270, -0.0133,  1.4609, -0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1711])
attention_mask shape: torch.Size([4, 1711])
reward: tensor([ 1.5000,  0.7305,  1.0000, -0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-0.0200, -0.6367,  0.2793, -0.1089], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-31 10:09:11,479] [INFO] [launch.py:351:main] Process 949400 exits successfully.
[2024-10-31 10:09:14,481] [INFO] [launch.py:351:main] Process 949403 exits successfully.
[2024-10-31 10:11:44,630] [INFO] [launch.py:351:main] Process 949402 exits successfully.
+ read -r -d '' training_commands
+ [[ /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-4th != \s\l\u\r\m ]]
+ deepspeed /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-4th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_4 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-31 10:11:48,687] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-31 10:11:50,564] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-10-31 10:11:50,564] [INFO] [runner.py:585:main] cmd = /root/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /root/eval_reward_openrlhf.py --pretrain /root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-4th --output_path /root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_4 --training_steps 512 --eval_train_steps 128 --eval_test_steps 128 --reward_pretrain OpenRLHF/Llama-3-8b-rm-mixture --save_steps -1 --logging_steps 1 --eval_steps -1 --micro_train_batch_size 16 --train_batch_size 128 --micro_rollout_batch_size 4 --rollout_batch_size 1024 --max_epochs 1 --prompt_max_len 1024 --generate_max_len 1024 --zero_stage 2 --bf16 --prompt_data OpenRLHF/prompt-collection-v0.1 --input_key context_messages --apply_chat_template --max_samples 100000 --normalize_reward --adam_offload --flash_attn --gradient_checkpointing --actor_init_on_gpu
[2024-10-31 10:11:51,993] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-31 10:11:53,998] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-10-31 10:11:53,998] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-10-31 10:11:53,998] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-10-31 10:11:53,999] [INFO] [launch.py:164:main] dist_world_size=4
[2024-10-31 10:11:53,999] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-10-31 10:11:53,999] [INFO] [launch.py:256:main] process 953151 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=0', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-4th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_4', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-31 10:11:53,999] [INFO] [launch.py:256:main] process 953152 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=1', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-4th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_4', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-31 10:11:54,000] [INFO] [launch.py:256:main] process 953153 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=2', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-4th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_4', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-31 10:11:54,000] [INFO] [launch.py:256:main] process 953154 spawned with command: ['/root/miniconda3/bin/python', '-u', '/root/eval_reward_openrlhf.py', '--local_rank=3', '--pretrain', '/root/.cache/huggingface/hub/models_OnlineIPO_512prompt_trivial-4th', '--output_path', '/root/autodl-tmp/ckpt/Eval_openrlhf_Llama-3.2-1B-Instruct_OnlineIPO_4', '--training_steps', '512', '--eval_train_steps', '128', '--eval_test_steps', '128', '--reward_pretrain', 'OpenRLHF/Llama-3-8b-rm-mixture', '--save_steps', '-1', '--logging_steps', '1', '--eval_steps', '-1', '--micro_train_batch_size', '16', '--train_batch_size', '128', '--micro_rollout_batch_size', '4', '--rollout_batch_size', '1024', '--max_epochs', '1', '--prompt_max_len', '1024', '--generate_max_len', '1024', '--zero_stage', '2', '--bf16', '--prompt_data', 'OpenRLHF/prompt-collection-v0.1', '--input_key', 'context_messages', '--apply_chat_template', '--max_samples', '100000', '--normalize_reward', '--adam_offload', '--flash_attn', '--gradient_checkpointing', '--actor_init_on_gpu']
[2024-10-31 10:11:55,578] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-31 10:11:55,612] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-31 10:11:55,630] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-10-31 10:11:55,634] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-10-31 10:11:59,158] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-31 10:11:59,158] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-10-31 10:11:59,572] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-31 10:11:59,573] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-10-31 10:11:59,574] [INFO] [comm.py:652:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.45it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.37it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  6.32it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.35it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  5.63it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  6.35it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.57it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.49it/s]
Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  5.72it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  6.35it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  5.68it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.57it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.48it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.98it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.87it/s]
[2024-10-31 10:12:02,104] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Actor(
  (model): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(128256, 2048, padding_idx=128009)
      (layers): ModuleList(
        (0-15): 16 x LlamaDecoderLayer(
          (self_attn): LlamaFlashAttention2(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=512, bias=False)
            (v_proj): Linear(in_features=2048, out_features=512, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)
            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        )
      )
      (norm): LlamaRMSNorm((2048,), eps=1e-05)
      (rotary_emb): LlamaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)
  )
)
RewardModel(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (value_head): Linear(in_features=4096, out_features=1, bias=False)
)
[2024-10-31 10:12:03,226] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-31 10:12:03,226] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-31 10:12:03,226] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 10:12:05,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|                                                                                                       | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████▊                                                                       | 1/4 [00:00<00:00,  7.58it/s]Loading checkpoint shards:  50%|███████████████████████████████████████████████▌                                               | 2/4 [00:00<00:00,  7.58it/s]Loading checkpoint shards:  75%|███████████████████████████████████████████████████████████████████████▎                       | 3/4 [00:00<00:00,  7.51it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.81it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.71it/s]
[2024-10-31 10:12:12,338] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 10:12:12,804] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 10:12:12,805] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-31 10:12:12,806] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-31 10:12:12,806] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 10:12:12,806] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 10:12:13,003] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-31 10:12:13,004] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-31 10:12:13,004] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.95 GB, percent = 2.8%
[2024-10-31 10:12:13,133] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-31 10:12:13,134] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB 
[2024-10-31 10:12:13,134] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.95 GB, percent = 2.8%
[2024-10-31 10:12:13,135] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-31 10:12:13,135] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f76d4bae4a0>
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-31 10:12:13,136] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-31 10:12:13,137] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-31 10:12:13,137] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
[2024-10-31 10:12:13,137] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2024-10-31 10:12:13,137] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[2024-10-31 10:12:13,137] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2024-10-31 10:12:17,186] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-10-31 10:12:17,188] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-10-31 10:12:17,319] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2024-10-31 10:12:17,320] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-31 10:12:17,320] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.96 GB, percent = 2.8%
[2024-10-31 10:12:17,440] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2024-10-31 10:12:17,441] [INFO] [utils.py:782:see_memory_usage] MA 16.28 GB         Max_MA 16.28 GB         CA 16.41 GB         Max_CA 16 GB 
[2024-10-31 10:12:17,441] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.96 GB, percent = 2.8%
[2024-10-31 10:12:17,442] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-10-31 10:12:17,442] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-10-31 10:12:17,442] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-10-31 10:12:17,442] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-10-31 10:12:17,442] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f76ccd1a920>
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-10-31 10:12:17,443] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   world_size ................... 4
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-10-31 10:12:17,444] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-10-31 10:12:17,444] [INFO] [config.py:989:print_user_config]   json = {
    "steps_per_print": 100, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": "auto", 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "train_micro_batch_size_per_gpu": 16, 
    "train_batch_size": 128
}
dataset: OpenRLHF/prompt-collection-v0.1
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
Using the latest cached version of the dataset since OpenRLHF/prompt-collection-v0.1 couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/OpenRLHF___prompt-collection-v0.1/default/0.0.0/1d3be64c51aa57fa16aa5dc70d1bfc26e9847e12 (last modified on Fri Oct  4 23:01:51 2024).
loaded OpenRLHF/prompt-collection-v0.1 from files
[Dataset({
    features: ['dataset', 'context', 'context_messages', 'id'],
    num_rows: 100000
})]
Preprocessing data:   0%|                                                                                                         | 0/100000 [00:00<?, ?it/s]Preprocessing data:   1%|▌                                                                                            | 626/100000 [00:00<00:15, 6253.52it/s]Preprocessing data:   2%|█▍                                                                                          | 1626/100000 [00:00<00:11, 8455.41it/s]Preprocessing data:   3%|██▍                                                                                         | 2649/100000 [00:00<00:10, 9264.77it/s]Preprocessing data:   4%|███▎                                                                                        | 3649/100000 [00:00<00:10, 9553.82it/s]Preprocessing data:   5%|████▎                                                                                       | 4678/100000 [00:00<00:09, 9817.89it/s]Preprocessing data:   6%|█████▏                                                                                      | 5699/100000 [00:00<00:09, 9949.25it/s]Preprocessing data:   7%|██████                                                                                     | 6714/100000 [00:00<00:09, 10012.88it/s]Preprocessing data:   8%|███████                                                                                    | 7729/100000 [00:00<00:09, 10055.06it/s]Preprocessing data:   9%|███████▉                                                                                   | 8746/100000 [00:00<00:09, 10088.29it/s]Preprocessing data:  10%|████████▉                                                                                  | 9761/100000 [00:01<00:08, 10105.37it/s]Preprocessing data:  11%|█████████▋                                                                                | 10817/100000 [00:01<00:08, 10243.72it/s]Preprocessing data:  12%|██████████▋                                                                               | 11860/100000 [00:01<00:08, 10299.64it/s]Preprocessing data:  13%|███████████▋                                                                              | 12931/100000 [00:01<00:08, 10422.89it/s]Preprocessing data:  14%|████████████▌                                                                             | 14002/100000 [00:01<00:08, 10506.96it/s]Preprocessing data:  15%|█████████████▌                                                                            | 15072/100000 [00:01<00:08, 10563.88it/s]Preprocessing data:  16%|██████████████▌                                                                           | 16142/100000 [00:01<00:07, 10602.52it/s]Preprocessing data:  17%|███████████████▍                                                                          | 17216/100000 [00:01<00:07, 10643.65it/s]Preprocessing data:  18%|████████████████▍                                                                         | 18286/100000 [00:01<00:07, 10659.71it/s]Preprocessing data:  19%|█████████████████▍                                                                        | 19355/100000 [00:01<00:07, 10667.87it/s]Preprocessing data:  20%|██████████████████▍                                                                       | 20437/100000 [00:02<00:07, 10711.82it/s]Preprocessing data:  22%|███████████████████▎                                                                      | 21517/100000 [00:02<00:07, 10735.40it/s]Preprocessing data:  23%|████████████████████▎                                                                     | 22591/100000 [00:02<00:07, 10717.91it/s]Preprocessing data:  24%|█████████████████████▎                                                                    | 23663/100000 [00:02<00:07, 10678.86it/s]Preprocessing data:  25%|██████████████████████▎                                                                   | 24731/100000 [00:02<00:07, 10647.66it/s]Preprocessing data:  26%|███████████████████████▏                                                                  | 25796/100000 [00:02<00:06, 10610.30it/s]Preprocessing data:  27%|████████████████████████▏                                                                 | 26858/100000 [00:02<00:06, 10571.72it/s]Preprocessing data:  28%|█████████████████████████                                                                 | 27916/100000 [00:02<00:06, 10545.99it/s]Preprocessing data:  29%|██████████████████████████                                                                | 28971/100000 [00:02<00:06, 10520.93it/s]Preprocessing data:  30%|███████████████████████████                                                               | 30024/100000 [00:02<00:06, 10480.68it/s]Preprocessing data:  31%|███████████████████████████▉                                                              | 31073/100000 [00:03<00:06, 10475.11it/s]Preprocessing data:  32%|████████████████████████████▉                                                             | 32121/100000 [00:03<00:06, 10457.13it/s]Preprocessing data:  33%|█████████████████████████████▊                                                            | 33167/100000 [00:03<00:06, 10444.52it/s]Preprocessing data:  34%|██████████████████████████████▊                                                           | 34213/100000 [00:03<00:06, 10447.45it/s]Preprocessing data:  35%|███████████████████████████████▋                                                          | 35258/100000 [00:03<00:06, 10438.84it/s]Preprocessing data:  36%|████████████████████████████████▋                                                         | 36302/100000 [00:03<00:06, 10420.63it/s]Preprocessing data:  37%|█████████████████████████████████▌                                                        | 37346/100000 [00:03<00:06, 10425.27it/s]Preprocessing data:  38%|██████████████████████████████████▌                                                       | 38389/100000 [00:03<00:05, 10292.72it/s]Preprocessing data:  39%|███████████████████████████████████▍                                                      | 39419/100000 [00:03<00:05, 10283.70it/s]Preprocessing data:  40%|████████████████████████████████████▍                                                     | 40450/100000 [00:03<00:05, 10289.45it/s]Preprocessing data:  41%|█████████████████████████████████████▎                                                    | 41481/100000 [00:04<00:05, 10295.41it/s]Preprocessing data:  43%|██████████████████████████████████████▎                                                   | 42511/100000 [00:04<00:05, 10275.68it/s]Preprocessing data:  44%|███████████████████████████████████████▏                                                  | 43570/100000 [00:04<00:05, 10367.67it/s]Preprocessing data:  45%|████████████████████████████████████████▏                                                 | 44635/100000 [00:04<00:05, 10451.06it/s]Preprocessing data:  46%|█████████████████████████████████████████▏                                                | 45704/100000 [00:04<00:05, 10520.24it/s]Preprocessing data:  47%|██████████████████████████████████████████                                                | 46769/100000 [00:04<00:05, 10558.76it/s]Preprocessing data:  48%|███████████████████████████████████████████                                               | 47827/100000 [00:04<00:04, 10563.89it/s]Preprocessing data:  49%|████████████████████████████████████████████                                              | 48893/100000 [00:04<00:04, 10589.73it/s]Preprocessing data:  50%|████████████████████████████████████████████▉                                             | 49961/100000 [00:04<00:04, 10614.93it/s]Preprocessing data:  51%|█████████████████████████████████████████████▉                                            | 51031/100000 [00:04<00:04, 10640.22it/s]Preprocessing data:  52%|██████████████████████████████████████████████▉                                           | 52096/100000 [00:05<00:04, 10636.81it/s]Preprocessing data:  53%|███████████████████████████████████████████████▊                                          | 53163/100000 [00:05<00:04, 10646.64it/s]Preprocessing data:  54%|████████████████████████████████████████████████▊                                         | 54228/100000 [00:05<00:04, 10643.17it/s]Preprocessing data:  55%|█████████████████████████████████████████████████▊                                        | 55294/100000 [00:05<00:04, 10647.48it/s]Preprocessing data:  56%|██████████████████████████████████████████████████▋                                       | 56359/100000 [00:05<00:04, 10621.32it/s]Preprocessing data:  57%|███████████████████████████████████████████████████▋                                      | 57428/100000 [00:05<00:04, 10640.43it/s]Preprocessing data:  58%|████████████████████████████████████████████████████▋                                     | 58495/100000 [00:05<00:03, 10646.47it/s]Preprocessing data:  60%|█████████████████████████████████████████████████████▌                                    | 59563/100000 [00:05<00:03, 10655.94it/s]Preprocessing data:  61%|██████████████████████████████████████████████████████▌                                   | 60631/100000 [00:05<00:03, 10662.62it/s]Preprocessing data:  62%|███████████████████████████████████████████████████████▌                                  | 61698/100000 [00:05<00:03, 10658.28it/s]Preprocessing data:  63%|████████████████████████████████████████████████████████▍                                 | 62775/100000 [00:06<00:03, 10689.85it/s]Preprocessing data:  64%|█████████████████████████████████████████████████████████▍                                | 63844/100000 [00:06<00:03, 10668.48it/s]Preprocessing data:  65%|██████████████████████████████████████████████████████████▍                               | 64911/100000 [00:06<00:03, 10646.11it/s]Preprocessing data:  66%|███████████████████████████████████████████████████████████▍                              | 65976/100000 [00:06<00:03, 10632.83it/s]Preprocessing data:  67%|████████████████████████████████████████████████████████████▎                             | 67040/100000 [00:06<00:03, 10626.88it/s]Preprocessing data:  68%|█████████████████████████████████████████████████████████████▎                            | 68103/100000 [00:06<00:03, 10602.85it/s]Preprocessing data:  69%|██████████████████████████████████████████████████████████████▏                           | 69164/100000 [00:06<00:02, 10590.30it/s]Preprocessing data:  70%|███████████████████████████████████████████████████████████████▏                          | 70225/100000 [00:06<00:02, 10594.59it/s]Preprocessing data:  71%|████████████████████████████████████████████████████████████████▏                         | 71285/100000 [00:06<00:02, 10579.21it/s]Preprocessing data:  72%|█████████████████████████████████████████████████████████████████                         | 72343/100000 [00:06<00:02, 10576.57it/s]Preprocessing data:  73%|██████████████████████████████████████████████████████████████████▊                        | 73401/100000 [00:07<00:02, 9604.88it/s]Preprocessing data:  74%|███████████████████████████████████████████████████████████████████▋                       | 74379/100000 [00:07<00:02, 9000.65it/s]Preprocessing data:  75%|████████████████████████████████████████████████████████████████████▌                      | 75297/100000 [00:07<00:02, 8635.09it/s]Preprocessing data:  76%|█████████████████████████████████████████████████████████████████████▎                     | 76174/100000 [00:07<00:02, 8379.98it/s]Preprocessing data:  77%|██████████████████████████████████████████████████████████████████████                     | 77021/100000 [00:07<00:02, 8219.54it/s]Preprocessing data:  78%|██████████████████████████████████████████████████████████████████████▊                    | 77849/100000 [00:07<00:02, 8097.51it/s]Preprocessing data:  79%|███████████████████████████████████████████████████████████████████████▌                   | 78662/100000 [00:07<00:02, 7996.64it/s]Preprocessing data:  79%|████████████████████████████████████████████████████████████████████████▎                  | 79464/100000 [00:07<00:02, 7914.60it/s]Preprocessing data:  80%|█████████████████████████████████████████████████████████████████████████                  | 80257/100000 [00:07<00:02, 7717.43it/s]Preprocessing data:  81%|█████████████████████████████████████████████████████████████████████████▊                 | 81061/100000 [00:08<00:02, 7808.82it/s]Preprocessing data:  82%|██████████████████████████████████████████████████████████████████████████▋                | 82010/100000 [00:08<00:02, 8291.58it/s]Preprocessing data:  83%|███████████████████████████████████████████████████████████████████████████▍               | 82842/100000 [00:08<00:02, 8239.37it/s]Preprocessing data:  84%|████████████████████████████████████████████████████████████████████████████▏              | 83668/100000 [00:08<00:02, 8118.65it/s]Preprocessing data:  85%|████████████████████████████████████████████████████████████████████████████▉              | 84573/100000 [00:08<00:01, 8389.89it/s]Preprocessing data:  86%|█████████████████████████████████████████████████████████████████████████████▉             | 85610/100000 [00:08<00:01, 8970.71it/s]Preprocessing data:  87%|██████████████████████████████████████████████████████████████████████████████▋            | 86510/100000 [00:08<00:01, 8610.72it/s]Preprocessing data:  87%|███████████████████████████████████████████████████████████████████████████████▌           | 87376/100000 [00:08<00:01, 8554.17it/s]Preprocessing data:  88%|████████████████████████████████████████████████████████████████████████████████▍          | 88380/100000 [00:08<00:01, 8984.79it/s]Preprocessing data:  89%|█████████████████████████████████████████████████████████████████████████████████▎         | 89378/100000 [00:08<00:01, 9275.26it/s]Preprocessing data:  90%|██████████████████████████████████████████████████████████████████████████████████▏        | 90309/100000 [00:09<00:01, 9179.25it/s]Preprocessing data:  91%|███████████████████████████████████████████████████████████████████████████████████        | 91230/100000 [00:09<00:01, 8449.66it/s]Preprocessing data:  92%|███████████████████████████████████████████████████████████████████████████████████▊       | 92088/100000 [00:09<00:01, 7899.40it/s]Preprocessing data:  93%|████████████████████████████████████████████████████████████████████████████████████▌      | 92892/100000 [00:09<00:00, 7587.47it/s]Preprocessing data:  94%|█████████████████████████████████████████████████████████████████████████████████████▏     | 93661/100000 [00:09<00:00, 7519.53it/s]Preprocessing data:  95%|██████████████████████████████████████████████████████████████████████████████████████     | 94544/100000 [00:09<00:00, 7879.89it/s]Preprocessing data:  96%|██████████████████████████████████████████████████████████████████████████████████████▉    | 95532/100000 [00:09<00:00, 8444.64it/s]Preprocessing data:  96%|███████████████████████████████████████████████████████████████████████████████████████▊   | 96433/100000 [00:09<00:00, 8606.47it/s]Preprocessing data:  97%|████████████████████████████████████████████████████████████████████████████████████████▌  | 97301/100000 [00:09<00:00, 8620.42it/s]Preprocessing data:  98%|█████████████████████████████████████████████████████████████████████████████████████████▎ | 98186/100000 [00:10<00:00, 8687.58it/s]Preprocessing data:  99%|██████████████████████████████████████████████████████████████████████████████████████████▎| 99187/100000 [00:10<00:00, 9078.11it/s]Preprocessing data: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:10<00:00, 9769.47it/s]
[1/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3242, -0.7617, -1.5078, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[2/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.7891, -1.3438, -1.2500, -0.0977], device='cuda:0',
       dtype=torch.bfloat16)
[3/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1209])
attention_mask shape: torch.Size([4, 1209])
reward: tensor([-1.0469, -0.5625,  0.2041, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[4/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 407])
attention_mask shape: torch.Size([4, 407])
reward: tensor([-0.1729,  0.1357, -0.6250,  1.4766], device='cuda:0',
       dtype=torch.bfloat16)
[5/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1466])
attention_mask shape: torch.Size([4, 1466])
reward: tensor([0.6172, 0.4082, 0.5000, 0.2285], device='cuda:0', dtype=torch.bfloat16)
[6/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1120])
attention_mask shape: torch.Size([4, 1120])
reward: tensor([ 1.7891, -0.6719, -0.6133, -0.2969], device='cuda:0',
       dtype=torch.bfloat16)
[7/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1954])
attention_mask shape: torch.Size([4, 1954])
reward: tensor([-1.2812,  0.0089,  0.0879, -1.9141], device='cuda:0',
       dtype=torch.bfloat16)
[8/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1336])
attention_mask shape: torch.Size([4, 1336])
reward: tensor([ 0.4453, -1.3047, -0.1001,  0.2656], device='cuda:0',
       dtype=torch.bfloat16)
[9/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1100])
attention_mask shape: torch.Size([4, 1100])
reward: tensor([ 0.1846,  0.8477, -0.4492,  0.1914], device='cuda:0',
       dtype=torch.bfloat16)
[10/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1483])
attention_mask shape: torch.Size([4, 1483])
reward: tensor([ 0.9102,  1.1562, -1.4922, -0.7422], device='cuda:0',
       dtype=torch.bfloat16)
[11/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1619])
attention_mask shape: torch.Size([4, 1619])
reward: tensor([-1.4219, -0.0776, -1.2656, -0.3965], device='cuda:0',
       dtype=torch.bfloat16)
[12/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1027])
attention_mask shape: torch.Size([4, 1027])
reward: tensor([ 0.7695, -0.5078,  0.1045,  1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[13/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 709])
attention_mask shape: torch.Size([4, 709])
reward: tensor([-1.4453,  0.1357, -1.1406,  0.0845], device='cuda:0',
       dtype=torch.bfloat16)
[14/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2007])
attention_mask shape: torch.Size([4, 2007])
reward: tensor([-0.5586, -0.2559, -1.6953, -0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[15/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 749])
attention_mask shape: torch.Size([4, 749])
reward: tensor([-0.7422,  0.5781,  1.9141, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[16/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1552])
attention_mask shape: torch.Size([4, 1552])
reward: tensor([ 0.4004, -0.5391, -0.2490, -1.0781], device='cuda:0',
       dtype=torch.bfloat16)
[17/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1719])
attention_mask shape: torch.Size([4, 1719])
reward: tensor([-0.1484, -1.2891, -1.4922, -0.1221], device='cuda:0',
       dtype=torch.bfloat16)
[18/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2041])
attention_mask shape: torch.Size([4, 2041])
reward: tensor([-0.0011,  0.1875, -0.4395, -0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[19/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1379])
attention_mask shape: torch.Size([4, 1379])
reward: tensor([ 0.0732, -0.5234, -0.8008,  1.3359], device='cuda:0',
       dtype=torch.bfloat16)
[20/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1225])
attention_mask shape: torch.Size([4, 1225])
reward: tensor([ 1.5938, -1.2109, -0.5859, -0.0444], device='cuda:0',
       dtype=torch.bfloat16)
[21/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1297])
attention_mask shape: torch.Size([4, 1297])
reward: tensor([-0.7305,  0.2119,  0.3711, -1.3281], device='cuda:0',
       dtype=torch.bfloat16)
[22/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1173])
attention_mask shape: torch.Size([4, 1173])
reward: tensor([0.2070, 0.2969, 0.1992, 0.6055], device='cuda:0', dtype=torch.bfloat16)
[23/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 919])
attention_mask shape: torch.Size([4, 919])
reward: tensor([ 0.4395,  1.1016,  0.0488, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[24/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1177])
attention_mask shape: torch.Size([4, 1177])
reward: tensor([-1.4297, -0.2695, -1.0781, -0.0601], device='cuda:0',
       dtype=torch.bfloat16)
[25/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1675])
attention_mask shape: torch.Size([4, 1675])
reward: tensor([ 0.8320, -0.2314,  0.1602, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[26/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1528])
attention_mask shape: torch.Size([4, 1528])
reward: tensor([ 0.6328,  0.1953,  0.7109, -0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[27/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1536])
attention_mask shape: torch.Size([4, 1536])
reward: tensor([-0.9258,  0.3242,  0.6914, -1.2812], device='cuda:0',
       dtype=torch.bfloat16)
[28/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 434])
attention_mask shape: torch.Size([4, 434])
reward: tensor([ 0.6523, -0.5234, -1.1172, -0.9883], device='cuda:0',
       dtype=torch.bfloat16)
[29/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1304])
attention_mask shape: torch.Size([4, 1304])
reward: tensor([ 0.5234,  0.7188, -1.2266,  1.6328], device='cuda:0',
       dtype=torch.bfloat16)
[30/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1391])
attention_mask shape: torch.Size([4, 1391])
reward: tensor([ 0.0344, -0.4043,  0.2002, -0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[31/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1644])
attention_mask shape: torch.Size([4, 1644])
reward: tensor([-0.8789,  0.4531, -0.9961, -0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[32/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1326])
attention_mask shape: torch.Size([4, 1326])
reward: tensor([-1.4766,  0.3516,  0.7383, -0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[33/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1969])
attention_mask shape: torch.Size([4, 1969])
reward: tensor([-0.3730, -0.2559,  1.8281, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[34/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1464])
attention_mask shape: torch.Size([4, 1464])
reward: tensor([ 0.4492,  0.4883, -1.2031,  0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[35/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 907])
attention_mask shape: torch.Size([4, 907])
reward: tensor([-0.3281,  0.2637, -0.8789, -1.0078], device='cuda:0',
       dtype=torch.bfloat16)
[36/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1612])
attention_mask shape: torch.Size([4, 1612])
reward: tensor([ 1.1562,  1.0234,  0.0457, -1.4844], device='cuda:0',
       dtype=torch.bfloat16)
[37/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1229])
attention_mask shape: torch.Size([4, 1229])
reward: tensor([ 0.1289, -1.3359, -0.9766,  0.1719], device='cuda:0',
       dtype=torch.bfloat16)
[38/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1348])
attention_mask shape: torch.Size([4, 1348])
reward: tensor([-0.0444,  0.7109,  0.2236, -0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[39/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([-1.6250,  0.9766,  1.2266, -2.1094], device='cuda:0',
       dtype=torch.bfloat16)
[40/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.6562, -0.2266, -2.0156,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[41/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1258])
attention_mask shape: torch.Size([4, 1258])
reward: tensor([-0.9258,  0.2715,  0.6133, -0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[42/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1616])
attention_mask shape: torch.Size([4, 1616])
reward: tensor([ 1.0938, -1.1172, -0.9258,  0.4473], device='cuda:0',
       dtype=torch.bfloat16)
[43/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 562])
attention_mask shape: torch.Size([4, 562])
reward: tensor([-0.4980, -0.6836, -0.5273, -0.0422], device='cuda:0',
       dtype=torch.bfloat16)
[44/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 896])
attention_mask shape: torch.Size([4, 896])
reward: tensor([-0.9258,  0.0557, -0.1089, -0.4395], device='cuda:0',
       dtype=torch.bfloat16)
[45/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1324])
attention_mask shape: torch.Size([4, 1324])
reward: tensor([-0.2871, -0.4395,  0.9102,  0.5898], device='cuda:0',
       dtype=torch.bfloat16)
[46/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1637])
attention_mask shape: torch.Size([4, 1637])
reward: tensor([-0.6484, -1.5469, -0.2559,  0.2773], device='cuda:0',
       dtype=torch.bfloat16)
[47/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 957])
attention_mask shape: torch.Size([4, 957])
reward: tensor([ 0.7188,  0.3555,  0.1504, -1.1562], device='cuda:0',
       dtype=torch.bfloat16)
[48/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.2285, -1.2812, -0.1953,  0.6680], device='cuda:0',
       dtype=torch.bfloat16)
[49/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1681])
attention_mask shape: torch.Size([4, 1681])
reward: tensor([-1.2500, -0.3555, -0.7383, -1.4609], device='cuda:0',
       dtype=torch.bfloat16)
[50/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1428])
attention_mask shape: torch.Size([4, 1428])
reward: tensor([-1.3828, -1.3672,  0.6328,  0.6914], device='cuda:0',
       dtype=torch.bfloat16)
[51/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1405])
attention_mask shape: torch.Size([4, 1405])
reward: tensor([-0.6094,  0.3594,  0.7070,  0.0111], device='cuda:0',
       dtype=torch.bfloat16)
[52/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1242])
attention_mask shape: torch.Size([4, 1242])
reward: tensor([-0.7773,  1.6016, -1.0938,  0.9570], device='cuda:0',
       dtype=torch.bfloat16)
[53/128] evaluate (training)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1598])
attention_mask shape: torch.Size([4, 1598])
reward: tensor([-0.7930, -0.9062,  0.5820,  0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[54/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1689])
attention_mask shape: torch.Size([4, 1689])
reward: tensor([-1.2500,  0.3496,  0.2559,  2.0469], device='cuda:0',
       dtype=torch.bfloat16)
[55/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1679])
attention_mask shape: torch.Size([4, 1679])
reward: tensor([-0.8711, -0.4746, -0.1021, -0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[56/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1270])
attention_mask shape: torch.Size([4, 1270])
reward: tensor([-1.9531, -0.4980,  0.8594, -0.9062], device='cuda:0',
       dtype=torch.bfloat16)
[57/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1103])
attention_mask shape: torch.Size([4, 1103])
reward: tensor([-0.6914,  0.4570, -1.1094,  0.6406], device='cuda:0',
       dtype=torch.bfloat16)
[58/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 586])
attention_mask shape: torch.Size([4, 586])
reward: tensor([-1.0547,  0.4980,  0.1445, -1.1719], device='cuda:0',
       dtype=torch.bfloat16)
[59/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1166])
attention_mask shape: torch.Size([4, 1166])
reward: tensor([ 0.3027,  0.7109,  0.3711, -1.4297], device='cuda:0',
       dtype=torch.bfloat16)
[60/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1372])
attention_mask shape: torch.Size([4, 1372])
reward: tensor([-0.9883,  0.8828, -0.7305, -0.0732], device='cuda:0',
       dtype=torch.bfloat16)
[61/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1081])
attention_mask shape: torch.Size([4, 1081])
reward: tensor([ 0.1670, -0.9492, -1.1719, -0.5117], device='cuda:0',
       dtype=torch.bfloat16)
[62/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1478])
attention_mask shape: torch.Size([4, 1478])
reward: tensor([-1.3125, -0.4805,  1.4688,  0.6211], device='cuda:0',
       dtype=torch.bfloat16)
[63/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.1001, -1.1562, -0.9141,  0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[64/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 716])
attention_mask shape: torch.Size([4, 716])
reward: tensor([-0.1157,  0.6797, -0.5625, -0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[65/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1103])
attention_mask shape: torch.Size([4, 1103])
reward: tensor([ 0.9258, -0.3906,  0.2021, -0.3867], device='cuda:0',
       dtype=torch.bfloat16)
[66/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 767])
attention_mask shape: torch.Size([4, 767])
reward: tensor([ 0.0991,  1.2500, -0.2676, -0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[67/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1572])
attention_mask shape: torch.Size([4, 1572])
reward: tensor([-0.5078, -0.4316, -1.7969, -0.8008], device='cuda:0',
       dtype=torch.bfloat16)
[68/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1493])
attention_mask shape: torch.Size([4, 1493])
reward: tensor([ 0.0366, -0.5352,  0.5664, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[69/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1250])
attention_mask shape: torch.Size([4, 1250])
reward: tensor([-1.1016, -0.1426,  1.3828,  0.3320], device='cuda:0',
       dtype=torch.bfloat16)
[70/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-2.1250, -1.0859,  0.6836,  0.7383], device='cuda:0',
       dtype=torch.bfloat16)
[71/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1717])
attention_mask shape: torch.Size([4, 1717])
reward: tensor([-0.4805, -1.1484, -0.3828,  0.8477], device='cuda:0',
       dtype=torch.bfloat16)
[72/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1364])
attention_mask shape: torch.Size([4, 1364])
reward: tensor([ 0.2021, -0.9688, -0.6445,  0.9727], device='cuda:0',
       dtype=torch.bfloat16)
[73/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1174])
attention_mask shape: torch.Size([4, 1174])
reward: tensor([ 0.6562,  0.7734, -0.5430, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[74/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1361])
attention_mask shape: torch.Size([4, 1361])
reward: tensor([ 0.2852,  1.5312, -0.0645,  1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[75/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 435])
attention_mask shape: torch.Size([4, 435])
reward: tensor([-1.5703,  0.2676, -0.3105, -0.5820], device='cuda:0',
       dtype=torch.bfloat16)
[76/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1484])
attention_mask shape: torch.Size([4, 1484])
reward: tensor([ 0.3340, -0.7500, -0.2715, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[77/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1490])
attention_mask shape: torch.Size([4, 1490])
reward: tensor([ 1.8281, -0.0864, -0.8008, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[78/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2042])
attention_mask shape: torch.Size([4, 2042])
reward: tensor([-2.1406,  0.1426,  0.4844, -0.2178], device='cuda:0',
       dtype=torch.bfloat16)
[79/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1390])
attention_mask shape: torch.Size([4, 1390])
reward: tensor([-0.1377,  0.7188,  0.3848,  0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[80/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1430])
attention_mask shape: torch.Size([4, 1430])
reward: tensor([ 0.2930, -0.8008,  0.2637, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[81/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1053])
attention_mask shape: torch.Size([4, 1053])
reward: tensor([-0.0378,  0.2637,  1.0469, -1.2266], device='cuda:0',
       dtype=torch.bfloat16)
[82/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1199])
attention_mask shape: torch.Size([4, 1199])
reward: tensor([ 1.9375, -2.2031,  0.8086, -0.1309], device='cuda:0',
       dtype=torch.bfloat16)
[83/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1455])
attention_mask shape: torch.Size([4, 1455])
reward: tensor([ 0.1777,  0.3906, -0.0820, -0.4570], device='cuda:0',
       dtype=torch.bfloat16)
[84/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 474])
attention_mask shape: torch.Size([4, 474])
reward: tensor([-0.3770, -2.0469, -1.4688, -0.7227], device='cuda:0',
       dtype=torch.bfloat16)
[85/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1444])
attention_mask shape: torch.Size([4, 1444])
reward: tensor([ 0.3965, -1.2812,  0.1885, -0.2598], device='cuda:0',
       dtype=torch.bfloat16)
[86/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1254])
attention_mask shape: torch.Size([4, 1254])
reward: tensor([-0.8164, -0.6641,  0.1377, -0.1113], device='cuda:0',
       dtype=torch.bfloat16)
[87/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1245])
attention_mask shape: torch.Size([4, 1245])
reward: tensor([0.1338, 0.1133, 0.7148, 0.5898], device='cuda:0', dtype=torch.bfloat16)
[88/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7656,  0.7227,  0.4609, -0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[89/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1435])
attention_mask shape: torch.Size([4, 1435])
reward: tensor([ 0.6484, -1.1484,  0.6406, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[90/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 808])
attention_mask shape: torch.Size([4, 808])
reward: tensor([ 0.9883, -0.1934,  1.3828, -1.9922], device='cuda:0',
       dtype=torch.bfloat16)
[91/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 790])
attention_mask shape: torch.Size([4, 790])
reward: tensor([-0.7305,  0.9062, -0.3594,  1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[92/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1866])
attention_mask shape: torch.Size([4, 1866])
reward: tensor([ 0.1484,  0.7148, -1.7578, -0.8789], device='cuda:0',
       dtype=torch.bfloat16)
[93/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 645])
attention_mask shape: torch.Size([4, 645])
reward: tensor([ 0.7383, -0.1221,  0.9883, -0.7617], device='cuda:0',
       dtype=torch.bfloat16)
[94/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.3828, -0.0266,  0.1289,  0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[95/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1673])
attention_mask shape: torch.Size([4, 1673])
reward: tensor([ 0.8750, -0.2002,  0.6445, -0.7812], device='cuda:0',
       dtype=torch.bfloat16)
[96/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 842])
attention_mask shape: torch.Size([4, 842])
reward: tensor([ 1.1016,  1.2422, -1.5391,  0.0356], device='cuda:0',
       dtype=torch.bfloat16)
[97/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1291])
attention_mask shape: torch.Size([4, 1291])
reward: tensor([-0.3594, -0.0713,  0.3047,  0.1309], device='cuda:0',
       dtype=torch.bfloat16)
[98/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1153])
attention_mask shape: torch.Size([4, 1153])
reward: tensor([-0.6094, -0.1001, -0.0732,  0.7188], device='cuda:0',
       dtype=torch.bfloat16)
[99/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1474])
attention_mask shape: torch.Size([4, 1474])
reward: tensor([-0.0133, -2.1250,  0.2773, -0.4707], device='cuda:0',
       dtype=torch.bfloat16)
[100/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1273])
attention_mask shape: torch.Size([4, 1273])
reward: tensor([-0.9062, -0.0334,  1.3828, -1.5391], device='cuda:0',
       dtype=torch.bfloat16)
[101/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1677])
attention_mask shape: torch.Size([4, 1677])
reward: tensor([-0.6367, -0.8789, -2.2031, -0.0557], device='cuda:0',
       dtype=torch.bfloat16)
[102/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 581])
attention_mask shape: torch.Size([4, 581])
reward: tensor([-0.4707, -0.7070,  0.7852, -1.3281], device='cuda:0',
       dtype=torch.bfloat16)
[103/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1792])
attention_mask shape: torch.Size([4, 1792])
reward: tensor([-0.2852, -0.1689, -1.0469, -1.6562], device='cuda:0',
       dtype=torch.bfloat16)
[104/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1291])
attention_mask shape: torch.Size([4, 1291])
reward: tensor([ 0.7539, -0.8359,  0.4668,  0.9922], device='cuda:0',
       dtype=torch.bfloat16)
[105/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1298])
attention_mask shape: torch.Size([4, 1298])
reward: tensor([-0.8125, -1.6328,  1.9609, -1.2500], device='cuda:0',
       dtype=torch.bfloat16)
[106/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1609])
attention_mask shape: torch.Size([4, 1609])
reward: tensor([-0.9883,  0.3457,  0.3496,  1.6875], device='cuda:0',
       dtype=torch.bfloat16)
[107/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1460])
attention_mask shape: torch.Size([4, 1460])
reward: tensor([-1.0078,  0.9219, -1.0703,  1.0312], device='cuda:0',
       dtype=torch.bfloat16)
[108/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 665])
attention_mask shape: torch.Size([4, 665])
reward: tensor([ 0.3281, -0.2969, -0.9766, -0.1670], device='cuda:0',
       dtype=torch.bfloat16)
[109/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 453])
attention_mask shape: torch.Size([4, 453])
reward: tensor([-0.5117, -0.9141, -0.1001,  0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[110/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1666])
attention_mask shape: torch.Size([4, 1666])
reward: tensor([-2.0938, -2.1406, -1.0469,  0.9609], device='cuda:0',
       dtype=torch.bfloat16)
[111/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1540])
attention_mask shape: torch.Size([4, 1540])
reward: tensor([-0.3281,  0.1011,  1.1562,  0.2559], device='cuda:0',
       dtype=torch.bfloat16)
[112/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 363])
attention_mask shape: torch.Size([4, 363])
reward: tensor([ 1.4766, -0.2754, -0.2578, -0.7109], device='cuda:0',
       dtype=torch.bfloat16)
[113/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1470])
attention_mask shape: torch.Size([4, 1470])
reward: tensor([-0.4082,  0.2930,  0.8398,  0.3125], device='cuda:0',
       dtype=torch.bfloat16)
[114/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 712])
attention_mask shape: torch.Size([4, 712])
reward: tensor([-5.8594e-01,  6.1328e-01, -1.1641e+00,  1.1139e-03], device='cuda:0',
       dtype=torch.bfloat16)
[115/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1211])
attention_mask shape: torch.Size([4, 1211])
reward: tensor([0.1533, 0.3203, 0.5898, 0.5078], device='cuda:0', dtype=torch.bfloat16)
[116/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 986])
attention_mask shape: torch.Size([4, 986])
reward: tensor([-0.3828, -0.2539, -1.2422,  0.1475], device='cuda:0',
       dtype=torch.bfloat16)
[117/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1261])
attention_mask shape: torch.Size([4, 1261])
reward: tensor([-0.8047, -0.4395,  0.5586, -0.3281], device='cuda:0',
       dtype=torch.bfloat16)
[118/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1035])
attention_mask shape: torch.Size([4, 1035])
reward: tensor([ 0.4844,  0.0422,  0.7031, -0.2520], device='cuda:0',
       dtype=torch.bfloat16)
[119/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1358])
attention_mask shape: torch.Size([4, 1358])
reward: tensor([ 0.0500, -0.9492, -0.7461, -0.1982], device='cuda:0',
       dtype=torch.bfloat16)
[120/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1409])
attention_mask shape: torch.Size([4, 1409])
reward: tensor([ 0.5859, -0.1953,  1.5859, -1.5469], device='cuda:0',
       dtype=torch.bfloat16)
[121/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1611])
attention_mask shape: torch.Size([4, 1611])
reward: tensor([-1.2891, -0.0757, -0.0957, -0.7500], device='cuda:0',
       dtype=torch.bfloat16)
[122/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1522])
attention_mask shape: torch.Size([4, 1522])
reward: tensor([-0.1270, -0.5234, -0.4531, -0.3027], device='cuda:0',
       dtype=torch.bfloat16)
[123/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1268])
attention_mask shape: torch.Size([4, 1268])
reward: tensor([-0.7695,  0.2393,  0.8008,  0.0100], device='cuda:0',
       dtype=torch.bfloat16)
[124/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1207])
attention_mask shape: torch.Size([4, 1207])
reward: tensor([-1.5078,  0.1318,  0.0957, -1.3594], device='cuda:0',
       dtype=torch.bfloat16)
[125/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1243])
attention_mask shape: torch.Size([4, 1243])
reward: tensor([ 0.6680, -1.2344, -1.6641,  0.8359], device='cuda:0',
       dtype=torch.bfloat16)
[126/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 690])
attention_mask shape: torch.Size([4, 690])
reward: tensor([-0.3867,  0.4258,  0.4043, -0.4258], device='cuda:0',
       dtype=torch.bfloat16)
[127/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1618])
attention_mask shape: torch.Size([4, 1618])
reward: tensor([-0.4004, -0.2812,  0.0913, -0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[128/128] evaluate (training)--------------------------------------------------
sequences shape: torch.Size([4, 1512])
attention_mask shape: torch.Size([4, 1512])
reward: tensor([ 0.0864,  0.0767,  0.6484, -0.0776], device='cuda:0',
       dtype=torch.bfloat16)
[513/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.1689,  0.2090, -1.8828,  0.9688], device='cuda:0',
       dtype=torch.bfloat16)
[514/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1602])
attention_mask shape: torch.Size([4, 1602])
reward: tensor([ 0.3730,  0.4551,  0.5469, -0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[515/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1611])
attention_mask shape: torch.Size([4, 1611])
reward: tensor([-0.8594,  0.3184, -0.2285, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[516/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1594])
attention_mask shape: torch.Size([4, 1594])
reward: tensor([-0.1914, -1.4609,  0.6211, -2.1562], device='cuda:0',
       dtype=torch.bfloat16)
[517/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1245])
attention_mask shape: torch.Size([4, 1245])
reward: tensor([-0.0133, -0.0557, -1.0781, -1.6094], device='cuda:0',
       dtype=torch.bfloat16)
[518/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 336])
attention_mask shape: torch.Size([4, 336])
reward: tensor([-0.0889, -0.8789, -0.4746, -0.6523], device='cuda:0',
       dtype=torch.bfloat16)
[519/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 596])
attention_mask shape: torch.Size([4, 596])
reward: tensor([ 0.5156,  0.4707, -0.2930,  0.6016], device='cuda:0',
       dtype=torch.bfloat16)
[520/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1407])
attention_mask shape: torch.Size([4, 1407])
reward: tensor([-0.3105, -1.0703, -0.0757,  0.8945], device='cuda:0',
       dtype=torch.bfloat16)
[521/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1148])
attention_mask shape: torch.Size([4, 1148])
reward: tensor([-1.6172,  0.2373, -0.7109, -0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[522/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1632])
attention_mask shape: torch.Size([4, 1632])
reward: tensor([ 0.5273, -1.1172,  0.2930, -1.7891], device='cuda:0',
       dtype=torch.bfloat16)
[523/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1612])
attention_mask shape: torch.Size([4, 1612])
reward: tensor([-0.4531,  0.6914, -1.0391,  0.1758], device='cuda:0',
       dtype=torch.bfloat16)
[524/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1165])
attention_mask shape: torch.Size([4, 1165])
reward: tensor([ 0.2676,  1.1250, -0.8789,  0.0923], device='cuda:0',
       dtype=torch.bfloat16)
[525/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1282])
attention_mask shape: torch.Size([4, 1282])
reward: tensor([ 0.1670, -0.6445, -1.2188,  0.3008], device='cuda:0',
       dtype=torch.bfloat16)
[526/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1077])
attention_mask shape: torch.Size([4, 1077])
reward: tensor([0.5195, 0.0654, 1.1719, 0.1582], device='cuda:0', dtype=torch.bfloat16)
[527/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1300])
attention_mask shape: torch.Size([4, 1300])
reward: tensor([ 0.1064, -0.0011, -0.9414,  0.2168], device='cuda:0',
       dtype=torch.bfloat16)
[528/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1007])
attention_mask shape: torch.Size([4, 1007])
reward: tensor([ 0.1270, -2.2188, -0.0045, -0.1157], device='cuda:0',
       dtype=torch.bfloat16)
[529/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 893])
attention_mask shape: torch.Size([4, 893])
reward: tensor([-0.3242,  0.2676,  0.9297,  1.2734], device='cuda:0',
       dtype=torch.bfloat16)
[530/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1302])
attention_mask shape: torch.Size([4, 1302])
reward: tensor([ 0.4004, -1.2500, -1.4375, -0.2559], device='cuda:0',
       dtype=torch.bfloat16)
[531/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1034])
attention_mask shape: torch.Size([4, 1034])
reward: tensor([-0.0289,  0.4316, -0.3457,  0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[532/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1050])
attention_mask shape: torch.Size([4, 1050])
reward: tensor([ 0.5312,  1.2266, -1.0391, -0.1641], device='cuda:0',
       dtype=torch.bfloat16)
[533/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([ 1.4609,  0.6914, -1.1406, -0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[534/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1224])
attention_mask shape: torch.Size([4, 1224])
reward: tensor([-1.3984,  0.2852, -1.1562,  0.0122], device='cuda:0',
       dtype=torch.bfloat16)
[535/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 962])
attention_mask shape: torch.Size([4, 962])
reward: tensor([-0.3730,  0.3711, -0.1758,  1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[536/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1219])
attention_mask shape: torch.Size([4, 1219])
reward: tensor([ 0.2734,  0.3203, -1.1406, -0.7344], device='cuda:0',
       dtype=torch.bfloat16)
[537/640] evaluate (test)--------------------------------------------------
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
sequences shape: torch.Size([4, 1521])
attention_mask shape: torch.Size([4, 1521])
reward: tensor([-0.7461,  0.5312, -0.8086, -0.3164], device='cuda:0',
       dtype=torch.bfloat16)
[538/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 716])
attention_mask shape: torch.Size([4, 716])
reward: tensor([-1.1172, -0.8789, -1.3594,  0.4355], device='cuda:0',
       dtype=torch.bfloat16)
[539/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1345])
attention_mask shape: torch.Size([4, 1345])
reward: tensor([ 0.3184, -1.9609,  0.6523,  0.5469], device='cuda:0',
       dtype=torch.bfloat16)
[540/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 587])
attention_mask shape: torch.Size([4, 587])
reward: tensor([-0.0178, -0.9414, -0.5195, -0.4141], device='cuda:0',
       dtype=torch.bfloat16)
[541/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1647])
attention_mask shape: torch.Size([4, 1647])
reward: tensor([-0.7734, -1.4922, -0.7344,  1.0156], device='cuda:0',
       dtype=torch.bfloat16)
[542/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1404])
attention_mask shape: torch.Size([4, 1404])
reward: tensor([-0.9961, -0.3867, -0.9961, -0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[543/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1685])
attention_mask shape: torch.Size([4, 1685])
reward: tensor([ 0.7969,  0.4141, -1.4219, -0.4180], device='cuda:0',
       dtype=torch.bfloat16)
[544/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1164])
attention_mask shape: torch.Size([4, 1164])
reward: tensor([-0.3105,  0.5781, -0.6445, -0.1021], device='cuda:0',
       dtype=torch.bfloat16)
[545/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1194])
attention_mask shape: torch.Size([4, 1194])
reward: tensor([-0.5898, -1.1250, -1.8594, -1.8672], device='cuda:0',
       dtype=torch.bfloat16)
[546/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1019])
attention_mask shape: torch.Size([4, 1019])
reward: tensor([ 0.0244,  0.8438,  0.2119, -0.0466], device='cuda:0',
       dtype=torch.bfloat16)
[547/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1068])
attention_mask shape: torch.Size([4, 1068])
reward: tensor([-0.0864,  0.6328,  1.9922,  0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[548/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1721])
attention_mask shape: torch.Size([4, 1721])
reward: tensor([ 0.0776, -0.6680, -0.6875, -0.0579], device='cuda:0',
       dtype=torch.bfloat16)
[549/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1340])
attention_mask shape: torch.Size([4, 1340])
reward: tensor([ 0.2334, -1.7031, -1.0703, -1.6719], device='cuda:0',
       dtype=torch.bfloat16)
[550/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.3984, -2.0781,  0.1436, -1.9766], device='cuda:0',
       dtype=torch.bfloat16)
[551/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1254])
attention_mask shape: torch.Size([4, 1254])
reward: tensor([ 0.9102, -1.4453, -0.3242,  0.1064], device='cuda:0',
       dtype=torch.bfloat16)
[552/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 804])
attention_mask shape: torch.Size([4, 804])
reward: tensor([ 0.4043, -0.0289, -0.8477,  0.3047], device='cuda:0',
       dtype=torch.bfloat16)
[553/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1397])
attention_mask shape: torch.Size([4, 1397])
reward: tensor([ 1.7969, -1.9531,  0.6094, -0.8594], device='cuda:0',
       dtype=torch.bfloat16)
[554/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1486])
attention_mask shape: torch.Size([4, 1486])
reward: tensor([ 0.5781, -0.7617,  0.3320,  0.1621], device='cuda:0',
       dtype=torch.bfloat16)
[555/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1857])
attention_mask shape: torch.Size([4, 1857])
reward: tensor([-0.8320,  0.3379, -1.8125,  0.4980], device='cuda:0',
       dtype=torch.bfloat16)
[556/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1503])
attention_mask shape: torch.Size([4, 1503])
reward: tensor([ 1.2734, -0.1729,  0.4805, -0.6992], device='cuda:0',
       dtype=torch.bfloat16)
[557/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 792])
attention_mask shape: torch.Size([4, 792])
reward: tensor([-0.6562, -1.0547, -0.3105,  0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[558/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1680])
attention_mask shape: torch.Size([4, 1680])
reward: tensor([-1.4219, -0.2334, -0.2354, -0.7148], device='cuda:0',
       dtype=torch.bfloat16)
[559/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1140])
attention_mask shape: torch.Size([4, 1140])
reward: tensor([-0.6562,  0.6445, -0.1553,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[560/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1421])
attention_mask shape: torch.Size([4, 1421])
reward: tensor([-1.1016, -1.0469, -0.2129, -0.3203], device='cuda:0',
       dtype=torch.bfloat16)
[561/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1852])
attention_mask shape: torch.Size([4, 1852])
reward: tensor([ 0.3691, -0.1357, -0.2598, -0.4043], device='cuda:0',
       dtype=torch.bfloat16)
[562/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1515])
attention_mask shape: torch.Size([4, 1515])
reward: tensor([ 0.2520, -0.3281,  0.7383,  0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[563/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1626])
attention_mask shape: torch.Size([4, 1626])
reward: tensor([-0.1797,  1.2422,  0.9609, -0.3340], device='cuda:0',
       dtype=torch.bfloat16)
[564/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1454])
attention_mask shape: torch.Size([4, 1454])
reward: tensor([ 0.2773, -1.4766, -0.4258,  0.2178], device='cuda:0',
       dtype=torch.bfloat16)
[565/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 764])
attention_mask shape: torch.Size([4, 764])
reward: tensor([-0.3828, -0.1865, -0.2930,  0.1621], device='cuda:0',
       dtype=torch.bfloat16)
[566/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 822])
attention_mask shape: torch.Size([4, 822])
reward: tensor([ 0.1699, -0.9336, -1.3828, -0.5234], device='cuda:0',
       dtype=torch.bfloat16)
[567/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1426])
attention_mask shape: torch.Size([4, 1426])
reward: tensor([-0.5430, -0.5391, -2.1094,  0.3730], device='cuda:0',
       dtype=torch.bfloat16)
[568/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 887])
attention_mask shape: torch.Size([4, 887])
reward: tensor([ 2.1094, -0.5469, -0.7773, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[569/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1705])
attention_mask shape: torch.Size([4, 1705])
reward: tensor([-0.6758, -0.4883,  1.6719, -0.3457], device='cuda:0',
       dtype=torch.bfloat16)
[570/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1621])
attention_mask shape: torch.Size([4, 1621])
reward: tensor([ 0.1289, -1.0859, -0.2129,  0.2109], device='cuda:0',
       dtype=torch.bfloat16)
[571/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1515])
attention_mask shape: torch.Size([4, 1515])
reward: tensor([-0.3105,  0.0255,  0.3438, -0.7227], device='cuda:0',
       dtype=torch.bfloat16)
[572/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1929])
attention_mask shape: torch.Size([4, 1929])
reward: tensor([ 0.3574,  0.9492, -1.1641,  1.1484], device='cuda:0',
       dtype=torch.bfloat16)
[573/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.1016,  0.0422,  0.9141, -0.4531], device='cuda:0',
       dtype=torch.bfloat16)
[574/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1341])
attention_mask shape: torch.Size([4, 1341])
reward: tensor([-1.0703, -0.8320, -0.1201, -0.0820], device='cuda:0',
       dtype=torch.bfloat16)
[575/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 842])
attention_mask shape: torch.Size([4, 842])
reward: tensor([ 0.4805, -0.2520,  1.5078,  0.6758], device='cuda:0',
       dtype=torch.bfloat16)
[576/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-1.9141, -0.2158,  1.5547, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[577/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1381])
attention_mask shape: torch.Size([4, 1381])
reward: tensor([-0.4805, -0.4707, -0.1396, -0.2285], device='cuda:0',
       dtype=torch.bfloat16)
[578/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1220])
attention_mask shape: torch.Size([4, 1220])
reward: tensor([-1.7344, -1.0703, -1.0312, -0.9336], device='cuda:0',
       dtype=torch.bfloat16)
[579/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1978])
attention_mask shape: torch.Size([4, 1978])
reward: tensor([ 0.6016, -0.1826,  0.7422, -0.8086], device='cuda:0',
       dtype=torch.bfloat16)
[580/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 861])
attention_mask shape: torch.Size([4, 861])
reward: tensor([-1.1172, -0.8984, -0.1797, -0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[581/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1818])
attention_mask shape: torch.Size([4, 1818])
reward: tensor([-2.1406, -1.0234,  0.2930, -0.7852], device='cuda:0',
       dtype=torch.bfloat16)
[582/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 797])
attention_mask shape: torch.Size([4, 797])
reward: tensor([ 0.8008, -0.6523, -0.5352, -0.5273], device='cuda:0',
       dtype=torch.bfloat16)
[583/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1440])
attention_mask shape: torch.Size([4, 1440])
reward: tensor([ 0.1001,  2.0156, -0.0045,  0.2227], device='cuda:0',
       dtype=torch.bfloat16)
[584/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1454])
attention_mask shape: torch.Size([4, 1454])
reward: tensor([-0.3340,  0.2520, -0.3418,  0.7031], device='cuda:0',
       dtype=torch.bfloat16)
[585/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1164])
attention_mask shape: torch.Size([4, 1164])
reward: tensor([-0.2637, -0.4707, -0.3281, -0.0222], device='cuda:0',
       dtype=torch.bfloat16)
[586/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1126])
attention_mask shape: torch.Size([4, 1126])
reward: tensor([ 0.0889, -1.4844,  0.7031, -1.4453], device='cuda:0',
       dtype=torch.bfloat16)
[587/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 751])
attention_mask shape: torch.Size([4, 751])
reward: tensor([-0.1357,  0.1055, -0.7539, -0.1484], device='cuda:0',
       dtype=torch.bfloat16)
[588/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 682])
attention_mask shape: torch.Size([4, 682])
reward: tensor([-1.1139e-03, -1.3594e+00,  1.6992e-01,  7.3242e-02], device='cuda:0',
       dtype=torch.bfloat16)
[589/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 841])
attention_mask shape: torch.Size([4, 841])
reward: tensor([ 0.5117, -1.4609, -0.1309, -0.3379], device='cuda:0',
       dtype=torch.bfloat16)
[590/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 505])
attention_mask shape: torch.Size([4, 505])
reward: tensor([-1.3125,  0.2852, -0.8320, -0.5430], device='cuda:0',
       dtype=torch.bfloat16)
[591/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 997])
attention_mask shape: torch.Size([4, 997])
reward: tensor([-1.0703, -0.4043,  0.5859, -0.8984], device='cuda:0',
       dtype=torch.bfloat16)
[592/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1441])
attention_mask shape: torch.Size([4, 1441])
reward: tensor([-0.2637, -0.8125,  0.1797, -0.2695], device='cuda:0',
       dtype=torch.bfloat16)
[593/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 1.1094,  0.7227, -1.6328,  0.3242], device='cuda:0',
       dtype=torch.bfloat16)
[594/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.7930, -1.0391, -1.0234,  1.1250], device='cuda:0',
       dtype=torch.bfloat16)
[595/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 648])
attention_mask shape: torch.Size([4, 648])
reward: tensor([-0.6055, -0.6016, -1.5391, -0.6133], device='cuda:0',
       dtype=torch.bfloat16)
[596/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1385])
attention_mask shape: torch.Size([4, 1385])
reward: tensor([-0.4004, -0.3340, -0.5078, -1.5703], device='cuda:0',
       dtype=torch.bfloat16)
[597/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2000])
attention_mask shape: torch.Size([4, 2000])
reward: tensor([-2.0156,  0.4941,  0.1079, -0.0532], device='cuda:0',
       dtype=torch.bfloat16)
[598/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1137])
attention_mask shape: torch.Size([4, 1137])
reward: tensor([-2.7930e-01, -6.4844e-01, -1.7188e+00,  1.1139e-03], device='cuda:0',
       dtype=torch.bfloat16)
[599/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.0791,  0.9492, -0.8125, -0.2559], device='cuda:0',
       dtype=torch.bfloat16)
[600/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.7656, -1.1562, -0.8516, -0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[601/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 642])
attention_mask shape: torch.Size([4, 642])
reward: tensor([-1.8047, -0.5195,  1.6016, -0.5938], device='cuda:0',
       dtype=torch.bfloat16)
[602/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1134])
attention_mask shape: torch.Size([4, 1134])
reward: tensor([-0.1201,  1.5703, -0.3242, -1.6094], device='cuda:0',
       dtype=torch.bfloat16)
[603/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1867])
attention_mask shape: torch.Size([4, 1867])
reward: tensor([ 0.1777,  0.6328, -1.7578, -0.6641], device='cuda:0',
       dtype=torch.bfloat16)
[604/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1079])
attention_mask shape: torch.Size([4, 1079])
reward: tensor([-1.4766, -0.6406,  0.2178, -1.5000], device='cuda:0',
       dtype=torch.bfloat16)
[605/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1055])
attention_mask shape: torch.Size([4, 1055])
reward: tensor([ 0.1670, -1.4297, -0.8711,  1.2109], device='cuda:0',
       dtype=torch.bfloat16)
[606/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1183])
attention_mask shape: torch.Size([4, 1183])
reward: tensor([-0.6562, -0.9062, -2.0938, -0.0178], device='cuda:0',
       dtype=torch.bfloat16)
[607/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1681])
attention_mask shape: torch.Size([4, 1681])
reward: tensor([ 0.2246, -0.5508, -0.4004,  1.8594], device='cuda:0',
       dtype=torch.bfloat16)
[608/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1235])
attention_mask shape: torch.Size([4, 1235])
reward: tensor([ 0.8086,  0.1934, -0.9141, -1.4062], device='cuda:0',
       dtype=torch.bfloat16)
[609/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 769])
attention_mask shape: torch.Size([4, 769])
reward: tensor([ 0.9609, -0.4180, -0.6875,  0.3555], device='cuda:0',
       dtype=torch.bfloat16)
[610/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1223])
attention_mask shape: torch.Size([4, 1223])
reward: tensor([ 1.3594, -0.8711,  0.5156,  0.5391], device='cuda:0',
       dtype=torch.bfloat16)
[611/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1722])
attention_mask shape: torch.Size([4, 1722])
reward: tensor([-0.9609, -0.6328, -0.9414, -0.8203], device='cuda:0',
       dtype=torch.bfloat16)
[612/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 695])
attention_mask shape: torch.Size([4, 695])
reward: tensor([-0.1709, -1.2656, -0.0400, -0.1289], device='cuda:0',
       dtype=torch.bfloat16)
[613/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1554])
attention_mask shape: torch.Size([4, 1554])
reward: tensor([-1.0859,  0.6406, -0.5898,  0.1064], device='cuda:0',
       dtype=torch.bfloat16)
[614/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1608])
attention_mask shape: torch.Size([4, 1608])
reward: tensor([-0.9492, -2.1719, -1.5859,  1.2969], device='cuda:0',
       dtype=torch.bfloat16)
[615/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1222])
attention_mask shape: torch.Size([4, 1222])
reward: tensor([ 1.1641, -0.1826,  0.3008, -0.4805], device='cuda:0',
       dtype=torch.bfloat16)
[616/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1828])
attention_mask shape: torch.Size([4, 1828])
reward: tensor([ 0.0835, -1.4922, -0.4668, -0.0845], device='cuda:0',
       dtype=torch.bfloat16)
[617/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1493])
attention_mask shape: torch.Size([4, 1493])
reward: tensor([ 0.8164, -1.0156,  2.2500, -0.1113], device='cuda:0',
       dtype=torch.bfloat16)
[618/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1721])
attention_mask shape: torch.Size([4, 1721])
reward: tensor([ 2.1094, -1.3984, -0.8320,  1.3828], device='cuda:0',
       dtype=torch.bfloat16)
[619/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 851])
attention_mask shape: torch.Size([4, 851])
reward: tensor([-0.8516, -0.5078,  1.2266, -0.2578], device='cuda:0',
       dtype=torch.bfloat16)
[620/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.4844, -0.4316, -0.9141, -0.4219], device='cuda:0',
       dtype=torch.bfloat16)
[621/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1307])
attention_mask shape: torch.Size([4, 1307])
reward: tensor([-1.7266, -1.4375, -0.0156,  0.4414], device='cuda:0',
       dtype=torch.bfloat16)
[622/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.6250,  0.3379, -1.3516, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[623/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1284])
attention_mask shape: torch.Size([4, 1284])
reward: tensor([ 1.0859, -0.4180, -0.3691,  0.6484], device='cuda:0',
       dtype=torch.bfloat16)
[624/640] evaluate (test)--------------------------------------------------
[2024-10-31 11:07:17,046] [INFO] [launch.py:351:main] Process 953152 exits successfully.
sequences shape: torch.Size([4, 1448])
attention_mask shape: torch.Size([4, 1448])
reward: tensor([ 0.7930, -2.1719,  0.6250, -0.0933], device='cuda:0',
       dtype=torch.bfloat16)
[625/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 874])
attention_mask shape: torch.Size([4, 874])
reward: tensor([ 0.6680,  1.2969, -0.0732, -0.2969], device='cuda:0',
       dtype=torch.bfloat16)
[626/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1533])
attention_mask shape: torch.Size([4, 1533])
reward: tensor([-0.7344, -1.4688,  0.2480, -0.5078], device='cuda:0',
       dtype=torch.bfloat16)
[627/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1317])
attention_mask shape: torch.Size([4, 1317])
reward: tensor([-0.1357, -0.8203, -0.1133,  0.5859], device='cuda:0',
       dtype=torch.bfloat16)
[628/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([-0.3691,  1.0469, -1.2422,  0.9453], device='cuda:0',
       dtype=torch.bfloat16)
[629/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1591])
attention_mask shape: torch.Size([4, 1591])
reward: tensor([-0.5742, -0.5156,  1.4766, -0.9414], device='cuda:0',
       dtype=torch.bfloat16)
[630/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 831])
attention_mask shape: torch.Size([4, 831])
reward: tensor([-0.5117, -0.3770, -1.6797,  0.4004], device='cuda:0',
       dtype=torch.bfloat16)
[631/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 895])
attention_mask shape: torch.Size([4, 895])
reward: tensor([ 0.4883, -1.2812, -0.1289, -0.4453], device='cuda:0',
       dtype=torch.bfloat16)
[632/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1349])
attention_mask shape: torch.Size([4, 1349])
reward: tensor([-0.2178, -0.5117, -1.7344, -1.0391], device='cuda:0',
       dtype=torch.bfloat16)
[633/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1987])
attention_mask shape: torch.Size([4, 1987])
reward: tensor([-1.0781, -1.0938, -1.0234,  0.2793], device='cuda:0',
       dtype=torch.bfloat16)
[634/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1469])
attention_mask shape: torch.Size([4, 1469])
reward: tensor([ 0.3223, -0.1602, -0.6680,  0.5586], device='cuda:0',
       dtype=torch.bfloat16)
[635/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 707])
attention_mask shape: torch.Size([4, 707])
reward: tensor([-0.7695, -0.4941, -1.5000,  0.4941], device='cuda:0',
       dtype=torch.bfloat16)
[636/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.1143, -0.1641, -1.0547, -2.2031], device='cuda:0',
       dtype=torch.bfloat16)
[637/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 627])
attention_mask shape: torch.Size([4, 627])
reward: tensor([-0.7422, -0.7422,  0.8320, -0.8281], device='cuda:0',
       dtype=torch.bfloat16)
[638/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1193])
attention_mask shape: torch.Size([4, 1193])
reward: tensor([-1.3125, -2.0781,  1.2891, -0.7695], device='cuda:0',
       dtype=torch.bfloat16)
[639/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 2048])
attention_mask shape: torch.Size([4, 2048])
reward: tensor([ 0.7109, -2.1719,  1.2109, -0.0222], device='cuda:0',
       dtype=torch.bfloat16)
[640/640] evaluate (test)--------------------------------------------------
sequences shape: torch.Size([4, 1327])
attention_mask shape: torch.Size([4, 1327])
reward: tensor([-1.3516, -0.7617,  1.3516,  0.7656], device='cuda:0',
       dtype=torch.bfloat16)
[2024-10-31 11:11:17,288] [INFO] [launch.py:351:main] Process 953151 exits successfully.
[2024-10-31 11:13:08,400] [INFO] [launch.py:351:main] Process 953154 exits successfully.
[2024-10-31 11:13:33,426] [INFO] [launch.py:351:main] Process 953153 exits successfully.
[?2004h(base) root@autodl-container-ec234bbd2e-925c6d34:~# 